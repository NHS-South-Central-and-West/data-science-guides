[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Data Science Guides",
    "section": "",
    "text": "Welcome\nFor the Python  version of this book, click here.\nData Science Guides is a series of worked examples of common data science tasks in R & Python, to use as a space for learning how to implement these methods, and as a template for applying data science at NHS South, Central and West CSU (and beyond).\nThe intention of these guides is not to teach the reader to use R or Python, nor is the intention to teach the foundations of mathematics, statistics, and probability that underpin many of the methods used in data science. While these guides should be easy enough to follow for a relative beginner, teaching these fundamentals is beyond the scope of this book.\nInstead, the hope is that these guides serve as simple starting points for anyone looking to use a particular method, using R/Python, or for anyone looking to kick off a data science project with limited experience.\nThis book is freely available and licensed under CC BY-NC-SA 4.0."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "R Data Science Guides",
    "section": "Resources",
    "text": "Resources\nThere are lots of resources for learning some of the foundations and principles that are not available in these data science guides.\n\nLearning Statistics with R\nBeyond Multiple Linear Regression\nStatistical Rethinking\nTelling Stories with Data\nStatQuest"
  },
  {
    "objectID": "intro.html#choosing-between-r-python",
    "href": "intro.html#choosing-between-r-python",
    "title": "1  Introduction",
    "section": "\n1.1 Choosing Between R & Python",
    "text": "1.1 Choosing Between R & Python\nR and Python are two of the most popular programming languages in the world, and they are both used extensively in data science. However, there is a lot of debate about which language is better, and which language is more suitable for data science. A quick Google search for “R vs Python” will quickly point you in the direction of plenty of strongly-held opinions about why R or Python is better and why the other language is rubbish and a total waste of time, but I think these debates are a little worn out, and it doesn’t help newcomers to the field to see these dichotomous views of two of the most popular languages for data science out there.\nIn my view, as someone that started out with R before moving on to Python, and that is comfortable using either, I don’t think the most important decision is about which language you will learn. It’s the decision to learn any programming language at all. I’ve found that different people find different languages easier to get started with. Personally, I find that Python makes a little more sense to me than R. While I’ve been using R a lot longer, and am quite capable with it (perhaps more so than I am with Python), I tend to be able to learn methods/concepts easier with Python, because the logic that underpins Python methods just works for me. But that doesn’t mean that the logic underpinning Python is better. It just happens to be the case that it is well-aligned with the way that I think about programming. I think it is more important to find which language makes the most sense to you. Both are very strong on data science, as well as being strong in a number of other areas, and both are very popular. Playing around with them both and working out which one makes most sense to you, and which one gives you the best shot at maintaining your coding journey, is what matters.\nIt’s also worth noting that learning a second language, when you are confident with your first, is a lot easier. The hard part is learning the first language. If you crack that part, then you will be able to tackle the second, third, and fourth languages with a lot more ease and at a much faster pace. Learning R doesn’t mean you have to stick to R forever, and vice versa for Python. The important thing is that you’ve learned a coding language. Any coding language.\n\n1.1.1 Popularity\nBoth R & Python are widely used in data science, and both are very popular programming languages. Although there are a number of ways to measure the popularity of a programming language, every measure demonstrates that Python is one of, if not the most popular language in the world, and R is in and around the top ten. A common measure of the popularity of a programming language is the Popularity of Programming Language (PYPL) Index, which is a measure of popularity based on the number of times they are searched for on Google. The PYPL Index is updated monthly, and the data is available from the PYPL website.\nHere is how the top ten most popular programming languages looks in 2023 (based on an average PYPL Index in 2023):\n\n\n\n\nFigure 1.1: The global popularity of programming languages in 2023, according to the PYPL Index\n\n\n\nWhile Python is way out in front in first, R is also very high on the list in seventh place. This is particularly impressive given that every other language on the list is a general purpose programming language, while R is a statistical programming language. When we look at the PYPL Index in the UK, we see a similar picture, with Python even further out in front in first place, and R in in fifth place. This is a testament to the popularity of R within the niche that it occupies, of which data science is a part.\nWe can also look at how the popularity of R and Python has changed over time, in comparison with other languages that are tracked in the PYPL Index. Here is a plot of the PYPL Index over time, from 2005 to 2023:\n\n\n\n\nFigure 1.2: The global popularity of programming languages from 2005 to 2023, according to the PYPL Index\n\n\n\nAs you can see, Python has been the most popular programming language since 2018, and R has been in and around the top ten for about a decade. Both languages are extremely popular, and both are likely to remain popular for the foreseeable future.\nWhile the PYPL Index is a good measure of language popularity, there are other ways of measuring the most popular programming language, such as GitHub’s The State Of Open Source Software Report and Stack Overflow’s Developer Survey and Tag Trends.\n\n1.1.2 NHS R & Python Communities\nThere are already a sizeable number of people using both languages in the NHS, and there are communities that are already in place to support people using R or Python in the NHS.\nThe NHS-R community is a pretty active community of people that are using R in the NHS. You can find them on the NHS-R website and Slack. They also have a Github organisation and a YouTube channel which both contain tons of really useful resources.\nWith regards to Python, the equivalent community is the NHS Python Community for Healthcare (typically shortened to NHS-Pycom). The NHS Python Community also have a website and a Slack channel, and they have a Github organisation and a YouTube channel which are both worth checking out. The NHS Python Community is a little smaller than the NHS-R community, but it is still a very active community, and it is growing quickly, with both communities collaborating regularly to try and spread the growth of both languages in the NHS."
  },
  {
    "objectID": "good_science.html#a-scientific-approach-to-problem-solving",
    "href": "good_science.html#a-scientific-approach-to-problem-solving",
    "title": "2  Doing Good Science",
    "section": "2.1 A Scientific Approach to Problem Solving",
    "text": "2.1 A Scientific Approach to Problem Solving\nWhile it is important to learn the methods detailed in these guides (and more), the greatest emphasis should be placed on learning how to use the scientific method to approach problems. Approaching business problems scientifically, trying to build theories based on our observations about the subject being studied, is often overlooked, as this is in many ways the most complex part of the process. It is also less exciting than learning new ways to analyse data, so for someone that is new to data science, it is quite easy to de-emphasise this and focus on doing the fun parts instead.\nThese issues are exacerbated by the fact that any data science education, whether formal or self-taught, is often focused on the methods and tools required for data science. There is less explicit focus on the “process”, but this is because the process is taught implicitly through the methods. Teaching the various methods and tools that a data scientist needs to know is time-consuming and complex, and in the process of learning and applying the methods, the student will also learn the scientific approach to problem solving. That said, once someone has mastered the methods, they are (relatively) east to apply to real-world problems, while the process of doing science the right way is more difficult, and requires careful consideration for every practitioner, no matter how experienced they are.\n\n2.1.1 Framing the Problem\nAll of this ties into the same questions that should be asked when identifying data science opportunities. But having identified these opportunities, it is important not to skip to identifying a cool method for solving the problem. Instead, focus on the problem itself, and then identify the method(s) that will be most appropriate for answering it.\nA simple, concise approach to framing the problem and maximising the value of data science is to use Tom Carpenter’s suggestions below:\n\nIf you follow these five steps detailed by Tom, you stand a good chance of approaching the problem in the right way, generating real business value, and really getting to the heart of the problem you are trying to solve.\n\n\n2.1.2 A Simple Framework for Data Science\nThe guides here obviously focus on the methods, but before diving into the code, I think it is important to highlight the importance of learning to implement these methods appropriately, and approaching data science in the “correct” way. This means understanding the problem, generating theories about how to solve it, identifying the data that is needed to test these theories, and building a robust model that is informed by your theory and that is appropriate for the data you have. Before jumping into the code and the iterative process of model building, it is important to take the time to think carefully about your theory, the variables that are relevant to the phenomenon you are studying and the causal mechanism that guides it (or the data generating process), and your expectations from your model. Only once you’ve thought carefully about all of these things should you start to think about the method(s) you should use.\nThe order of these steps is important, but the right order is dependent on context. In an ideal situation…\nFor anyone that is analytically minded, it is very easy to fall into the trap of hyperfocusing on the cool methodological approaches you could take to solving a problem. This is something that I regularly have to check myself on, because I find it so easy to get excited about the fancy methods I could apply to a particular situation. However, despite the fact that the methods are often the most fun part of the job, the fact remains that a very fancy model won’t get you very far if you haven’t spent a lot of time thinking carefully about the problem that model is trying to solve, gathering the data that is needed to solve it, and preparing the data in a way that maximises the model’s performance.\nThe easiest way to avoid falling into the same trap that I regularly slip into is to treat the methods as a means to an end. The goal should always be to build a solution to a specific problem, with a clear understanding of what “success” looks like in that context. And if you also get to make something fancy in the process? Well isn’t that exciting!\nData plays an increasingly important role in business decision-making nowadays, due to the technological advances that have made it possible to collect and analyse large amounts of data. This is not just true of tech companies, but also of traditional businesses in sectors such as retail, manufacturing, and healthcare. The NHS is increasingly embracing the role that data plays in driving decision-making, improving clinical services, and, ultimately, improving patient outcomes.\nBut as more data becomes available, the challenge of making good use of it becomes even more difficult. The old methods are still useful, but they are no longer sufficient. In order to make the most of the data that is available, the NHS and SCW need to be able to identify when there are opportunities to use data science to improve the services that they provide. Recognising these opportunities can be challenging. It requires not only a good understanding of what is possible with data science, but also a proactive approach to identifying when it is possible to do more with data than is currently being done.\nWhile the other guides in this project will hopefully give a better understanding of what is possible with data science, this particular guide is intended to help anyone frame business problems, whether internal or external, in terms of data science, and to identify opportunities to develop more advanced solutions to those problems."
  },
  {
    "objectID": "good_science.html#identifying-data-science-opportunities",
    "href": "good_science.html#identifying-data-science-opportunities",
    "title": "2  Doing Good Science",
    "section": "2.2 Identifying Data Science Opportunities",
    "text": "2.2 Identifying Data Science Opportunities\nA lot of the analytics work that is done at SCW tends to be descriptive in nature, and takes the form of regular reports, dashboards, and Excel spreadsheets summarising data. These descriptive approaches are and always will be valuable, but there is also a limit to what descriptive analytics can tell us. Descriptive analytics can tell us what has happened, but it cannot tell us why, nor can it tell us what will happen in the future. When a piece of work is concerned with explaining past events, or predicting future events, there is an opportunity to use some of the data science techniques that are covered in these guides.\nIdentifying these opportunities is not always easy, however, because requests for work will often come in the form of requests for a certain type of solution to address a business problem, with expectations of what that solution should look like. Customers and colleagues will often have a clear idea of what they want, because that have seen or used similar solutions in the past, and this limits the scope for identifying data science opportunities. If we always provide the exact solution that is requested, without considering whether there is a better way to solve the problem, we will miss out on opportunities to extract more value from our data products.\nIt is important to understand the business problem and the solution that is being requested, but it is also important to identify why a certain solution is being requested, and how the customer or colleague plans to use it. For example, a request for a dashboard that shows utilisation of a particular service in the past twelve months is not necessarily a request for a data science solution. It is possible that the reason for wanting this dashboard is to help plan how to meet future demand for that service, but it is also possible that the reason for wanting this dashboard is simply to understand how the service is being used. While a dashboard will offer valuable context, it is possible that a machine learning model that predicts future demand might be more useful.\n\n2.2.1 Asking the Right Questions\nDesigning data science solutions to business problems is a process of discovery. The best way to identify potential data science work is to ask questions.\nThere is no foolproof set of questions you can ask, and no perfect framework for designing data science solutions, but there are some questions that are more likely to lead to useful insights than others:\n\n2.2.1.1 Identifying the Problem\n\nWhat is the problem you are trying to solve?\nWhat is the business impact of solving this problem?\nWhat is the current solution to this problem?\n\n\n\n2.2.1.2 Considering Solutions\n\nWhat do you think the solution to this problem should look like?\nWhy do you think this approach is the best way to solve this problem?\nHow will you use the solution to this problem?\n\n\n\n2.2.1.3 Measuring Success\n\nHow will you know if the solution to this problem is successful?\nHow is success defined and measured for this problem?\n\n\n\n2.2.1.4 Identifying Feasible Approaches\n\nWhat data do you need to solve this problem?\nWhat data do you have that might be useful in solving this problem?\nWhen does this piece of work need to be completed by?\nHow often will this piece of work need to be repeated/updated?"
  },
  {
    "objectID": "good_science.html#next-steps",
    "href": "good_science.html#next-steps",
    "title": "2  Doing Good Science",
    "section": "2.3 Next Steps",
    "text": "2.3 Next Steps\nAlthough the data science guides focus, for the most part, on the implementation of various data science techniques, developing SCW’s data science capabilities is more than just upskilling analysts and equipping individuals with the tools to do this work. Perhaps a more important part of this process is helping a much wider group of SCW colleagues to think about data science, and to recognise opportunities to use data science to solve business problems. Hopefully the guides will help SCW colleagues to understand what is possible, and this guide will help them to identify opportunities to do more with data using these techniques."
  },
  {
    "objectID": "good_science.html#resources",
    "href": "good_science.html#resources",
    "title": "2  Doing Good Science",
    "section": "2.4 Resources",
    "text": "2.4 Resources\n\nTechniques for Problem Solving\nProblem Solving and the Scientific Method\nRichard Feynman on the Scientific Method"
  },
  {
    "objectID": "sql.html#packages",
    "href": "sql.html#packages",
    "title": "3  Importing Data from SQL",
    "section": "\n3.1 Packages",
    "text": "3.1 Packages\nThe R packages you need to interact with SQL are:\n\nDBI\nODBC\n\nI won’t import either package here because they are only needed for a handful of functions."
  },
  {
    "objectID": "sql.html#establish-sql-connection",
    "href": "sql.html#establish-sql-connection",
    "title": "3  Importing Data from SQL",
    "section": "\n3.2 Establish SQL Connection",
    "text": "3.2 Establish SQL Connection\n\ncon &lt;- DBI::dbConnect(\n  odbc::odbc(),\n  driver = 'SQL Server',\n  server = '{db-server}',\n  database = '{db-name}',\n  trustedconnection = TRUE\n)"
  },
  {
    "objectID": "sql.html#running-sql-query",
    "href": "sql.html#running-sql-query",
    "title": "3  Importing Data from SQL",
    "section": "\n3.3 Running SQL Query",
    "text": "3.3 Running SQL Query\n\ndf &lt;- DBI::dbGetQuery(\n  con,\n  'SELECT col_1,\n          col_2,\n          col_3\n  FROM    db_name.table_name'\n)\n\n\nhead(df)\n\ndplyr::glimpse(df)"
  },
  {
    "objectID": "sql.html#export-to-csv",
    "href": "sql.html#export-to-csv",
    "title": "3  Importing Data from SQL",
    "section": "\n3.4 Export to CSV",
    "text": "3.4 Export to CSV\n\nreadr::write_csv(df, here::here('path-to-save-df', 'df.csv'))"
  },
  {
    "objectID": "wrangling_r.html#r-packages-for-data-wrangling",
    "href": "wrangling_r.html#r-packages-for-data-wrangling",
    "title": "4  Wrangling Data",
    "section": "\n4.1 R Packages for Data Wrangling",
    "text": "4.1 R Packages for Data Wrangling\nData wrangling is a task that R is extremely well suited for, particularly thanks to the tidyverse ecosystem, which provides several packages that can be used to do a wide variety of data wrangling tasks, including:\n\n\nreadr - package for reading tabular/rectangular data (e.g. csv files).\n\nreadxl - package for reading Excel files.\n\ndplyr - set of functions for data manipulation (filtering rows, selecting columns, re-ordering rows, adding new variables with functions of existing variables, and collapsing many values down to a single summary).\n\ntidyr - set of functions for data transformation (changing the representation of a dataset from wide to long, or vice versa, and splitting and combining columns).\n\nstringr - tools for working with strings.\n\nlubridate - tools for dealing with dates and times.\n\nIn addition to the tidyverse packages, there are a number of other packages that come in handy when dealing with data, including:\n\n\nhere - package that simplifies how you access files on your local machine by using the relative path based on the location of your R project.\n\njanitor - set of tools for data cleaning.\n\nFinally, there are a number of packages that help you work with particularly large data sets:\n\n\nvroom - package that reads files quickly using the ALTREP framework.\n\ndata.table - package that provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed.\n\narrow - package for using Apache Arrow, which handles in-memory and larger-than-memory data very quickly.\n\nsparklyr - R interface for Apache Spark.\n\nAlthough the tidyverse packages are very popular, they are not required to work with data in R. There will be equivalent ways to do things using only base R functions too. However, I think the tidyverse is particularly user-friendly, especially for those that are coming to R from SQL, so I will focus on data wrangling using a tidyverse workflow in this guide."
  },
  {
    "objectID": "wrangling_r.html#fingertips-data",
    "href": "wrangling_r.html#fingertips-data",
    "title": "4  Wrangling Data",
    "section": "\n4.2 Fingertips Data",
    "text": "4.2 Fingertips Data\nWe will use Public Health England’s Fingertips API (for which there are packages available in R and Python) as the data source for this guide, as it represents a good opportunity to give a brief overview of this data and how to use it. Fingertips is a repository of public health data on a wide range of topics. The data included in this tutorial relates to the wider determinants of health.\nFirst, we can look at what is available. The fingertipsR::indicators_unique() function returns a list of all unique indicators from the Fingerips API, as well as their indicator IDs. We can use the View() function to open the data in a new window and take a closer look at what is available.\n\nlibrary(fingertipsR)\n\ninds &lt;- fingertipsR::indicators_unique()\n\n# View(inds)\n\nIf we were interested in a particular indicator, this would be a good way of finding the ID and getting the data. However, if we’re looking for a group of indicators about a particular topic, then the Fingertips profiles are going to be more useful.\n\nprofiles &lt;- fingertipsR::profiles()\n\n# View(profiles)\n\nFrom the profiles, we can identify topics that are of particular interest, for example the Public Health Outcomes Framework (profile ID = 19).\n\nphof_inds &lt;- fingertipsR::indicators(ProfileID=19)\n\n# View(phof_inds)\n\nThere are lots of indicators available from this profile. We can also check the area types that the indicators in the Public Health Outcomes Framework are available for, using either the fingerprintsR::area_types() function, with the ProfileID argument set to the profile we are interested in, or the fingertipsR::indicator_area_types() function, with the IndicatorID argument set to one of the indicators that we are interested in.\n\nfingertipsR::area_types(ProfileID=19) |&gt;\n  dplyr::select(AreaTypeID, AreaTypeName)\n\n   AreaTypeID                             AreaTypeName\n1           6                 Government Office Region\n2         301 Lower tier local authorities (4/20-3/21)\n3         301 Lower tier local authorities (4/20-3/21)\n4         301 Lower tier local authorities (4/20-3/21)\n5         301 Lower tier local authorities (4/20-3/21)\n6         301 Lower tier local authorities (4/20-3/21)\n7         301 Lower tier local authorities (4/20-3/21)\n8         301 Lower tier local authorities (4/20-3/21)\n9         302 Upper tier local authorities (4/20-3/21)\n10        302 Upper tier local authorities (4/20-3/21)\n11        302 Upper tier local authorities (4/20-3/21)\n12        302 Upper tier local authorities (4/20-3/21)\n13         15                                  England\n14        501 Lower tier local authorities (post 4/23)\n15        501 Lower tier local authorities (post 4/23)\n16        501 Lower tier local authorities (post 4/23)\n17        501 Lower tier local authorities (post 4/23)\n18        501 Lower tier local authorities (post 4/23)\n19        502 Upper tier local authorities (post 4/23)\n20        502 Upper tier local authorities (post 4/23)\n21        502 Upper tier local authorities (post 4/23)\n22        502 Upper tier local authorities (post 4/23)\n\nfingertipsR::indicator_areatypes(IndicatorID=phof_inds$IndicatorID[1])\n\n# A tibble: 7 × 2\n  IndicatorID AreaTypeID\n        &lt;int&gt;      &lt;int&gt;\n1       90362        502\n2       90362        402\n3       90362        302\n4       90362        202\n5       90362        102\n6       90362         15\n7       90362          6\n\n\nThe list of area types is quite long, but from the indicator_area_types() function, we can see that there are six area types available for the first indicator in the list of profile indicators. The area types ending in 2 are all upper tier local authorities (the different types being different local authority boundaries from different points in time), while the other two area types refer to Government Office Regions and England as a whole.\nWe will use the most recent upper tier local authority boundaries, which have the AreaTypeID, 502.\nHaving identified the profile and area type we are interested in, we can now pull the data from the Fingertips API using the fingertipsR::fingertips_data() function.\n\n# pull wider determinants raw data from fingertips\nphof_raw &lt;-\n  fingertipsR::fingertips_data(\n    ProfileID = 19,\n    AreaTypeID = 502\n  )\n\n\ndplyr::glimpse(phof_raw)\n\nRows: 276,809\nColumns: 27\n$ IndicatorID                         &lt;int&gt; 90362, 90362, 90362, 90362, 90362,…\n$ IndicatorName                       &lt;chr&gt; \"A01a - Healthy life expectancy at…\n$ ParentCode                          &lt;chr&gt; NA, NA, \"E92000001\", \"E92000001\", …\n$ ParentName                          &lt;chr&gt; NA, NA, \"England\", \"England\", \"Eng…\n$ AreaCode                            &lt;chr&gt; \"E92000001\", \"E92000001\", \"E120000…\n$ AreaName                            &lt;chr&gt; \"England\", \"England\", \"North East …\n$ AreaType                            &lt;chr&gt; \"England\", \"England\", \"Region\", \"R…\n$ Sex                                 &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", …\n$ Age                                 &lt;chr&gt; \"All ages\", \"All ages\", \"All ages\"…\n$ CategoryType                        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Category                            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Timeperiod                          &lt;chr&gt; \"2009 - 11\", \"2009 - 11\", \"2009 - …\n$ Value                               &lt;dbl&gt; 63.02647, 64.03794, 59.71114, 60.7…\n$ LowerCI95.0limit                    &lt;dbl&gt; 62.87787, 63.88135, 59.19049, 60.3…\n$ UpperCI95.0limit                    &lt;dbl&gt; 63.17508, 64.19453, 60.23179, 61.1…\n$ LowerCI99.8limit                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ UpperCI99.8limit                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Count                               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Denominator                         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Valuenote                           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ RecentTrend                         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ComparedtoEnglandvalueorpercentiles &lt;chr&gt; \"Not compared\", \"Not compared\", \"W…\n$ ComparedtoRegionvalueorpercentiles  &lt;chr&gt; \"Not compared\", \"Not compared\", \"N…\n$ TimeperiodSortable                  &lt;int&gt; 20090000, 20090000, 20090000, 2009…\n$ Newdata                             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Comparedtogoal                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Timeperiodrange                     &lt;chr&gt; \"3y\", \"3y\", \"3y\", \"3y\", \"3y\", \"3y\"…\n\nphof_raw |&gt;\n  dplyr::distinct(IndicatorName) |&gt;\n  head()\n\n                                                  IndicatorName\n1                       A01a - Healthy life expectancy at birth\n2                               A01b - Life expectancy at birth\n3 A02b - Inequality in healthy life expectancy at birth ENGLAND\n4                                  A01b - Life expectancy at 65\n5      A02c - Inequality in healthy life expectancy at birth LA\n6                 A02a - Inequality in life expectancy at birth\n\n\nWith the raw data imported, we are now able to start the process of cleaning and wrangling the data into a format that is ready for analysis."
  },
  {
    "objectID": "wrangling_r.html#data-wrangling",
    "href": "wrangling_r.html#data-wrangling",
    "title": "4  Wrangling Data",
    "section": "\n4.3 Data Wrangling",
    "text": "4.3 Data Wrangling\nFirst, we will clean the column names using the janitor::clean_names() function. This will convert all column names to lower case, remove any spaces and replace them with underscores, and remove any special characters. This will make it easier to work with the data in the future.\n\nphof_raw &lt;-\n  phof_raw |&gt;\n  janitor::clean_names()\n\nWe’ve got a total of 168 indicators, which means some of the indicators from the Public Health Outcomes Framework are not available for upper tier local authorities. We can filter the data to just the indicators we are interested in (as well as filtering any rows where the value is missing), using the dplyr::filter() function. We can also use the dplyr::group_by() and dplyr::summarise() functions to get the mean value of the indicators across all areas in the data. Below is an example:\n\nphof_raw |&gt;\n  dplyr::filter(\n    indicator_id %in% c(90366, 20101, 11601) &\n    !is.na(value)) |&gt;\n    dplyr::group_by(indicator_name) |&gt;\n    dplyr::summarise(mean(value))\n\n# A tibble: 3 × 2\n  indicator_name                                                 `mean(value)`\n  &lt;chr&gt;                                                                  &lt;dbl&gt;\n1 A01b - Life expectancy at birth                                        80.2 \n2 B16 - Utilisation of outdoor space for exercise/health reasons         16.3 \n3 C04 - Low birth weight of term babies                                   2.87\n\n\nThe data pulled from the Fingertips API has a number of idiosyncracies that need addressing when wrangling the data. For example, the timeperiod_sortable column refers to the year that the data relates to, and it contains four superfluous zeros at the end of each year (i.e, 20230000). We can use the dplyr::mutate() function to create a year column which strips out the zeros from the timeperiod_sortable column (using the stringr::str_remove_all() function).\n\n# filter for the indicators of interest and wrangle data into tidy structure\nphof_raw |&gt;\n  dplyr::filter(indicator_id %in% c(90366, 20101, 11601) & area_code == \"E92000001\") |&gt;\n  dplyr::group_by(indicator_name, area_code, timeperiod_sortable) |&gt;\n  dplyr::summarise(value = mean(value)) |&gt;\n  dplyr::mutate(year = stringr::str_remove_all(timeperiod_sortable, \"0000\")) |&gt;\n  dplyr::select(indicator_name, area_code, year, value)\n\n# A tibble: 39 × 4\n# Groups:   indicator_name, area_code [3]\n   indicator_name                  area_code year  value\n   &lt;chr&gt;                           &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n 1 A01b - Life expectancy at birth E92000001 2001   78.4\n 2 A01b - Life expectancy at birth E92000001 2002   78.7\n 3 A01b - Life expectancy at birth E92000001 2003   79.0\n 4 A01b - Life expectancy at birth E92000001 2004   79.4\n 5 A01b - Life expectancy at birth E92000001 2005   79.6\n 6 A01b - Life expectancy at birth E92000001 2006   79.8\n 7 A01b - Life expectancy at birth E92000001 2007   80.1\n 8 A01b - Life expectancy at birth E92000001 2008   80.4\n 9 A01b - Life expectancy at birth E92000001 2009   80.7\n10 A01b - Life expectancy at birth E92000001 2010   81.0\n# ℹ 29 more rows\n\n\nWe can also remove the ID that appended to the beginning of each indicator name. We can use str_remove_all() again, using a regular expression to remove everything up to and including the dash in the indicator name (and the space immediately after).\n\nphof_raw |&gt;\n  dplyr::filter(indicator_id %in% c(90366, 20101, 11601)) |&gt;\n  dplyr::mutate(indicator_name = stringr::str_remove(indicator_name, \"^[^-]+- \")) |&gt;\n  dplyr::distinct(indicator_name)\n\n                                            indicator_name\n1                                 Life expectancy at birth\n2 Utilisation of outdoor space for exercise/health reasons\n3                          Low birth weight of term babies\n\n\nFinally, the data is structured in a wide format, with each indicator having its own column. This is not a tidy structure, as each column should represent a variable, and each row should represent an observation. We can use the tidyr::pivot_wider() function to convert the data into a tidy structure, with each indicator having its own row. We can also use the dplyr::rename() function to rename the columns to something more meaningful, and the tidyr::drop_na() function to remove any rows where the value is missing.\n\nphof_raw |&gt;\n  dplyr::filter(indicator_id %in% c(90366, 20101, 11601)) |&gt;\n  dplyr::select(indicator_id, indicator_name, area_code, timeperiod_sortable, sex, value) |&gt;\n  tidyr::pivot_wider(\n    names_from = indicator_name,\n    values_from = value\n  ) |&gt;\n  dplyr::rename(\n      life_expectancy = `A01b - Life expectancy at birth`,\n      low_birth_weight = `C04 - Low birth weight of term babies`,\n      outdoor_space = `B16 - Utilisation of outdoor space for exercise/health reasons`\n  )\n\n# A tibble: 9,159 × 7\n   indicator_id area_code timeperiod_sortable sex    life_expectancy\n          &lt;int&gt; &lt;chr&gt;                   &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1        90366 E92000001            20010000 Male              76.2\n 2        90366 E92000001            20010000 Female            80.7\n 3        90366 E12000001            20010000 Male              74.7\n 4        90366 E12000002            20010000 Male              74.8\n 5        90366 E12000003            20010000 Male              75.5\n 6        90366 E12000004            20010000 Male              76.2\n 7        90366 E12000005            20010000 Male              75.6\n 8        90366 E12000006            20010000 Male              77.3\n 9        90366 E12000007            20010000 Male              76.0\n10        90366 E12000008            20010000 Male              77.4\n# ℹ 9,149 more rows\n# ℹ 2 more variables: outdoor_space &lt;dbl&gt;, low_birth_weight &lt;dbl&gt;\n\n\nWe can combine all of the above steps into a single function, which we can then apply to the raw data to get the data into a tidy structure.\n\n# function to wrangle data into tidy structure\nwrangle_phof_data &lt;- function(data) {\n  data |&gt;\n    dplyr::filter(indicator_id %in% c(90366, 20101, 11601)) |&gt;\n    dplyr::group_by(indicator_name, area_code, timeperiod_sortable) |&gt;\n    dplyr::summarise(value = mean(value)) |&gt;\n    dplyr::mutate(year = stringr::str_remove_all(timeperiod_sortable, \"0000\")) |&gt;\n    dplyr::select(indicator_name, area_code, year, value) |&gt;\n    tidyr::pivot_wider(\n      names_from = indicator_name,\n      values_from = value\n    ) |&gt;\n    dplyr::rename(\n      life_expectancy = `A01b - Life expectancy at birth`,\n      low_birth_weight = `C04 - Low birth weight of term babies`,\n      outdoor_space = `B16 - Utilisation of outdoor space for exercise/health reasons`\n    ) |&gt;\n    tidyr::drop_na()\n}\n\nphof &lt;- wrangle_phof_data(phof_raw)"
  },
  {
    "objectID": "wrangling_r.html#next-steps",
    "href": "wrangling_r.html#next-steps",
    "title": "4  Wrangling Data",
    "section": "\n4.4 Next Steps",
    "text": "4.4 Next Steps\nWe have successfully imported, cleaned, and wrangled Fingertips data into a tidy structure. We can now move on to the next step of the data science process, which is to explore the data and perform some analysis.\nWe could save the data to a CSV file at this point, using the readr::write_csv() function (for example, readr::write_csv(phof, here::here(\"data/phof.csv\"))), or we could perform the analysis on the dataframe object we have created."
  },
  {
    "objectID": "wrangling_r.html#resources",
    "href": "wrangling_r.html#resources",
    "title": "4  Wrangling Data",
    "section": "\n4.5 Resources",
    "text": "4.5 Resources\n\nHadley Wickham - Tidy Data\nData Transformation with {dplyr}\nData Tidying with {tidyr}\nTop 10 Must-Know {dplyr} Commands for Data Wrangling in R!\nData Wrangling with R\nData Wrangling Essentials: Comparisons in JavaScript, Python, SQL, R, and Excel\nR to Python: Data Wrangling Snippets"
  },
  {
    "objectID": "eda_r.html#inspecting-the-data",
    "href": "eda_r.html#inspecting-the-data",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.1 Inspecting the Data",
    "text": "5.1 Inspecting the Data\nThe first step when doing EDA is to inspect the data itself and get an idea of the structure of the dataset, the variable types, and the typical values of each variable. This gives a better understanding of exactly what data is being used and informs decisions both about the next steps in the exploratory process and any modelling choices.\nWe can use the head() and glimpse() functions to get a sense of the structure of the data. The head() function returns the first five rows of the data, and glimpse() returns a summary of the data, including the number of rows, the number of columns, the column names, the data type of each column, and the first few rows of the data. In addition to these two methods, we can use the distinct() function to get a list of all unique values of a particular variable. This is useful for discrete variables, such as the outcome variable, which can take on a limited number of values. For continuous variables (or any variables with a large number of unique values) the output of distinct() (a tibble) can be difficult to read, so we can use the unique() function to get a list of all unique values, which will be returned as a vector.\n\n# view first rows in the dataset\nhead(df)\n\n# A tibble: 6 × 10\n    age sex   resting_bp cholesterol fasting_bs resting_ecg max_hr angina\n  &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;        &lt;dbl&gt; &lt;fct&gt; \n1    40 M            140         289 0          Normal         172 N     \n2    49 F            160         180 0          Normal         156 N     \n3    37 M            130         283 0          ST              98 N     \n4    48 F            138         214 0          Normal         108 Y     \n5    54 M            150         195 0          Normal         122 N     \n6    39 M            120         339 0          Normal         170 N     \n# ℹ 2 more variables: heart_peak_reading &lt;dbl&gt;, heart_disease &lt;fct&gt;\n\n# overview of the data\nglimpse(df)\n\nRows: 918\nColumns: 10\n$ age                &lt;dbl&gt; 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39,…\n$ sex                &lt;fct&gt; M, F, M, F, M, M, F, M, M, F, F, M, M, M, F, F, M, …\n$ resting_bp         &lt;dbl&gt; 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 1…\n$ cholesterol        &lt;dbl&gt; 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 2…\n$ fasting_bs         &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ resting_ecg        &lt;fct&gt; Normal, Normal, ST, Normal, Normal, Normal, Normal,…\n$ max_hr             &lt;dbl&gt; 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 14…\n$ angina             &lt;fct&gt; N, N, N, Y, N, N, N, N, Y, N, N, Y, N, Y, N, N, N, …\n$ heart_peak_reading &lt;dbl&gt; 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0…\n$ heart_disease      &lt;fct&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n\n# unique values of the outcome variable\ndf |&gt;\n  distinct(heart_disease)\n\n# A tibble: 2 × 1\n  heart_disease\n  &lt;fct&gt;        \n1 0            \n2 1            \n\n# unique values of a continuous explanatory variable\nunique(df$cholesterol)\n\n  [1] 289 180 283 214 195 339 237 208 207 284 211 164 204 234 273 196 201 248\n [19] 267 223 184 288 215 209 260 468 188 518 167 224 172 186 254 306 250 177\n [37] 227 230 294 264 259 175 318 216 340 233 205 245 194 270 213 365 342 253\n [55] 277 202 297 225 246 412 265 182 218 268 163 529 100 206 238 139 263 291\n [73] 229 307 210 329 147  85 269 275 179 392 466 129 241 255 276 282 338 160\n [91] 156 272 240 393 161 228 292 388 166 247 331 341 243 279 198 249 168 603\n[109] 159 190 185 290 212 231 222 235 320 187 266 287 404 312 251 328 285 280\n[127] 192 193 308 219 257 132 226 217 303 298 256 117 295 173 315 281 309 200\n[145] 336 355 326 171 491 271 274 394 221 126 305 220 242 347 344 358 169 181\n[163]   0 236 203 153 316 311 252 458 384 258 349 142 197 113 261 310 232 110\n[181] 123 170 369 152 244 165 337 300 333 385 322 564 239 293 407 149 199 417\n[199] 178 319 354 330 302 313 141 327 304 286 360 262 325 299 409 174 183 321\n[217] 353 335 278 157 176 131"
  },
  {
    "objectID": "eda_r.html#summary-statistics",
    "href": "eda_r.html#summary-statistics",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.2 Summary Statistics",
    "text": "5.2 Summary Statistics\nSummary statistics are a quick and easy way to get a sense of the distribution, central tendency, and dispersion of the variables in the dataset. We can use the summary() function to get a summary of the data, including the mean and median values, the 1st and 3rd quartiles, and the minimum and maximum values of each numeric column. It also returns the count values for each factor column, and the number of NA values for each column.\nWhile the base summary() function is pretty effective and works right out of the box, the package skimr can provide a more detailed summary of the data, using the skim() function. If you are looking for a single function to capture the entire process of inspecting the data and computing summary statistics, skim() is the function for the job, giving you a wealth of information about the dataset as a whole and each variable in the data.\nIf we want to examine a particular variable, the functions mean(), median(), quantile(), min(), and max() will return the same information as the summary() function. We can also get a sense of dispersion by computing the standard deviation or variance of a variable. The sd() function returns the standard deviation of a variable, and the var() function returns the variance.\nFinally, we can use the count() function to get a count of the number of observations in each category of a discrete variable. Proportions can also be computed by dividing the count by the total number of observations. Using the group_by() function to group the data by a particular variable, and the mutate() function to add a new column to the data, we can compute the proportion as n/sum(n).\n\n# summary of the data\nsummary(df)\n\n      age        sex       resting_bp     cholesterol    fasting_bs\n Min.   :28.00   F:193   Min.   :  0.0   Min.   :  0.0   0:704     \n 1st Qu.:47.00   M:725   1st Qu.:120.0   1st Qu.:173.2   1:214     \n Median :54.00           Median :130.0   Median :223.0             \n Mean   :53.51           Mean   :132.4   Mean   :198.8             \n 3rd Qu.:60.00           3rd Qu.:140.0   3rd Qu.:267.0             \n Max.   :77.00           Max.   :200.0   Max.   :603.0             \n resting_ecg      max_hr      angina  heart_peak_reading heart_disease\n LVH   :188   Min.   : 60.0   N:547   Min.   :-2.6000    0:410        \n Normal:552   1st Qu.:120.0   Y:371   1st Qu.: 0.0000    1:508        \n ST    :178   Median :138.0           Median : 0.6000                 \n              Mean   :136.8           Mean   : 0.8874                 \n              3rd Qu.:156.0           3rd Qu.: 1.5000                 \n              Max.   :202.0           Max.   : 6.2000                 \n\n# more detailed summary of the data\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n918\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nsex\n0\n1\nFALSE\n2\nM: 725, F: 193\n\n\nfasting_bs\n0\n1\nFALSE\n2\n0: 704, 1: 214\n\n\nresting_ecg\n0\n1\nFALSE\n3\nNor: 552, LVH: 188, ST: 178\n\n\nangina\n0\n1\nFALSE\n2\nN: 547, Y: 371\n\n\nheart_disease\n0\n1\nFALSE\n2\n1: 508, 0: 410\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nage\n0\n1\n53.51\n9.43\n28.0\n47.00\n54.0\n60.0\n77.0\n▁▅▇▆▁\n\n\nresting_bp\n0\n1\n132.40\n18.51\n0.0\n120.00\n130.0\n140.0\n200.0\n▁▁▃▇▁\n\n\ncholesterol\n0\n1\n198.80\n109.38\n0.0\n173.25\n223.0\n267.0\n603.0\n▃▇▇▁▁\n\n\nmax_hr\n0\n1\n136.81\n25.46\n60.0\n120.00\n138.0\n156.0\n202.0\n▁▃▇▆▂\n\n\nheart_peak_reading\n0\n1\n0.89\n1.07\n-2.6\n0.00\n0.6\n1.5\n6.2\n▁▇▆▁▁\n\n\n\n\n# mean age\nmean(df$age)\n\n[1] 53.51089\n\n# median age\nmedian(df$age)\n\n[1] 54\n\n# min and max age\nmin(df$age)\n\n[1] 28\n\nmax(df$age)\n\n[1] 77\n\n# dispersion of age\nsd(df$age)\n\n[1] 9.432617\n\nvar(df$age)\n\n[1] 88.97425\n\n# heart disease count\ndf |&gt;\n  count(heart_disease)\n\n# A tibble: 2 × 2\n  heart_disease     n\n  &lt;fct&gt;         &lt;int&gt;\n1 0               410\n2 1               508\n\n# resting ecg count\ndf |&gt;\n  count(resting_ecg)\n\n# A tibble: 3 × 2\n  resting_ecg     n\n  &lt;fct&gt;       &lt;int&gt;\n1 LVH           188\n2 Normal        552\n3 ST            178\n\n# angina\ndf |&gt;\n  count(angina)\n\n# A tibble: 2 × 2\n  angina     n\n  &lt;fct&gt;  &lt;int&gt;\n1 N        547\n2 Y        371\n\n# cholesterol\ndf |&gt;\n  count(cholesterol)\n\n# A tibble: 222 × 2\n   cholesterol     n\n         &lt;dbl&gt; &lt;int&gt;\n 1           0   172\n 2          85     1\n 3         100     2\n 4         110     1\n 5         113     1\n 6         117     1\n 7         123     1\n 8         126     2\n 9         129     1\n10         131     1\n# ℹ 212 more rows\n\n# heart disease proportion\ndf |&gt; \n  group_by(resting_ecg) |&gt; \n  count(heart_disease) |&gt; \n  mutate(freq = n/sum(n))\n\n# A tibble: 6 × 4\n# Groups:   resting_ecg [3]\n  resting_ecg heart_disease     n  freq\n  &lt;fct&gt;       &lt;fct&gt;         &lt;int&gt; &lt;dbl&gt;\n1 LVH         0                82 0.436\n2 LVH         1               106 0.564\n3 Normal      0               267 0.484\n4 Normal      1               285 0.516\n5 ST          0                61 0.343\n6 ST          1               117 0.657"
  },
  {
    "objectID": "eda_r.html#data-visualisation",
    "href": "eda_r.html#data-visualisation",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.3 Data Visualisation",
    "text": "5.3 Data Visualisation\nWhile inspecting the data directly and using summary statistics to describe it is a good first step, data visualisation is a more effective way to explore the data. It allows us to quickly identify patterns and relationships in the data, and to identify any data quality issues that might not be immediately obvious without a visual representation of the data.\nWhen using data visualisation for exploratory purposes, the intent is generally to visualise the way data is distributed, both within and between variables. This can be done using a variety of different types of plots, including histograms, bar charts, box plots, scatter plots, and line plots. How variables are distributed can tell us a lot about the variable itself, and how variables are distributed relative to each other can tell us a lot about the potential relationship between the variables.\nIn this tutorial, we will use the ggplot2 package to create a series of data visualisations to explore the data in more detail. ggplot2 is an incredibly flexible and powerful package for creating data visualisations. While it can be a little difficult to make sense of the syntax at first, it is well worth the effort to learn how to use it. Learning how to use ggplot2 is beyond the scope of this tutorial, but there are a number of excellent resources available online, including the ggplot2 documentation.\n\n5.3.1 Visualising Data Distributions\nThe first step in the exploratory process is to visualise the data distributions of key variables in the dataset. This allows us to get a sense of the typical values and central tendency of the variable, as well as identifying any outliers or other data quality issues.\n\n5.3.1.1 Continuous Distributions\nFor continuous variables, we can use histograms to visualise the distribution of the data. We can use the geom_histogram() function to create a histogram of a continuous variable. The binwidth argument can be used to control the width of the bins in the histogram.\n\n# age distribution\ndf |&gt;\n  ggplot(aes(age)) +\n  geom_histogram(binwidth = 5, colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n# max hr distribution\ndf |&gt;\n  ggplot(aes(max_hr)) +\n  geom_histogram(binwidth = 10, colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n# cholesterol distribution\ndf |&gt;\n  ggplot(aes(cholesterol)) +\n  geom_histogram(binwidth = 25, colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n(a) Age\n\n\n\n\n\n(b) Maximum Heart Rate\n\n\n\n\n\n(c) Cholesterol\n\n\n\nFigure 5.1: Histograms plotting the distributions of relevant variables from the heart disease dataset.\n\n\n\n\n# filter zero values\ndf |&gt; \n  filter(cholesterol != 0) |&gt; \n  ggplot(aes(cholesterol)) +\n  geom_histogram(binwidth = 25, colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = \"Cholesterol\", y = NULL)\n\n\n\nFigure 5.2: The distribution of cholesterol readings, with zero values filtered out to better visualise the non-zero distribution.\n\n\n\nThe inflated zero values in the cholesterol distribution suggests that there may be an issue with data quality that needs addressing.\n\n5.3.1.2 Discrete Distributions\nWe can use bar charts to visualise the distribution of discrete variables. We can use the geom_bar() function to create a bar chart of a discrete variable.\n\n# sex distribution\ndf |&gt;\n  ggplot(aes(sex)) +\n  geom_bar(colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n\n# angina distribution\ndf |&gt;\n  ggplot(aes(angina)) + \n  geom_bar(colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n(a) Patient Sex\n\n\n\n\n\n(b) Angina\n\n\n\nFigure 5.3: Bar charts plotting discrete variables from the heart disease dataset.\n\n\n\n\n# heart disease distribution\ndf |&gt;\n  ggplot(aes(heart_disease)) +\n  geom_bar(colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = \"Heart Disease\", y = NULL)\n\n\n\nFigure 5.4: The distribution of the outcome variable, heart disease.\n\n\n\n\n5.3.2 Comparing Distributions\nThere are a number of ways to compare the distributions of multiple variables. Bar charts can be used to visualise two discrete variables, while histograms and box plots are useful for comparing the distribution of a continuous variable across the groups of a discrete variable, and scatter plots are particularly useful for comparing the distribution of two continuous variables.\n\n5.3.2.1 Visualising Multiple Discrete Variables\nBar charts are an effective way to visualize the observed relationship (or association, at least) between a discrete explanatory variable and a discrete outcome (whether binary, ordinal, or categorical).\nWe can use the geom_bar() function to create bar charts, but the default behaviour is to display the bars as stacked bars, which is not necessarily ideal for visualising discrete variables (though I’d recommend playing around with this yourself to decide what works in each case)\nThe position argument controls how the bars are displayed. The default position = 'stack' argument will display the bars as stacked bars, while the position = 'dodge' argument will display the bars side-by-side, and the position = 'fill' argument will display the bars as a proportion of the total number of observations in each category.\nFinally, the fill argument splits the bars by a particular variable and display them in different colours.\n\n# heart disease by sex\ndf |&gt;\n  ggplot(aes(heart_disease, fill = sex)) +\n  geom_bar(position = 'dodge', colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Heart Disease\", y = NULL)\n\n# resting ecg\ndf |&gt;\n  ggplot(aes(heart_disease, fill = resting_ecg)) +\n  geom_bar(position = 'dodge', colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Heart Disease\", y = NULL)\n\n# angina\ndf |&gt;\n  ggplot(aes(heart_disease, fill = angina)) +\n  geom_bar(position = 'dodge', colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Heart Disease\", y = NULL)\n\n# fasting bs\ndf |&gt;\n  ggplot(aes(heart_disease, fill = fasting_bs)) +\n  geom_bar(position = 'dodge', colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Heart Disease\", y = NULL)\n\n\n\n\n\n(a) Patient Sex\n\n\n\n\n\n(b) Resting ECG\n\n\n\n\n\n\n\n(c) Angina\n\n\n\n\n\n(d) Fasting Blood Sugar\n\n\n\nFigure 5.5: Bar charts plotting discrete variables from the heart disease dataset against the outcome variable, heart disease.\n\n\n\n\n5.3.2.2 Visualising A Continuous Variable Across Discrete Groups\nHistograms and box plots are useful for comparing the distribution of a continuous variable across the groups of a discrete variable.\n\n5.3.2.2.1 Histogram Plots\nWe can use the geom_histogram() function to create histograms. The fill and position arguments can be used to split the bars by a particular variable and display them in different colours, as discussed above.\n\n# age distribution by heart disease\ndf |&gt;\n  ggplot(aes(age, fill = heart_disease)) +\n  geom_histogram(\n    binwidth = 5, position = 'dodge', \n    colour = \"#333333\", linewidth = 1\n    ) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = NULL, y = NULL)\n\n# filter zero values\ndf |&gt; \n  filter(cholesterol != 0) |&gt; \n  ggplot(aes(cholesterol, fill = heart_disease)) +\n  geom_histogram(\n    binwidth = 25, position = 'dodge', \n    colour = \"#333333\", linewidth = 1\n    ) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n(a) Patient Age by Heart Disease\n\n\n\n\n\n(b) Cholesterol (Zero Values Filtered) by Heart Disease\n\n\n\nFigure 5.6: Histograms plotting the distribution of continuous variables from the heart disease dataset against the outcome variable, heart disease.\n\n\n\nThe fact that there is a significantly larger proportion of positive heart disease cases in the zero cholesterol values further demonstrates the need to address this data quality issue.\n\n5.3.2.2.2 Box Plots\nBox plots visualize the characteristics of a continuous distribution over discrete groups. We can use the geom_boxplot() function to create box plots, and the fill() argument to split the box plots by a particular variable and display them in different colours.\nHowever, while box plots can be very useful, they are not always the most effective way of visualising this information, as explained here by Cedric Scherer. This guide uses box plots for the sake of simplicity, but it is worth considering other options when visualising distributions.\n\n# age & heart disease\ndf |&gt;\n  ggplot(aes(age, heart_disease)) +\n  geom_boxplot(size=0.8) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Heart Disease\")\n\n# age & heart disease, split by sex\ndf |&gt;\n  ggplot(aes(age, heart_disease, fill = sex)) +\n  geom_boxplot(size=0.8) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Heart Disease\")\n\n# max hr & heart disease\ndf |&gt;\n  ggplot(aes(max_hr, heart_disease)) +\n  geom_boxplot(size=0.8) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Maximum Heart Rate\", y = \"Heart Disease\")\n\n# max hr & heart disease, split by sex\ndf |&gt;\n  ggplot(aes(max_hr, heart_disease, fill = sex)) +\n  geom_boxplot(size=0.8) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Maximum Heart Rate\", y = \"Heart Disease\")\n\n\n\n\n\n(a) Patient Age\n\n\n\n\n\n(b) Patient Age, Split by Sex\n\n\n\n\n\n\n\n(c) Maximum Heart Rate\n\n\n\n\n\n(d) Maximum Heart Rate, Split by Sex\n\n\n\nFigure 5.7: Box plots visualising continuous distributions over the discrete outcome variable, heart disease.\n\n\n\n\n5.3.2.3 Visualising Multiple Discrete Variables\nScatter plots are an effective way to visualize how two continuous variables vary together. We can use the geom_point() function to create scatter plots, and the colour argument to split the scatter plots by a particular variable and display them in different colours.\n\n# age & resting bp\ndf |&gt; \n  ggplot(aes(age, resting_bp)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  labs(x = \"Patient Age\", y = \"Resting Blood Pressure\")\n  \n# filter zero values\ndf |&gt;\n  filter(resting_bp != 0) |&gt; \n  ggplot(aes(age, resting_bp)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  labs(x = \"Patient Age\", y = \"Resting Blood Pressure\")\n\n# age & cholesterol\ndf |&gt;\n  filter(cholesterol != 0) |&gt; \n  ggplot(aes(age, cholesterol)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  labs(x = \"Patient Age\", y = \"Cholesterol\")\n\n# age & max hr\ndf |&gt;\n  ggplot(aes(age, max_hr)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  labs(x = \"Patient Age\", y = \"Maximum Heart Rate\")\n\n\n\n\n\n(a) Patient Age & Resting Blood Pressure\n\n\n\n\n\n(b) Patient Age & Resting Blood Pressure (Zero Values Filtered)\n\n\n\n\n\n\n\n(c) Patient Age & Cholesterol (Zero Values Filtered)\n\n\n\n\n\n(d) Patient Age & Maximum Heart Rate\n\n\n\nFigure 5.8: Scatter plots visualising two continuous distributions together.\n\n\n\nThe scatter plot visualising age and resting blood pressure highlights another observation that needs to be removed due to data quality issues.\nIf there appears to be an association between the two continuous variables that you have plotted, as is the case with age and maximum heart rate in the above plot, you can also add a regression line to visualize the strength of that association. The geom_smooth() function can be used to add a regression line to a scatter plot. The method argument specifies the type of regression line to be added, and the se argument specifies whether or not to display the standard error of the regression line.\n\n# age & max hr\ndf |&gt;\n  ggplot(aes(age, max_hr)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  geom_smooth(method = lm, se = FALSE, size = 2, colour='#005EB8') +\n  labs(x = \"Patient Age\", y = \"Maximum Heart Rate\")\n\n\n\nFigure 5.9: Scatter plot visualising the distribution of patient age and maximum heart rate with a regression line fit to the data.\n\n\n\nYou can also include discrete variables by assigning the discrete groups different colours in the scatter plot, and if you add regression lines to these plots, separate regression lines will be fit to the discrete groups. This can be useful for visualising how the association between the two continuous variables varies across the discrete groups.\n\n# age & resting bp, split by heart disease\ndf |&gt;\n  filter(resting_bp != 0) |&gt; \n  ggplot(aes(age, resting_bp, colour = heart_disease)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_colour_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Resting Blood Pressure\")\n\n# age & cholesterol, split by heart disease (with regression line)\ndf |&gt;\n  filter(cholesterol!=0) |&gt; \n  ggplot(aes(age, cholesterol, colour = heart_disease)) +\n  geom_point(size = 3, alpha = 0.8) +\n  geom_smooth(method = lm, se = FALSE, size = 1.5) +\n  scale_colour_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Resting Blood Pressure\")\n\n# age & max hr, split by heart disease (with regression line)\ndf |&gt;\n  ggplot(aes(age, max_hr, colour = heart_disease)) +\n  geom_point(size = 3, alpha = 0.8)+\n  geom_smooth(method = lm, se = FALSE, size = 1.5) +\n  scale_colour_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Maximum Heart Rate\")\n\n\n\n\n\n(a) Patient Age & Resting Blood Pressure\n\n\n\n\n\n(b) Patient Age & Cholesterol (Zero Values Filtered)\n\n\n\n\n\n(c) Patient Age & Maximum Heart Rate\n\n\n\nFigure 5.10: Scatter plots visualising continuous distributions together, with the data split and coloured by the discrete outcome variable, heart disease."
  },
  {
    "objectID": "eda_r.html#next-steps",
    "href": "eda_r.html#next-steps",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.4 Next Steps",
    "text": "5.4 Next Steps\nThere are many more visualisation techniques that you can use to explore your data, and you can find a comprehensive list of them on the ggplot2 function reference page. There are also a wide variety of ggplot extension packages that can be used to create more complex visualisations.\nThe next step in the data science process is to build a model to either explain or predict the outcome variable, heart disease. The exploratory work done here can help inform decisions about the choice of the model, and the choice of the variables that will be used to build the model. It will also help clean up the data, particularly the zero values in the cholesterol and resting blood pressure variables, to ensure that the model is built on the best possible data."
  },
  {
    "objectID": "eda_r.html#resources",
    "href": "eda_r.html#resources",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.5 Resources",
    "text": "5.5 Resources\nThere are a wealth of resources available to help you learn more about data visualisation.\n\nData Visualization: A Practical Introduction\nFundamentals of Data Visualization\nData Visualization with R\n{ggeasy} - Easy Access to {ggplot2} Commands"
  },
  {
    "objectID": "hypothesis_testing_r.html#the-motivation-for-hypothesis-testing",
    "href": "hypothesis_testing_r.html#the-motivation-for-hypothesis-testing",
    "title": "6  Hypothesis Testing",
    "section": "\n6.1 The Motivation for Hypothesis Testing",
    "text": "6.1 The Motivation for Hypothesis Testing\nA hypothesis is a statement about an effect that we expect to observe in our sample data, that is intended to generalise to the population. We do not know what the true effect in the population is, but we have a theory about what it might be, and we can test that theory using a hypothesis about what we will observe in the sample.\nOur hypothesis might relate to how the mean value of a certain quantity of interest (like height) differs from one group to another, or how we expect the value of one quantity in our sample to change as the value of another quantity changes (like the effect of weight on height). A hypothesis test is a statistical method for empirically evaluating a hypothesis, to determine whether what we observe in our sample data is likely to exist in the population.\nHypothesis testing is effectively a statistical method for testing the compatibility of the observed data against a relevant hypothesised effect, often the hypothesis of zero effect. The measure of that incompatibility is a test statistic, which quantifies the distance between the observed data and the expectations of the statistical test, given all the assumptions of the test (including the hypothesised effect size). For example, if testing the hypothesis that there is no effect, a distribution under the assumptions of the statistical test, including the null hypothesis, is generated, and the test statistic measures the distance between the observed data and the null distribution. A p-value is then calculated from this test statistic, representing the probability of a test statistic at least as extreme as the computed test statistic if all the assumptions of the statistical test are true, including the hypothesis that is being tested.\nThe most common approach to testing hypotheses is the Null Hypothesis Significance Testing (NHST) framework, which involves testing the hypothesis that there is no effect (the null hypothesis) and, where it is possible to reject the null, this can be used as evidence that something is going on, to which this may help present evidence for the theory developed by the researcher (the alternative hypothesis).\nHypothesis testing can be valuable for a variety of different use-cases, but they are not without their issues. Despite those issues, they remain a useful tool when you want to know whether the difference between two groups is real, or whether it’s a product of noise, or when you are interested in knowing whether an effect size is different from zero. When you have a need to answer questions with greater precision, I would argue that there is a need to go further than hypothesis testing, but in industry, where speed and simplicity are often of equal importance to precision2, hypothesis testing can serve real value.\n\n\n\n\n\n\nFurther Details\n\n\n\n\n\nThe goal of NHST is to “adjudicate between two complementary hypotheses” (Blackwell 2023), the null hypothesis (\\(H_0\\)) and the alternative/research hypothesis (\\(H_1\\)). The null hypothesis, which is the hypothesis that no effect exists in the population, is our starting assumption, and we require strong evidence to reject it. If we are able to find strong enough evidence to reject the null, this moves us one step closer to making a case for our alternative hypothesis.\nThe reasoning for using two complementary hypotheses is based on Popper (1935)’s The Logic of Scientific Discovery. Popper argues that it is impossible to confirm a hypothesis, because hypotheses are a form of inductive reasoning, which involves drawing conclusions about a general principle based on specific observations. We cannot confirm a hypothesis because we cannot prove that it is true by generalising from specific instances. However, we can falsify a hypothesis. If what we observe in our sample provides strong evidence that our hypothesis cannot be true, then we can conclude (with a degree of certainty) that the hypothesis is false.\nFor a better understanding about the null and alternative hypothesis, I would recommend Josh Starmer’s StatQuest videos, embedded below:\n\n\nWhen we carry out a hypothesis test, we are quantifying our confidence that what we observe in the sample did not occur by chance. If we are able to show that a effect is extremely unlikely to have occurred by chance, we can reject the null hypothesis. Rejecting the null doesn’t demonstrate that the observed effect can be explained by our theory, however, demonstrating that the observed data does not conform to our expectations given that all of our test’s assumptions, including the null hypothesis, are true, moves us one step closer to supporting our theory."
  },
  {
    "objectID": "hypothesis_testing_r.html#the-problems-with-hypothesis-testing",
    "href": "hypothesis_testing_r.html#the-problems-with-hypothesis-testing",
    "title": "6  Hypothesis Testing",
    "section": "\n6.2 The Problems With Hypothesis Testing",
    "text": "6.2 The Problems With Hypothesis Testing\nHypothesis testing serves an important purpose in science, both in academia and in industry. We use hypothesis testing to sift through noise in data to dry and draw conclusions, and this is a perfectly reasonable goal.\nHowever, the dominant framework for testing hypotheses, Null Hypothesis Significance Testing, is rife with issues. The primary problems with NHST are that it defines tests in terms of them being statistically significant or not, based on an arbitrary threshold value for p-values, and it frames p-values as the primary concern when interpreting a statistical model. While p-values are still relevant, they are not the whole story (nor the main part of the story).\nFurther, the importance afforded to p-values in the NHST framework encourages certain common misconceptions about them, and the framing that prioritises them also makes those misconceptions even more harmful. P-values are not the measure of the substantive importance of the effect; they are not the probability that the null hypothesis is true; and finally, a statistically significant p-value does not confirm the alternative hypothesis, just as a non-significant p-values does not confirm the null. They are a measure of how surprising the observed data would be if the hypothesis being tested (like the null hypothesis), and all other assumptions of the statistical test, are true.\nAlthough these problems are of real significance in academia, where precision is of the greatest importance,\n\n\n\n\n\n\nFurther Details\n\n\n\n\n\nThere are many complaints and criticisms that can be levelled at hypothesis testing. Here are some of the main ones.\nP-Values & Statistical Significance\nThe biggest problems with NHST relate to the misuse and misunderstanding of p-values, and their dichotomisation in order to make arbitrary judgements about statistical significance. The extent of the issues surrounding p-values and statistical significance are so extensive that it led the American Statistical Association to make an official statement addressing the misconceptions and clarifying the correct usage of p-values (Wasserstein and Lazar 2016). The statement makes the point that p-values do not measure the probability that a hypothesis is true, or that the data was produced by random chance alone; nor should they be interpreted as measuring the size of an effect or the importance of a result; and that by themselves p-values are not a good measure of the evidence for a model or a hypothesis (Wasserstein and Lazar 2016). Unfortunately, these are all very common misconceptions that find their way into scientific research. The misconceptions go much further than this, however, as illustrated by Greenland et al. (2016). Greenland et al. (2016) build on the wider discussion to include statistical tests, laying out a total of 25 different misconceptions about statistical testing, p-values (and confidence intervals) and statistical power3. They make the case that the biggest culprit is the arbitrary use of thresholds for defining statistical significance, which frame p-values in problematic terms, reduce a need for critical thinking about findings, and are far too easily abused.\nThis discussion has even permeated some of the media (Aschwanden 2015, 2016). FiveThirtyEight have written about this issue in some detail, and their discussion of the topic is perhaps more accessible than the more detailed discussions going on. Aschwanden (2015) is a particularly good summary of the discussion around p-values, and does a good job highlighting that the issues around p-values and the “replication crisis” are not evidence that science can’t be trusted, but rather that this stuff is extremely difficult, and that it takes a community of scientists working towards the truth, to try and achieve results.\nGarden of Forking Paths\nGelman and Loken (2021) add to the discussion by highlighting the role that the “garden of forking paths” approach to analysis, where there are many possible explanations for a statistically significant result, plays in the statistical crisis in science. It’s important to recognise how a scientific hypothesis can correspond to multiple statistical hypotheses (Gelman and Loken 2021), and to guard against this possibility. This effectively creates a “multiple comparisons” situation, where an analysis can uncover statistically significant results that can be used to support a theory by multiple “forking paths”. A “one-to-many mapping from scientific to statistical hypotheses” can create a situation that is equivalent to p-hacking but without the necessary intent (Gelman and Loken 2021).\nOne of the central questions that Gelman and Loken (2021) ask, is whether the same data-analysis decisions would have been made with a different data set. The garden of forking paths is driven by the fact that we devise scientific hypotheses using our common sense and domain knowledge, given the data we have, and we do not consider the myriad implicit choices that this involves. By not considering these implicit choices, we fail to account for the garden of forking paths.\nComparisons Are Not Enough\nI think that the biggest issue with p-values themselves is that we have placed far too much value in them. We use methods that are designed to compute p-values, we define “success” as the discovery of a statistically significant finding, and we give little thought to the design choices made in the process of carrying out these tests. This is partly driven by the frequency of misconceptions and misunderstandings of p-values, but it’s also a result of the methods we use.\nIf we are more precise about how we think about p-values, this should help us avoid making the mistake of thinking they are proving a hypothesis or that they are sufficient evidence for a theory, but if we are more precise about what a p-value really represents, this raises significant questions about the methods we use. What purpose does a test for “statistical significance” serve if we acknowledge that p-values are not enough?\nGelman (2016) argue that p-values are not the real problem, because the real problem is null hypothesis significance testing, which they refer to as a “parody of falsificationism in which straw-man null hypothesis A is rejected and this is taken as evidence in favour of preferred alternative B.” The null hypothesis significance testing framework places far too much weight on the shoulders of p-values, and even if we are doing a better job of hypothesis testing (being particularly careful not to make mistakes about how we use p-values), what is it really worth? When we are treating p-values as a part of the wider toolkit, hypothesis tests become less useful."
  },
  {
    "objectID": "hypothesis_testing_r.html#avoiding-the-pitfalls",
    "href": "hypothesis_testing_r.html#avoiding-the-pitfalls",
    "title": "6  Hypothesis Testing",
    "section": "\n6.3 Avoiding the Pitfalls",
    "text": "6.3 Avoiding the Pitfalls\nThere has been plenty of debate among statisticians about the issues around hypothesis testing and p-values, and while there is no detail too minor for two academics to throw hands over, there is a general consensus that our approach to p-values is a problem. What has not been settled, however, is what the right approach is. That makes this section of this chapter a little trickier. However, I will attempt to give some sensible advice about how to avoid the greatest dangers pose by hypothesis testing and the use of p-values more broadly.\nA more general approach to hypothesis testing would frame things in terms of a test hypothesis, which is a hypothesised effect (including but not limited to zero effect – the null hypothesis) that is assumed to be true, and a statistical test is carried out that considers how well the observed date conforms to our expectations given that the test’s assumptions, including the test hypothesis, are true.\nPerhaps unsurprisingly, the primary way to improve how we do hypothesis testing is to approach p-values differently. The simplest solution is not to use statistical significance at all. This is an arbitrary threshold that gives us very little good information, and allows researchers to avoid thinking critically about their findings. Instead, we should report the p-value itself, and be clear about what it is that p-values actually tell us!\nBeyond framing hypothesis tests in more appropriate terms, a big part of the improvements that can be made is about transparency. Being transparent about results, about what those results mean, and about what they don’t mean, is important. Further, it is important to report all findings, and not pick and choose what we found based on which results support our theory (this becomes easier if we’re not in search of statistical significance).\nUltimately, hypothesis testing requires a careful, critical approach, and this applies to the development of the hypothesis, the design of the test, and the analysis of the findings.\n\n\n\n\n\n\nFurther Details\n\n\n\n\n\nRethink P-Values\nThe p-value is not a magic number. It does not give the probability that your research hypothesis is true, nor does it give the probability that the observed association is a product of random chance (Wasserstein, Schirm, and Lazar 2019). One of the biggest issues I have with p-values is that they feel, given the way we use them in NHST, like they should be the probability that our hypothesis is true. However, it is important to never let those urges win - the p-value is not a hypothesis probability.\nInstead, p-values are the degree to which the observed data are consistent with the pattern predicted by the research hypothesis, given the statistical test’s assumptions are not violated (Greenland et al. 2016)4.\nLakens (2022) recommends thinking about p-values as a statement about the probability of data, rather than a statement about the probability of a given hypothesis or theory. I think remembering this framing is a good way to avoid misconstruing exactly what p-values represent. Similarly, we can think about p-values as our confidence in what it is we observe in our data. How confident can we be that the data we observed is the product of random chance (and the assumptions that underpin our statistical model)? A p-value is an approximation of this.\nThe point is that none of these definitions (or more precisely, intuitive explanations) of p-values are claiming that p-values are the probability that the research hypothesis is true. They say nothing about a hypothesis, because they are not a hypothesis probability.\nFraming p-values appropriately is one part of the necessary rethink about p-values. The other is the need to place less weight in what p-values tell us. Having acknowledged all the things that p-values are not, this part should be easy to understand. Given that p-values are not a hypothesis probability, and they are really a statement about our data, they are far from sufficient (though I would argue they are at least necessary) for our understanding of our data. Instead, they are part of a wider toolkit. A toolkit that contains even more important tools, like effect sizes!\nStop Using Statistical Significance\nIn addition to the common misuse of p-values, the null hypothesis significance testing framework also defines an arbitrary threshold for what is considered a sufficiently “meaningful” result - statistical significance.\nThe commonly defined significance threshold (or the alpha (\\(\\alpha\\)) level) is a p-value of 0.05. A p-value of less than 0.05 is deemed statistically significant, while a p-value greater than 0.05 is insignificant. So two p-values, one that equals 0.499 and another that equals 0.501 are both deemed to offer something very different to each other. One is treated as a sign of success, and the other is filed away in the desk drawer, never to be seen again! The arbitrariness of this threshold has been best highlighted by Gelman and Stern (2006) when they demonstrated the fact that the difference between statistically significant and insignificant is not itself statistically significant.\nDefining whether a research finding is of real value based on such a meaningless threshold is clearly one of the contributing factors in the problems with hypothesis testing, and the simple solution is to just stop treating p-values as either statistically significant or not!\nIf you need further evidence of the problems with how we have been using statistical significance, look no further than the man credited with first developing the method, Ronald A Fisher. Fisher (1956) claimed that no researcher has a fixed level of significance that they are using to reject all hypotheses, regardless of their context. Ronald, I have some terrible, terrible news.\nWhere possible, do not use terms like statistical significance, and do not use a threshold (the \\(\\alpha\\) level) for significance. Report p-values instead. P-values should be “viewed as a continuous measure of the compatibility between the data and the entire model used to compute it, ranging from 0 for complete incompatibility to 1 for perfect compatibility, and in this sense may be viewed as measuring the fit of the model to the data.” (Greenland et al. 2016, 339)\nReport All Findings\nThis suggestion ties in closely with the previous two. If p-values are treated like a continuous measure of compatibility, and we are no longer using thresholds to define whether a result is good or bad, this should reduce the urge to report findings selectively, based only on those that say the right thing.\nDo not carry out multiple tests and only show the significant results. Be open and transparent about your process and what was tested. Only reporting significant results is problematic for many reasons. There is no reason to favour statistical significant results. Instead, we should either report all our findings, as this is the most transparent approach, or we should select the substantively meaningful results, taking great care to avoid only reporting those that confirm our theory5.\nI think this principle is easier to follow in academia, where extensive reporting of findings is encouraged. I don’t think there is a downside to open and transparent reporting in academia (ignoring the potential risks posed to publication). However, in industry, there is often a need to be as clear and concise as possible. Reporting all results could potentially cause confusion, and make statistical findings much harder for non-technical audiences to understand. Therefore I think the advice for industry should be that reporting results should not be defined by the statistical significance of the results. We should report the results that matter, that contribute the most to our understanding of a quantity of interest, and that help others understand the findings. Communication of findings in industry should focus on communicating the key results from the analysis, and that should not be defined by significance.\nBuild Scientific Models\nFinally, it is important to think carefully about the implications of any hypothesis, and to understand that there is a difference between a scientific hypothesis and a statistical hypothesis6. Statistical models do not, on their own, allow scientific hypotheses\nThere are often many different statistical hypotheses that would be consistent with a particular scientific hypothesis. This is well explained and multiple examples are given by Gelman and Loken (2021).\nThe garden of forking paths may be a particularly big issue in hypothesis testing, but it doesn’t go away if we scrap NHST. It is important to think about the ways that any scientific hypothesis can be explained by multiple statistical hypotheses, and it’s also very important to think about the many choices made when building any statistical model, and what this might mean for our work. There are so many different assumptions that go into the process of building a statistical model, and when we fail to consider the breadth of assumptions that underpin our work, we can easily invalidate our findings. As discussed earlier in this chapter, we have to remember that p-values are assuming that all of our assumptions are true, and if any of them are not true, we are in hot water.\nWhat is needed is to build coherent, logically-specified scientific models from which we design our statistical analysis. McElreath (2023) makes this case, arguing that we need to build theory-driven scientific models that describe the causal relationship we are modelling before we build statistical models that attempt to test our scientific models.\nBuilding scientific models doesn’t necessarily address all concerns around the “garden of forking paths” issue, but by doing so we are forced to be more thoughtful about our research, and we are more able to identify problems with our analysis."
  },
  {
    "objectID": "hypothesis_testing_r.html#building-statistical-tests",
    "href": "hypothesis_testing_r.html#building-statistical-tests",
    "title": "6  Hypothesis Testing",
    "section": "\n6.4 Building Statistical Tests",
    "text": "6.4 Building Statistical Tests\nA generalised framework for hypothesis testing, that discards of NHST’s problematic baggage (instead following a Fisherian approach (Fisher 1956)), would take the following steps:\n\nSpecify the test hypothesis (\\(H_0\\)) that assumes some effect size against which the observed effect size will be compared - this will often be the assumption of zero effect (the null hypothesis), but it can be anything7.\nGenerate the test distribution, defined by the data being analysed and the test being carried out, which represents our expectation of what we would observe if the statistical model’s assumptions, which include the test hypothesis, are all true.\nCompute the test statistic, which quantifies how extreme the observed data is, given the test distribution.\nCompute the p-value, representing the probability of observing a test statistic as large or larger than the observed test statistic if all of the assumptions of the statistical model, including the test hypothesis, are true.\n\nThe important takeaway is that the test statistic is a measure of the sample data, the test distribution is a measure of what we would expect to observe if the test hypothesis, and all other model assumptions, are true, and the statistical test is simply quantifying how compatible the observed data would be with the generated distribution.\nWhile there are many different statistical tests that are designed to test different data distributions, using different assumptions, the general framework is the same. Below are some examples of statistical tests that are commonly used.\n\n6.4.1 T-Tests\nThe general idea of a t-test is to calculate the statistical significance of the difference between two groups. What the groups represent depends on the type of t-test being carried out, whether it be comparing a single group of observed data against a null distribution that assumes the data was generated by chance, or comparing the difference between two observed groups against a null distribution that assumes the difference is zero.\nThere are several broad groups of t-tests:\n\nOne-sample t-test - comparing the sample mean of a single group against a “known mean”, generally testing whether a sample is likely to be generated by chance.\nPaired t-test - comparing means from the same group measured multiple times, like how someone responds to a survey at different points in time.\nTwo samples t-test - comparing the means of two groups, such as measuring the difference in blood glucose levels of two different groups.\n\nThere are several variations within these groups, for example a paired t-test generally assumes equal variance between the two groups, but this is often not true, so an unequal variance t-test can be used.\nFinally, when deciding on the nature of a t-test, you must also decide if the test should be one-tailed or two-tailed. A one-tailed t-test is used to test a hypothesis that includes a directional component (such as \\(x &gt; y\\)), while a two-tailed t-test is just testing whether there is a difference between two groups. A one-tailed t-test is appropriate when you believe the difference between two groups should be positive or negative, but if the only belief is that the groups are different, a two-tailed t-test is appropriate.\nA two-tailed t-test compares whether the sample mean(s) is different, whether larger or smaller, while a one-tailed t-test assumes the direction of the difference is known. For example, if it is known that Group A is either the same or bigger than Group B (or that Group A being smaller is not of interest), then a one-tailed t-test would be appropriate.\nI will simulate data from real-world examples to demonstrate one-sample, paired, and two-sample t-tests, however, these examples are not exhaustive, and there are many variations on t-test implementations.8\n\n6.4.1.1 One-Sample T-Test\nWe can carry out a one-sample t-test using IQ scores as our example, because there is plenty of data out there about IQ scores that we can base our simulations on. The average IQ score is said to be around 100, and scores are normally distributed with a standard deviation of 15, meaning that the majority of people have an IQ in the range of 85-115.\nIn a one-sample t-test we are interested in testing a sample mean against an “expected” mean. For example, perhaps we have a group of individuals that are viewed as “high achievers”, and we want to use IQ scores to test whether this group is meaningfully different to the rest of the population (despite being aware that IQ is not a particularly good test of either an individual’s intelligence or their potential).\nA one-sample t-test would allow us to do this because we are able to test the mean IQ score for our high achievers against the expected mean value if there was no real difference between them and the population, which would just be the mean IQ score in the population (100).\nLets first start by simulating our high achievers’ IQ scores, assuming that their mean score is 105 (perhaps some of the group studied IQ tests extensively and are now able to ace the test), with a standard deviation of 15.\n\n# label: simulate-high-achievers\n\nhigh_achievers &lt;- tibble(score = rnorm(10, mean = 105, sd = 15))\n\nWe can take a look at the sample data we have simulated, to see how close it is to the true mean and standard deviation (because we are simulating the data it won’t be a perfect match).\n\n# compute summary statistics for our sample\nhigh_achievers |&gt; \n  summarise(\n    mean = mean(score),\n    std_dev = sd(score)\n    )\n\n# A tibble: 1 × 2\n   mean std_dev\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  106.    14.3\n\n\nAnd we can visualise the data to get a better sense of what the group looks like.\n\nhigh_achievers |&gt; \n  ggplot(aes(score)) +\n  geom_histogram(binwidth = 5, colour = \"#FFFFFF\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_x_continuous(breaks = seq(80, 130, 5)) +\n  labs(x = \"IQ Score\", y = NULL)\n\n\n\n\nIt is not clear what the underlying distribution of our sample data is here. We already know that the data was drawn from a normal distribution, but if we did not, we would need more observations to confirm this.\n\nt_test(high_achievers, response = score, mu = 100, alternative = \"greater\")\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      1.35     9   0.105 greater         106.     97.8      Inf\n\n\nThe p-value of our one-sample t-test is 0.105. This means that we would expect to observe a sample mean at least as extreme as our observed mean (106), if the null that there is no difference between the high achiever group’s mean IQ score and the population mean IQ score is true, a little more than 10% of the time, or 1 in 10 times. That’s quite high, so I wouldn’t be confident that we are observing a meaningful difference.\nHowever, we know that there is a meaningful difference between our high achievers and the population, because we specified a difference when we simulated our sample data. If we were to conclude that the results we observe in our t-test do not lend any support to our hypothesis that the high achievers are better at IQ tests than the average person, this would be a false negative (or sometimes unhelpfully referred to as a Type 1 Error).\nIf we increase our sample size, maybe we will have more luck!\n\nlarger_sample &lt;- tibble(score = rnorm(30, mean = 105, sd = 15))\n\nt_test(larger_sample, response = score, mu = 100, alternative = \"greater\")\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      2.26    29  0.0159 greater         106.     101.      Inf\n\n\nIncreasing the sample size from 10 to 30 has decreased the probability of observing a test statistic at least as extreme as our sample mean if our data was generated by chance (and the assumptions of our statistical model are all met) from around 10% to just over 1.5%. That seems a lot more reasonable, and at this point I’d be more comfortable concluding that the high achiever’s IQ scores are meaningfully different to the average score in the population.\nOf course, our p-value doesn’t tell us anything about the substantive importance of the difference in IQ scores. We might be willing to conclude that there is a difference, but does it matter? Well, the answer is no because it’s IQ scores and IQ scores don’t matter.\nBut what if, for some truly baffling reason, we had reason to believe IQ scores are not nonsense? In that case, how do we know if the difference between the mean value in the high achievers group and the population is large enough to be of scientific and substantive relevance? This is not a question that significance tests can answer, and is instead something that our domain expertise should answer.\n\n6.4.1.2 Paired T-Test\nWhen respondents in surveys are asked to self-report certain details about themselves, this can invite bias. Take, for example, the difference in self-reported and interviewer-measured height among men in the UK. Although there are many proud short kings out there that will confidently declare their height, safe in the knowledge that good things come in small packages, there are plenty of men that wish they were a little bit taller (someone should write a song about that), and for whom a self-reported survey is an opportunity to dream big (literally). Any time that respondents are given the opportunity to report details about themselves that have any social baggage attached, there is a possibility that the responses will be subject to social desirability bias.\nIn the case of self-reported and measured male heights, the differences are not particularly large in absolute terms. It’s very possible that the difference is actually a product of some random variation, or measurement error. We can test this!\nUsing the mean values for reported and measured heights in 2016, calculating the standard deviation from the reported standard error9, and calculating the correlation between the two variables using the value for each variable from 2011-201610, we can simulate our sample data.\n\nshort_kings &lt;-\n  faux::rnorm_multi(\n    n = 100,\n    vars = 2,\n    mu = c(177.23, 175.71),\n    sd = c(9.02, 8.99),\n    r = 0.83,\n    varnames = c(\"self_reported_height\", \"measured_height\")\n  ) |&gt; \n  mutate(height_diff = self_reported_height - measured_height)\n\nWe have simulated the self-reported and measured height for 100 men, given the mean, standard deviation, and correlation as detailed in Table 1 of the above analysis11. Using these two simulated variables, we are then able to calculate the difference between the two, and carry out a simple one-sample t-test that compares that difference against a null hypothesis with mean zero.\n\nt_test(short_kings, response = height_diff, mu = 0, direction = \"greater\")\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      2.75    99 0.00719 two.sided       1.40    0.387     2.40\n\n\nThe difference between the self-reported and measured heights has a p-value of 0.00719, suggesting there is a 0.7% probability of observing a t-statistic at least as extreme as we observe here, if the data was generated by chance and if the assumptions of our statistical model are valid.\nThe estimated difference between self-reported height and measured height is 1.4cm, with about 1cm difference from our estimate and our lower and upper confidence intervals, which suggests that the difference between self-reported height for men is larger than their measured heights. However, is 1.4cm significant enough to really care? I mean, none of this is important enough (or valid enough) to really care, but if we suspend disbelief for a moment, is a 1.4cm difference substantively meaningful? I’ll leave that up to you to decide.\n\n6.4.1.3 Two Samples T-Test\nSo we’ve established a very real, totally scientific over-reporting of male height across the UK, and some clever, entrepreneurial, and not at all exploitative pharmaceutical company has had the good sense to develop a medication that gives you an extra couple of inches in height.\nIn order to prove that it works it needs to go through testing (because we are particularly reckless, we don’t really care if it is safe in this example). An experiment is designed, with a treatment and control group that is representative of the wider male population, and the wonder drug is given to the treatment group.\nWe can compare the change in height for the two groups in the 6 months after the treatment group have taken the medication, hypothesising that the drug has had a positive effect on the treatment group, helping them grow by a whopping 1.5cm! This is compared against the average increase in height for the control group that is approximately zero, with a very small standard deviation. A two-sample t-test can help us shed some light on whether we can be confident that this difference did not occur by chance.\nWe simulate our treatment and control groups, with 20 observations in each, with the treatment group having a mean growth of 3cm and standard deviation of 3cm, and the control group having a mean growth of 0cm and a standard deviation of 0.5cm.\n\ngrowers &lt;-\n  tibble(\n    group = rep(c(\"treatment\", \"control\"), times = 20),\n    growth = rnorm(40, mean = c(3, 0), sd = c(3, .5))\n  )\n\nHaving simulated our treatment and control groups, we can compute a t-test to see if there is a difference in how much each group has grown, on average.\n\nt_test(growers, formula = growth ~ group,\n       order = c(\"treatment\", \"control\"),\n       alternative = \"greater\")\n\n# A tibble: 1 × 7\n  statistic  t_df     p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      6.84  20.6 0.000000511 greater         3.94     2.95      Inf\n\n\nThe probability that we would observe a test statistic as large or larger than observed here is very, very small, and the results of the t-test estimate that the difference between the two groups is a little bit less than 4cm, with a lower confidence interval of just under 3cm, suggesting that this medication is having a positive difference on height in the treatment group!\n\n6.4.2 Chi-Squared Tests\nAlthough there are many ways that you may encounter chi-squared (\\(\\chi^2\\)) tests, the two most common are tests of independence and a goodness of fit tests. The chi-squared test of independence is a test of the association between two categorical variables, meaning whether the variables are related or not. It is testing whether they vary independently of each other. A chi-squared goodness of fit test is used to examine how well a theoretical distribution fits the distribution of a categorical variable. We will focus on the test of independence here, because I think this is the most common use case.\nA common use-case for chi-squared tests of independence is survey data, where individuals from different groups respond to a survey question that offers a finite number of discrete answers, like a scale from good to bad. A good example of categorical variables used in survey data is the impact of male baldness on 46 year old men, carried out by Sinikumpu et al. (2021). In this survey they grouped men by levels of baldness, and asked them various questions about their lives, from education to sex-life, to consider the impact that being bald has on men’s lives and well-being.\nWe will use the education question as the basis for simulating the data for our chi-squared test. The survey splits education-level in to three groups - basic, secondary, and tertiary - while I have simplified the question by turning the levels of baldness into a binary categorical variable.\n\nnot_bald &lt;-\n  tibble(\n    hair_status = rep(\"Not Bald\", times = 300),\n    education = sample(\n      c(\"Basic\", \"Secondary\", \"Tertiary\"),\n      size = 300, replace = TRUE, \n      prob = c(0.039, 0.594, 0.367))\n  )\n\nbald &lt;-\n  tibble(\n    hair_status = rep(\"Bald\", times = 600),\n    education = sample(\n      c(\"Basic\", \"Secondary\", \"Tertiary\"),\n      size = 600, replace = TRUE, \n      prob = c(0.03, 0.603, 0.367))\n    )\n\nbaldness_survey &lt;-\n  rbind(not_bald, bald)\n\nWe have simulated each of the different baldness categories individually, before combining them into a single dataset. I think there should be a more concise way of doing this, but I haven’t wrapped my head round how I should do that yet.\nWe can visualise the difference between the three education-levels in terms of proportion of baldness, to see if there is an obvious difference.\n\nbaldness_survey |&gt; \n  ggplot(aes(education, fill = hair_status)) +\n  geom_bar(position = \"fill\", colour = \"#FFFFFF\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scwplot::scale_fill_qualitative(\"nhs\") +\n  labs(x = \"Education Level\", y = \"Proportion\")\n\n\n\n\nThere’s a small difference between people with a basic education, but for secondary and tertiary education it seems to be pretty evenly split. Our chi-squared test can test this empirically.\n\nchisq_test(baldness_survey, education ~ hair_status)\n\n# A tibble: 1 × 3\n  statistic chisq_df p_value\n      &lt;dbl&gt;    &lt;int&gt;   &lt;dbl&gt;\n1      3.38        2   0.184\n\n\nSo our p-value tells us that we should expect to observe data at least extreme as our simulated survey data a little less than 20% of the time, if the null hypothesis that there is no difference in education levels across levels of hair loss is true. This is a smaller p-value than Sinikumpu et al. (2021) observe (0.299), but that will be a consequence of the fact we are simulating the data using probabilistic samples, not replicating their results exactly.\nIf we were to fudge the numbers a little bit, so as to artificially increase the difference in education levels based on the amount of hair a 46 year old man has, what would our results look like?\n\nnot_bald &lt;-\n  tibble(\n    hair_status = rep(\"Not Bald\", times = 300),\n    education = sample(\n      c(\"Basic\", \"Secondary\", \"Tertiary\"),\n      size = 300, replace = TRUE, \n      prob = c(0.15, 0.55, 0.3))\n  )\n\nbald &lt;-\n  tibble(\n    hair_status = rep(\"Bald\", times = 600),\n    education = sample(\n      c(\"Basic\", \"Secondary\", \"Tertiary\"),\n      size = 600, replace = TRUE, \n      prob = c(0.05, 0.6, 0.35))\n    )\n\nbaldness_survey_with_effect &lt;-\n  rbind(not_bald, bald)\n\nchisq_test(baldness_survey_with_effect, education ~ hair_status)\n\n# A tibble: 1 × 3\n  statistic chisq_df     p_value\n      &lt;dbl&gt;    &lt;int&gt;       &lt;dbl&gt;\n1      31.6        2 0.000000134\n\n\nHaving increased the number of respondents with a basic education (and in turn decreasing those with secondary and tertiary educations) for the not bald group, our chi-squared test results suggest the data we observe is much more extreme. The p-value for this test is extremely small, but that’s not totally surprising. We did fake our data, after all.\n\n6.4.3 Simulation-Based Statistical Tests\nOne of the reasons I always struggled with hypothesis testing when first learning about them is the seemingly endless variations of tests, each suited to a slightly different data distribution or context. While you could take on the arduous task of learning as many different tests as possible, it is probably (definitely) better to think in terms of the framework for carrying out a hypothesis test that was detailed earlier, and understanding all tests as generalisations of this framework.\nThis was an idea pitched by the computer scientist Allen Downey, when he wrote that “there is only one test”. He’s right. This framework for understanding all hypothesis tests as variations on the same basic method uses simulated data from permutation or bootstrapping methods to generate the null distribution against which observed data can be evaluated.\nWhen classical significance testing methods were developed, simulation wasn’t possible, due to the absence of computational power. Now that we have high-powered machines that will do everything for us, we don’t have to worry about this so much, which makes simulation much more viable.\nDowney visualises how this simulation-based workflow for hypothesis testing works using the following diagram:\n\nThe infer workflow for carrying out simulation-based hypothesis tests is designed based on the same principles (specify(), hypothesize(), generate(), calculate(), & visualize()). This all-in-one process for hypothesis testing is a little more involved than the humble t-test, and the code for computing hypothesis tests using this approach is definitely more verbose. However, the power that this process gives you is that you can easily carry out any hypothesis test once you’ve learned Downey’s framework, and the consistency of the infer workflow makes it easier to learn how to do this. The infer documentation also provides a pretty exhaustive list of how to compute hypothesis tests under a variety of conditions.\nWe can see infer in action by recalculating our IQ scores t-test that we carried out earlier on. Here is what that looks like using simulation-based methods.\n\n# compute test statistic\ntest_statistic &lt;- \n  larger_sample |&gt; \n  specify(response = score) |&gt; \n  calculate(stat = \"mean\")\n\n# generate null distribution\nnull_distribution &lt;- \n  larger_sample |&gt;\n  specify(response = score) |&gt; \n  hypothesize(null = \"point\", mu = 100) |&gt; \n  generate(reps = 1000, type = \"bootstrap\") |&gt; \n  calculate(stat = \"mean\")\n\n# calculate the p-value of observing a test statistic at least as extreme as \n# our observed statistic if the data was generated by chance\np_value &lt;- \n  null_distribution |&gt;\n  get_p_value(obs_stat = test_statistic, direction = \"greater\")\n\n# visualise the test statistic against the null\nnull_distribution |&gt;\n  visualize() + \n  shade_p_value(test_statistic, direction = NULL, color = \"#00A499\") +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  annotate(\n    \"text\", x = 95, y = 125,\n    label = paste0(\"t = \", round(test_statistic, 2), \"\\n p = \", p_value),\n    size = rel(8), color=\"grey30\"\n    ) +\n  labs(x = \"IQ Score\", y = NULL, title = NULL)\n\n\n\n\nI think that one of the main strengths of this approach is the intuition it gives us for how hypothesis testing really works. By going through the process step-by-step, we can see what we are doing more easily. We compute our test statistic, which is some summary measure of the data (in this case the mean value), before simulating a null distribution under the assumption that there is no difference between our high achievers and the population. We do this by specifying the data we are testing (the response variable), stating our null hypothesis (that we are carrying out a test using a point estimate, and that the mean of the null is 100), and generating a distribution by taking 1000 bootstrap samples (where sample size is equal to our input and samples are drawn from our data with replacement).\nHaving built a null distribution, we can compare our test statistic against that null and see how extreme it appears to be if the assumption that the null hypothesis is true is correct. We can quantify this by computing a p-value, which is just a measure of the proportion of values in our null distribution that are at least as extreme as the test statistic. In the above example, because we are carrying out a one-tailed test (direction = \"greater\"), we would just need to filter the null distribution for values equal to or greater than our sample mean. If we were carrying out a two-tailed test, we would have to calculate the absolute distance of all values from the null mean, and then filter for all values equal to or greater than the difference between our sample mean and the null mean.\nTo further illustrate this workflow, below is the chi-squared test equivalent.\n\n# calculate test statistic\nchisq_statistic &lt;- \n  baldness_survey |&gt; \n  specify(education ~ hair_status) |&gt; \n  hypothesize(null = \"independence\") |&gt; \n  calculate(stat = \"Chisq\")\n\n# generate null distribution\nchisq_null &lt;- \n  baldness_survey |&gt; \n  specify(education ~ hair_status) |&gt; \n  hypothesize(null = \"independence\") |&gt; \n  generate(reps = 1000, type = \"permute\") |&gt; \n  calculate(stat = \"Chisq\")\n\n# calculate the p value from the test statistic and null distribution\nchisq_p &lt;-\n  chisq_null |&gt; \n  get_p_value(chisq_statistic, direction = \"greater\")\n\n# visualize the null distribution and test statistic\nchisq_null |&gt; \n  visualize() + \n  shade_p_value(chisq_statistic, direction = NULL, color = \"#00A499\") +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  annotate(\n    \"text\", x = 7.5, y = 250,\n    label = \n      paste0(\"χ2 = \", round(chisq_statistic$stat, 2),\n             \"\\n p = \", chisq_p),\n    size = rel(8), color=\"grey30\"\n    ) +\n  labs(x = \"Chi-Squared Statistic\", y = NULL, title = NULL)\n\n\n\n\nAs discussed above, the simulation-based approach to hypothesis testing involves carrying out the processes in the hypothesis test more explicitly, and this helps us gain intuition for what we are doing. This also forces us to think about how we specify and compute our test. It’s pretty easy to build a t-test without really getting as far as actually checking if it is the right approach, and you don’t need to think long and hard about exactly what is going on when you run a t-test. Downey’s “There is only one test” framework, and the infer implementation, forces you to think about what you are doing and to be a little more deliberate about your choices."
  },
  {
    "objectID": "hypothesis_testing_r.html#moving-beyond-testing",
    "href": "hypothesis_testing_r.html#moving-beyond-testing",
    "title": "6  Hypothesis Testing",
    "section": "\n6.5 Moving Beyond Testing",
    "text": "6.5 Moving Beyond Testing\nHypothesis testing is still a useful tool when we are looking to make conclusions about data without overinterpreting noise (Gelman, Hill, and Vehtari 2020). Simple statistical tests of difference between groups or differences from a hypothesised value have plenty of utility, especially in industry, where getting reasonably reliable results quickly can often be more valuable than taking more time to get more precision.\nHowever, where more precision is needed, a different approach may be necessary. Testing a test hypothesis can only give us some sense of the degree to which our data is consistent with the hypothesis. This may move us closer to being able to make inferences, especially if the testing approach is robust, and sound scientific models have been developed for which the hypothesis test is a reasonable statistical representation, but answering meaningful questions usually involves trying to make some assertions not just about the existence of an effect or a difference, but also quantifying it too. Where this is the goal, estimation is a better choice. The estimation approach to statistical inference does not rely on a hypothesis, and instead focuses on a “quantity of interest”, for which we use the data to estimate (Prytherch 2022). While testing tries to give us a sense of whether or not what we observe in our sample is likely to exist in the population, the estimation approach tries to measure the effect size itself.\nAlthough estimating an effect size is a good start, it is also important to quantify the precision of that estimate as well. Estimating the size of an effect, a point estimate, is inherently uncertain. This uncertainty can be a strength, rather than a weakness. By incorporating uncertainty in to our estimates, we can say more about what our sample data is telling us, and we are better able to make inferences about the population.\nFurther, by acknowledging the uncertainty in the statistical modelling process, not just about the model’s point estimates, the process should become a more honest representation of the quantity of interest. Greenland et al. (2016) argue that the goal of statistical analysis is to evaluate the certainty of an effect size, and acknowledging and embracing that uncertainty should include every aspect of the process, including the conclusions we draw and the way we present our findings.\n\n\n\n\n\n\nFurther Details\n\n\n\n\n\nEstimate Meaningful Quantities\nThis is, I believe, the most important suggestion made in this chapter. I am borrowing a phrase used by the UNC Epidemiologist, Charles Poole, in a talk he gave about the value of p-values. He states that we should “stop testing null hypotheses, start estimating meaningful quantities” (Poole 2022). His (very firmly-held) position is that there is no reforming null hypothesis significance testing. Instead, our focus should be on estimation. Poole (2022) argues that wherever significance testing is of some utility, we should instead frame the results in terms of substantive results and estimation of effect size. This would still do what hypothesis testing is attempting to do, but it should be more robust (if done properly), and the framing itself offers a lot more.\nGelman, Hill, and Vehtari (2020) argue that we are generally capable of being interested in issues where effects exist, and that the presence of an effect is therefore not entirely interesting, in and of itself. Therefore, we should concern ourselves with the things that do matter, like the size of the effect, not the existence of the effect itself.\nAlthough I’m not going to take quite as strong a position as Poole (2022) or Gelman, Hill, and Vehtari (2020), I do think his position is pretty much correct. I am of the view that hypothesis testing is generally a sub-optimal approach when trying to better understand phenomena using data. I think there are still plenty of situations where a hypothesis test is sufficient, and in industry where there is often value to providing functionally useful (if relatively imprecise) conclusions quickly, it is reasonable to turn to testing. However, there are lots of situations where an estimate of the effect size, and the precision of that estimate, are both very valuable!\nIt is rare that we should only care that the effect exists. Generally speaking, this should be a prerequisite to the more important question of what the magnitude and direction of that effect is. Even where the knowledge of the existence of an effect is sufficient, it is hard for me to imagine a situation where knowing the magnitude and direction of the effect wouldn’t be useful, and wouldn’t allow us to estimate our confidence in that effect more effectively.\nTaking an estimation approach to statistical inference doesn’t necessarily alter the statistical methods used (though more on this at the end of this chapter), but it does require our adjusting how we approach these methods and what we look to get out of them.\nEstimation is not specifically about method. If statistical tests are used, then it requires taking the estimate of the effect size, or calculating it if the context means that any output does not offer an intuitive interpretation of effect. Other methods, like regression, can give a more direct measure of effect size (though there are plenty of regression models that also require work to produce more interpretable effect estimates).\nThe important change in approach is to focus on the importance of the estimated effect size, not on the p-value, or statistical significance!\nEmbrace Uncertainty\nWhen we move away from testing null hypotheses and focus instead on the estimation of effect sizes, we go from testing for the existence of an effect to estimating the magnitude (effect size) and the direction (whether the effect is positive or negative). However, a key part of this that is missing is the precision with which we are able to estimate the effect. It’s important to remember, in any statistical modelling, that we are estimating quantities, and therefore any point estimate is just our best effort at quantifying an unknown population parameter. The point estimate is probably wrong! It is important that we acknowledge the uncertainty in our estimations and account for them in the way we present model outputs12.\nHowever, the embrace of uncertainty is not limited to our uncertainty in our estimates. It is a wider issue. We have to accept that every part of the process of estimating meaningful quantities is done with a degree of uncertainty, and as such the way we think about statistical modelling and the way we present our results should account for the existence of variance. Uncertainty exists everywhere in statistical analysis and inference (Wasserstein, Schirm, and Lazar 2019).\nWasserstein, Schirm, and Lazar (2019) describe the use of “significance tests and dichotomized p-values” as an attempt by some practitioners to avoid dealing with uncertainty by escaping to a more simplistic world where results are either statistically significant or not. Perhaps a little uncharitable (though in an academic context I am more inclined to agree), but we’ve already discussed the importance of moving away from significance testing, so we should complete the journey and confront uncertainty head-on! We have to accept that findings are more uncertain than is often acknowledged. Our embrace of uncertainty should help us to seek out better methods, apply the necessary amount of care and critical thinking to our work, and draw inferences with the appropriate amount of confidence.\nOne way of acknowledging and working with this uncertainty is the use of confidence intervals when estimating effect sizes. Confidence intervals estimate a range of values that would be compatible with the data, given all of the statistical model’s assumptions are met. While confidence intervals are ultimately estimated from the p-value, and the typical 95% confidence interval corresponds to the p &gt; .05 threshold that I have warned against, they are useful because they frame results in terms of effect sizes, and they incorporate uncertainty in our estimates, helping to quantify the precision with which we are able to estimate effect size.\nConfidence intervals are a step in the right direction, because point estimates, while they are our best shot at the quantity of interest, are highly likely to be wrong. Estimating the interval within which we would expect the quantity of interest to fall, acknowledges that uncertainty, and there’s a better chance that the population parameter falls within the estimated range than bang on the point estimate. However, it’s important that we don’t treat confidence intervals as the fix for all uncertainty! Gelman (2016) states that “the solution is not to reform p-values or to replace them with some other statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty and embracing of variation”.\nUltimately, confidence intervals are valuable when estimating meaningful quantities, but there are very far from sufficient when it comes to embracing uncertainty. We have to acknowledge the uncertainty in the entire statistical modelling process, both when we are making decisions about that modelling process and when we present our findings to others."
  },
  {
    "objectID": "hypothesis_testing_r.html#next-steps",
    "href": "hypothesis_testing_r.html#next-steps",
    "title": "6  Hypothesis Testing",
    "section": "\n6.6 Next Steps",
    "text": "6.6 Next Steps\nThe goal for this chapter is that readers come away with an understanding of the significant limitations of null hypothesis significance testing and the more robust approaches to testing hypotheses, and the alternative approach of estimating meaningful effects. I hope that this chapter has helped set people up reasonably well when carrying out tests. Equally, I hope that they will consider the alternatives, recognising when it is defensible to carry out a statistical test and when estimating the effect size is necessary.\nI have taken a slightly more forgiving position than many of the statisticians that I respect, and from which I have learned so much, but I have deliberately done so because I think industry has slightly different incentives, and therefore slightly different requirements. I think that there are situations where the use of hypothesis testing is sufficient and defensible, and those situations are much more common in industry. The criticisms remain perfectly valid, but I hope I have given readers the tools to carry out more robust testing, and a good understanding of when to choose estimation instead, where necessary.\nWhile it is possible to use statistical tests for estimation, I also believe that the big wide world of estimation is better served by regression models. I think regression modelling is a better starting point than statistical tests because the structure of regression modelling places more emphasis on the effect size. Further, estimating effect sizes does not rule out the use of statistical tests, but it does require redirecting our focus and making sure to treat the magnitude and direction of effect size as the most important part of the analysis. The premise of statistical tests is that they test for the existence of an effect, and while packages like {infer} frame test outputs in a way that makes using statistical tests for these purposes, it does still require a little work to think about about the results of statistical tests in these terms. Further, I think that regression modelling gives a lot more flexibility when we are attempting to estimate an effect as precisely as possible, and the process and resulting outputs of a regression model lend themselves more naturally to a focus that steers away from hypothesis testing and towards estimating effects.\nIf it hasn’t been made clear before, I think it is important to stress the distinction between the null hypothesis testing framework and the statistical tests used to carry them out. The tests themselves are not really the problem. However, they are closely tied to null hypothesis testing. Ultimately, the statistical tests discussed in this chapter (and many more) are just special cases of linear models (Lindeløv 2019), and the results in both will be identical (Zablotski 2019) (if not the way the outputs are presented), so the same things can be achieved with either approach, but regression modelling offers more flexibility, allows for the layering of complexity/precision in estimates, and the way regression outputs are framed are generally a little more balanced. Treating regressions as the first point of call for inferential statistics is not a necessary prerequisite for “doing things the right way”, but doing things the right way becomes easier, because it is easier to frame everything in terms of the magnitude and direction of effects.\nThe good news? The next chapter is about modelling linear effects using regression!"
  },
  {
    "objectID": "hypothesis_testing_r.html#resources",
    "href": "hypothesis_testing_r.html#resources",
    "title": "6  Hypothesis Testing",
    "section": "\n6.7 Resources",
    "text": "6.7 Resources\n\n{infer}\nSeeing Theory\nThe Permutation Test\nThere is Only One Test\nMore Hypotheses, Less Trivia\nPermutation Tests\nPermutation Test as an Alternative to Two-Sample T-Test Using R\nPower Analysis\nRecommended Resources for A/B Testing\nMcElreath - None of the Above\nIntroduction to Scientific Programming and Simulations in R\n\n\n\n\n\n\n\nAschwanden, Christie. 2015. “Science Isn’t Broken: It’s Just a Hell of a Lot Harder Than We Give It Credit For.” FiveThirtyEight. https://fivethirtyeight.com/features/science-isnt-broken/.\n\n\n———. 2016. “Statisticians Found One Thing They Can Agree on: It’s Time to Stop Misusing p-Values.” FiveThirtyEight. https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/.\n\n\nBlackwell, Matthew. 2023. “A User’s Guide to Statistical Inference and Regression.” https://mattblackwell.github.io/gov2002-book/.\n\n\nFisher, Ronald A. 1956. “Statistical Methods and Scientific Inference.”\n\n\nGelman, Andrew. 2016. “The Problems with p-Values Are Not Just with p-Values.” The American Statistician 70. http://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nGelman, Andrew, and Eric Loken. 2021. “The Statistical Crisis in Science.” American Scientist 102 (6): 460. https://doi.org/10.1511/2014.111.460.\n\n\nGelman, Andrew, and Hal Stern. 2006. “The Difference Between \"Significant\" and \"Not Significant\" Is Not Itself Statistically Significant.” The American Statistician 60 (4): 328–31.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. “Statistical Tests, p-Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31: 337–50.\n\n\nLakens, Daniel. 2022. Improving Your Statistical Inferences. https://doi.org/10.5281/zenodo.6409077.\n\n\nLindeløv, Jonas Kristoffer. 2019. “Common Statistical Tests Are Linear Models.” https://lindeloev.github.io/tests-as-linear/.\n\n\nMcElreath, Richard. 2023. “None of the Above.” https://elevanth.org/blog/2023/07/17/none-of-the-above/.\n\n\nPoole, Charles. 2022. “The Statistical Arc of Epidemiology.” Presented at the \"What is the Value of the P-Value?\" Panel Discussion, Cosponsored by UNC TraCS, Duke University, and Wake Forest University CTSA Biostatistics, Epidemiology and Research Design (BERD) Cores.\n\n\nPopper, Karl. 1935. The Logic of Scientific Discovery. Routledge.\n\n\nPrytherch, Ben. 2022. DSCI 335: Inferential Reasoning in Data Analysis. https://bookdown.org/csu_statistics/dsci_335_spring_2022/.\n\n\nSinikumpu, Suvi-Päivikki, Jari Jokelainen, Juha Auvinena, Markku Timonen, and Laura Huilaja. 2021. “Association Between Psychosocial Distress, Sexual Disorders, Self-Esteem and Quality of Life with Male Androgenetic Alopecia: A Population-Based Study with Men at Age 46.” BMJ Open 11 (12).\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019. “Moving to a World Beyond \"p &lt; 0.05\".” The American Statistician 73 (sup1): 1–19. https://doi.org/10.1080/00031305.2019.1583913.\n\n\nZablotski, Yury. 2019. “Statistical Tests Vs. Linear Regression.” https://yury-zablotski.netlify.app/post/statistical-tests-vs-linear-regression/."
  },
  {
    "objectID": "hypothesis_testing_r.html#footnotes",
    "href": "hypothesis_testing_r.html#footnotes",
    "title": "6  Hypothesis Testing",
    "section": "",
    "text": "I could stubbornly refuse to write about hypothesis tests, like covering my eyes and ears and screaming as someone asks me how to do a t-test properly, but something tells me that isn’t going to stop them happening. I would rather give someone all the necessary resources (or at least as much as is possible in a relatively limited guide like this), and hope it helps them improve the way they go about asking questions of data.↩︎\nThis is in contrast to academia, where it is rare that anything other than precision will matter.↩︎\nI won’t discuss all 25 misinterpretations that Greenland et al. (2016) discuss, because that would be overkill here. For anyone that is interested in learning more about the problems with p-values, I’d recommend going straight to the source and reading their paper.↩︎\nThe probability that a p-value represents are specific to the statistical model we are specifying. It’s important to remember that violations of our assumptions in specifying our model can bias the p-value and invalidate our results.↩︎\nI’m assuming that anyone following this advice has good intentions, and is not looking to cheat their way to the results they want. Someone wanting to achieve this is going to find a way to do it regardless.↩︎\nA scientific hypothesis reflects a theory about the world (for example, Drug \\(X\\) decreases the risk of Disease \\(Y\\)) while a statistical hypothesis reflect what is observed in the data (A one-unit increase in \\(x\\) will correspond with a one-unit decrease in \\(y\\)).↩︎\nIt could be a hypothesis that captures a consequence of a scientific hypothesis being true (as opposed to false, in the case of the null hypothesis), or it can test two scientific hypotheses against each other (instead of one against the possibility of nothing going on) (Prytherch 2022).↩︎\nIf you are unclear on some of the foundations that underpin what I’m discussing here, I’d recommend starting with StatQuest’s Statistics Fundamentals YouTube playlist, and for a more detailed introduction, I would give Answering Questions With Data a try.↩︎\nStandard deviation can be computed from standard error by multiplying the standard error by the square root of the sample size. The formula for this is as follows: \\[\\sigma = SE \\times \\sqrt{n}\\]↩︎\nUsing only five observations is less than ideal, but given that what we are doing is just illustrative, this will work fine.↩︎\nWe have used the {faux} package to simulate this data because simulating two variables that are related to each other is a little more involved.↩︎\nIt is also worth noting that while the guidance here is about the estimation approach to statistical inference, it can also be applied to hypothesis testing too.↩︎"
  },
  {
    "objectID": "linear_regression_r.html#exploratory-data-analysis",
    "href": "linear_regression_r.html#exploratory-data-analysis",
    "title": "7  Linear Regression",
    "section": "\n7.1 Exploratory Data Analysis",
    "text": "7.1 Exploratory Data Analysis\nFirst we will carry out some exploratory data analysis (EDA) to get a feel for the data, and to see if there are any obvious relationships between the variables that we can use to inform our model.\nWe will use the dplyr package’s glimpse() function to get a quick overview of the data, before using the psych package’s describe() function to calculate summary statistics. The describe() function can compute a wide range of summary statistics, but we will only use a few of them here (the select() function below indicates the summary statistics that will be removed).\nThe knitr package’s kable() function is used to format the table output to make it easier to read in HTML format.\n\n# get a quick overview of the data\nglimpse(deprivation)\n\nRows: 583\nColumns: 7\n$ area_code           &lt;chr&gt; \"E06000001\", \"E06000001\", \"E06000001\", \"E06000001\"…\n$ year                &lt;dbl&gt; 2015, 2016, 2017, 2018, 2015, 2016, 2017, 2018, 20…\n$ life_expectancy     &lt;dbl&gt; 78.73839, 79.07321, 79.08000, 78.81000, 77.78726, …\n$ imd_score           &lt;dbl&gt; 35.037, 35.037, 35.037, 35.037, 40.460, 40.460, 40…\n$ imd_decile          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 4, 4, 4, 4, 4,…\n$ physiological_score &lt;dbl&gt; 89.9, 88.8, 84.8, 84.6, 96.4, 95.8, 96.7, 96.2, 87…\n$ behavioural_score   &lt;dbl&gt; 84.9, 84.4, 82.7, 81.0, 80.3, 82.6, 84.5, 82.2, 96…\n\n\n\ndeprivation |&gt; \n  select(!year) |&gt;\n  janitor::clean_names(case = \"title\", abbreviations = c(\"IMD\")) |&gt; \n  modelsummary::datasummary_skim(align = \"lccddddd\", output = \"gt\")\n\n\n\n\n\n\nTable 7.1:  Summary statistics \n  \n \n      Unique (#)\n      Missing (%)\n      Mean\n      SD\n      Min\n      Median\n      Max\n    \n\n\nLife Expectancy\n562\n0\n81.2 \n 1.6 \n76.5 \n 81.2 \n 86.0 \n\n\nIMD Score\n146\n0\n23.2 \n 8.0 \n 5.8 \n 23.0 \n 45.0 \n\n\nIMD Decile\n10\n0\n 5.4 \n 2.8 \n 1.0 \n  5.0 \n 10.0 \n\n\nPhysiological Score\n294\n0\n99.0 \n10.0 \n78.2 \n 97.9 \n125.9 \n\n\nBehavioural Score\n285\n0\n99.4 \n 8.7 \n72.4 \n100.6 \n120.0 \n\n\n\n\n\n\n\nHaving taken a quick look at the data, we can now start to explore how life expectancy, our outcome variable (the variable that we are trying to explain), varies with the other variables in our dataset (the explanatory variables). We will start by plotting how life expectancy is distributed by IMD score, which is a measure of deprivation. The expectation is that there will be a negative association between the two variables, which means that areas with higher levels of deprivation (i.e. lower IMD scores) will have lower life expectancies.\n\ndeprivation |&gt;\n  ggplot(aes(life_expectancy, imd_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = \"grey50\") +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour=\"#005EB8\") +\n  labs(x = \"Life Expectancy\", y = \"IMD Score\")\n\n\n\nFigure 7.1: Plotting the relationship between life expectancy and IMD scores\n\n\n\nThere is a clear negative association between life expectancy and IMD score, which is what we would expect. However, there are some outliers where life expectancy seems to be higher than would be expected given the level of deprivation. The plot seems to suggest that deprivation may have a ‘floor effect’ on life expectancy, i.e. that higher deprivation raises the floor of life expectancy, but that there are other factors that can raise life expectancy above the floor.\nWe can also plot the distribution of life expectancy by the Health Index scores for physiological and behavioural risk factors. These scores are index values that measure the prevalence of physiological and behavioural risk factors that are associated with poor health outcomes, against a baseline value of the national average for each risk factor in 2015. This baseline score is 100, and higher scores mean that an area has a higher prevalence of that risk factor than the national average in 2015, and lower scores mean that the area has lower prevalence. The expectation is that when the risk factor index scores increase, meaning that the risk factors are less prevalent compared against the national average, life expectancy will increase, while a decrease in the scores will be associated with a decrease in life expectancy.\n\ndeprivation |&gt;\n  ggplot(aes(life_expectancy, physiological_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = \"grey50\") +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour = \"#005EB8\") +\n  labs(x = \"Life Expectancy\", y = \"Physiological Index\")\n  \n\ndeprivation |&gt;\n  ggplot(aes(life_expectancy, behavioural_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = \"grey50\") +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour = \"#005EB8\") +\n  labs(x = \"Life Expectancy\", y = \"Behavioural Index\")\n\n\n\n\n\n(a) Physiological Risk Factor Score\n\n\n\n\n\n(b) Behavioural Risk Factor Score\n\n\n\nFigure 7.2: Plotting the relationship between life expectancy and the Health Index’s physiological and behavioural risk factor scores\n\n\n\nThe plots show that there is a clear negative association between life expectancy and both the physiological and behavioural risk factor scores, which is what we would expect. However, the plots also show that there are some outliers where life expectancy seems to be higher than would be expected given the level of risk factors. The plots seem to suggest that risk factors may have a ‘floor effect’ on life expectancy, i.e. that higher risk factors raise the floor of life expectancy, but that there are other factors that can raise life expectancy above the floor.\nThe above plots suggest there are strong correlations between life expectancy and the three independent variables. We can confirm this by calculating a correlation matrix for the variables in our dataset, using the correlation package.\n\ndeprivation |&gt; \n  group_by(area_code) |&gt;\n  summarise_all(.funs = \"mean\") |&gt; \n  select(\n    life_expectancy, \n    imd_score, \n    behavioural_score, \n    physiological_score\n    ) |&gt;\n  janitor::clean_names(case = \"title\", abbreviations = c(\"IMD\")) |&gt; \n  modelsummary::datasummary_correlation(output = \"gt\")\n\n\n\n\n\n\nTable 7.2:  Correlations between life expectancy, deprivation, and socioeconomic\nfactors \n  \n \n      Life Expectancy\n      IMD Score\n      Behavioural Score\n      Physiological Score\n    \n\n\nLife Expectancy\n1\n.\n.\n.\n\n\nIMD Score\n-.84\n1\n.\n.\n\n\nBehavioural Score\n.91\n-.83\n1\n.\n\n\nPhysiological Score\n.60\n-.39\n.62\n1\n\n\n\n\n\n\n\nThere are strong correlations between life expectancy and the three explanatory variables, however, the correlation between IMD score and the two Health Index scores is also relatively strong, and in the case of behavioural risk factors, it is very strong. This could be an issue, as it could mean that the model is not able to distinguish between the effects of deprivation and the effects of risk factors on life expectancy. This is known as multicollinearity, and it can cause problems when interpreting the results of a regression model.\nWe can also visualise the correlation between our explanatory variables, to get a better sense of how they might be related to each other. We will start by plotting IMD score against the two Health Index scores, using scatter plots, before plotting the distributions of the scores by IMD decile, using density plots. We can use the ggridges package to plot the density plots, which can be useful for plotting how continuous variables are distributed across discrete groups.\n\ndeprivation |&gt;\n  ggplot(aes(imd_score, physiological_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = \"grey50\") +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour = \"#005EB8\") +\n  labs(x = \"IMD Score\", y = \"Physiological Index\")\n\ndeprivation |&gt;\n  ggplot(aes(imd_score, behavioural_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = \"grey50\") +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour = \"#005EB8\") +\n  labs(x = \"IMD Score\", y = \"Behavioural Index\")\n\ndeprivation |&gt;\n  mutate(imd_decile = as.factor(imd_decile)) |&gt;\n  ggplot(aes(physiological_score, imd_decile)) +\n  ggridges::geom_density_ridges() +\n  labs(x = \"IMD Decile\", y = \"Physiological Index\")\n\ndeprivation |&gt;\n  mutate(imd_decile = as.factor(imd_decile)) |&gt;\n  ggplot(aes(behavioural_score, imd_decile)) +\n  ggridges::geom_density_ridges() +\n  labs(x = \"IMD Decile\", y = \"Behavioural Index\")\n\n\n\n\n\n(a) IMD & Physiological Score\n\n\n\n\n\n(b) IMD & Behavioural Score\n\n\n\n\n\n\n\n(c) IMD Decile & Physiological Score\n\n\n\n\n\n(d) IMD Decile & Behavioural Score\n\n\n\nFigure 7.3: Plotting the relationship between IMD scores/deciles and the Health Index’s physiological and behavioural risk factor scores\n\n\n\nThe correlations are a little more obvious when visualised. The behavioural index scores have a negative linear association with IMD scores (and a positive linear association with IMD decile). This means that areas with higher level of deprivation also tend to have a higher prevalence of behavioural risk factors that are associated with poor health outcomes. The physiological index scores have a similar association with deprivation, but the relationship is not as strong as the behavioural risk factors, and there is significant variance around the linear trend."
  },
  {
    "objectID": "linear_regression_r.html#linear-regression",
    "href": "linear_regression_r.html#linear-regression",
    "title": "7  Linear Regression",
    "section": "\n7.2 Linear Regression",
    "text": "7.2 Linear Regression\nFirst we will transform each of the explanatory variables to make them a little easier to interpret (particularly with regard to the intercept). If we don’t transform the variables, the intercept will be based on zero values of each explanatory variable, which is not very meaningful. For example, a zero value of either risk factors variable is effectively meaningless because the ONS Health Index is centred around a baseline value of 100 (which is the average value for England in 2015, the first year for which the Health Index was calculated).\nThese transformations won’t impact the regression results, but will just make the results easier to interpret and explain, as the intercept is no longer based on zero values of each explanatory variable.\n\n# normalise independent variables\nnormalised_data &lt;- \n  deprivation |&gt; \n  mutate(\n    # subtract mean and divide by standard deviation\n    across(ends_with(\"_score\"), ~ (.x - mean(.x))/sd(.x))\n    )\n\n# check the transformations\nglimpse(normalised_data)\n\nRows: 583\nColumns: 7\n$ area_code           &lt;chr&gt; \"E06000001\", \"E06000001\", \"E06000001\", \"E06000001\"…\n$ year                &lt;dbl&gt; 2015, 2016, 2017, 2018, 2015, 2016, 2017, 2018, 20…\n$ life_expectancy     &lt;dbl&gt; 78.73839, 79.07321, 79.08000, 78.81000, 77.78726, …\n$ imd_score           &lt;dbl&gt; 1.4739156, 1.4739156, 1.4739156, 1.4739156, 2.1481…\n$ imd_decile          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 4, 4, 4, 4, 4,…\n$ physiological_score &lt;dbl&gt; -0.9104599, -1.0203229, -1.4198247, -1.4397998, -0…\n$ behavioural_score   &lt;dbl&gt; -1.6664775, -1.7239620, -1.9194091, -2.1148563, -2…\n\n\nHaving transformed the variables, we can now fit a linear regression model to the data. We will use the lm() function from the Base R stats package. Constructing a linear regression in R is very simple, and the syntax is relatively intuitive. The first argument in the function is the formula, which is the dependent variable on the left-hand side, and the independent variables on the right-hand side, separated by a ~ symbol. The second required argument is the data argument, which gives the lm() function a dataset to fit the regression to. There are a number of optional arguments that can be passed to the function to control the model fitting process, but we will use the default values for now.\nAfter fitting the regression model, we can call the summary() function to get a summary of the model results. This will give us the model coefficients, standard errors, t-statistics, p-values, and \\(R^2\\) value. However, in this instance we will use the sjPlot::tab_model() function to get a nice table of the model results which renders better in HTML.\n\n# fit linear regression\nmodels &lt;- list(\n  \"OLS (Raw)\" = \n    lm(life_expectancy ~ imd_score + physiological_score +\n         behavioural_score, data = deprivation),\n  \"OLS (Normalised)\" =\n    lm(life_expectancy ~ imd_score + physiological_score + \n         behavioural_score, data = normalised_data)\n  )\n\n# get model results table\nmodels |&gt; \n  modelsummary::modelsummary(\n    estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n    statistic = \"p = {p.value}\",\n    coef_rename = c(\n      imd_score = \"IMD Score\",\n      physiological_score = \"Physiological Score\",\n      behavioural_score = \"Behavioural Score\"\n      ),\n    gof_map = c(\"nobs\", \"r.squared\",  \"rmse\"),\n    output = \"gt\"\n  ) |&gt; \n  gt::tab_source_note(\n    source_note = \"Source: PHE Fingertips/ONS Health Index\"\n    )\n\n\n\n\n\n\nTable 7.3:  Regression model estimating the effect of socioeconomic factors on\nlife expectancy \n  \n \n      OLS (Raw)\n      OLS (Normalised)\n    \n\n\n(Intercept)\n71.592 [70.367, 72.816]\n81.167 [81.115, 81.219]\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\nIMD Score\n-0.077 [-0.089, -0.066]\n-0.623 [-0.713, -0.533]\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\nPhysiological Score\n0.024 [0.017, 0.030]\n0.239 [0.173, 0.304]\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\nBehavioural Score\n0.091 [0.079, 0.103]\n0.788 [0.685, 0.892]\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\nNum.Obs.\n583\n583\n\n\nR2\n0.844\n0.844\n\n\nRMSE\n0.64\n0.64\n\n\n\nSource: PHE Fingertips/ONS Health Index"
  },
  {
    "objectID": "linear_regression_r.html#evaluating-and-interpreting-regression-models",
    "href": "linear_regression_r.html#evaluating-and-interpreting-regression-models",
    "title": "7  Linear Regression",
    "section": "\n7.3 Evaluating and Interpreting Regression Models",
    "text": "7.3 Evaluating and Interpreting Regression Models\nWhen we fit a regression model, we have to consider both how well the model fits the data (evaluation) and the influence that each explanatory variable is having on the outcome (interpretation). The evaluation process is making sure that the model is doing a good enough job that we can reasonably draw some conclusions from it, while the interpretation process is understanding the conclusions we can draw. It’s important to understand the purpose that both of these processes serve, and to understand that the evaluation process is just kicking the tires, while what we are ultimately concerned with is the interpretation.\nThere are several elements to consider when interpreting the results of a linear regression model. Broadly speaking the focus should be on the magnitude and the precision of the model coefficients. The magnitude of the coefficients indicates the strength of the association between the explanatory variables and the outcome variable, while the precision of the coefficients indicates the uncertainty in the estimated coefficients. We use the regression model coefficients to understand the magnitude of the effect and standard errors (and confidence intervals) to understand the precision of the effect.\n\n7.3.1 Model Evaluation\nThere are a number of ways we might evaluate model performance, but the starting point should be the model’s \\(R^2\\) and the p-values of the model coefficients. These can help us consider whether the model fits the data and whether we have enough data to trust the conclusions we can draw from each explanatory variable.\n\n7.3.1.1 \\(R^2\\)\n\nThe \\(R^2\\) value is the proportion of the variance in the outcome that is explained by the model. It is often used as a measure of the goodness of fit of the model, and by extension the extent to which the specified model explains the outcome, but caution should be applied when using \\(R^2\\) this way, because \\(R^2\\) can be misleading and doesn’t always tell the whole story. One of the main problems with \\(R^2\\) when used in an inferential context is that adding more variables to the model will always increase the \\(R^2\\) value, even if the new variables are not actually related to the outcome. Adjusted \\(R^2\\) values are used to account for this problem, but they don’t resolve all issues with \\(R^2\\), so it is important to consider \\(R^2\\) as just one of a number of indicators of the model’s fit. If the \\(R^2\\) is high, this doesn’t necessarily mean that the model is a good fit for the data, but if the \\(R^2\\) is particularly low, this might be a good indication that the model is not a good fit for the data, and at the very least it is good reason to be cautious about the results of the analysis, and to take a closer look at the model and the data to understand why the \\(R^2\\) is so low.\nDefining high/low \\(R^2\\) values is dependent on the context of the analysis. The theory behind the model should inform our expectations about the \\(R^2\\) value, and we should also consider the number of explanatory variables in the model (adding variables to a regression will increase the \\(R^2\\)). If the theory suggests that the model should explain a small amount of the variance in the outcome, and the model has only a few explanatory variables, then a low \\(R^2\\) value might not be a cause for concern.\nThe Adjusted \\(R^2\\) value for the above regression model is 0.843, which means that the model explains 84.3% of the observed variance in life expectancy in our dataset. This is a relatively high \\(R^2\\) value.\n\n7.3.1.2 P-Values\nP-values are the probability of observing a coefficient as large as the estimated coefficient for a particular explanatory variable, given that the null hypothesis is true (i.e. that the coefficient is equal to zero), indicating that no effect is present. This can be a little tricky to understand at first, but a slightly crude way of thinking about this is that p-values are an estimate of the probability that the model coefficient you observe is actually distinguishable from zero.\nP-values are used to test the statistical significance of the coefficients. If the p-value is less than the significance level then the coefficient is statistically significant, and we can reject the null hypothesis that the coefficient is equal to zero. If the p-value is greater than the significance level, then the coefficient is not statistically significant, and we cannot reject the null hypothesis that the coefficient is equal to zero. The significance level is often set at 0.05, which means that we are willing to accept a 5% chance of incorrectly rejecting the null hypothesis. This means that if the p-value is less than 0.05, then there is a greater than 95% probability that the coefficient is not equal to zero. However, the significance level is a subjective decision, and can be set at any value. For example, if the significance level is set at 0.01, then we are willing to accept a 1% chance of incorrectly rejecting the null hypothesis.\nIn recent years p-values (and the wider concept of statistical significance) have been the subject of a great deal of discussion in the scientific community, because there is a belief among many practitioners that a lot of scientific research places too much weight in the p-value, treating it as the most important factor when interpreting a regression model. I think this perspective is correct, but I think dismissing p-values entirely is a mistake too. The real problem is not with p-values themselves, but with the goal of an analysis being the identification of a statistically significant effect.\nI think the best way to think about p-values is the position prescribed by Gelman, Hill, and Vehtari (2020). Asking whether our coefficient is distinguishable from zero is the wrong question. As analysts, we should typically know enough about the context we are studying to build regression models with explanatory variables that will some effect. The question is how much. A statistically insignificant model coefficient is less a sign that a variable has no effect, and more a sign that there is insufficient data to detect a meaningful effect. If our model coefficients are not statistically significant, it is simply a good indication that we need more data. If the model coefficients are statistically significant, then our focus should be on interpreting the coefficients, not on the p-value itself.\n\n7.3.2 Model Interpretation\nModel interpretation is the process of understanding the influence that each explanatory variable has on the outcome. This involves understanding the direction, the magnitude, and the precision of the model coefficients. The coefficients themselves will tell us about direction and magnitude, while the standard errors and confidence intervals will tell us about precision.\n\n7.3.2.1 Model Coefficients\nThe model coefficients are the estimated values of the regression coefficients, or the estimated change in the outcome variable for a one unit change in the explanatory variable, while holding all other explanatory variables constant. The intercept is the estimated value of the outcome variable when all of the explanatory variables are equal to zero (or the baseline values if the variables are transformed).\nA regression’s coefficients are the most important part of interpreting the model. They specify the association between the explanatory variables and the outcome, telling us both the direction and the magnitude of the association. The direction of the association is indicated by the sign of the coefficient (positive = outcome increases with the explanatory variable, negative = outcome decreases with the explanatory variable). The magnitude of the association is indicated by the size of the coefficient (the larger the coefficient, the stronger the association). If the direction of the association is going in the wrong direction to that which we expect, this obviously indicates issues either in the theory or in the model, and if the magnitude of the effect is too small to be practically significant, then either the explanatory variable is not a good predictor of the outcome, or the model is not doing a good job of fitting the data (either the explanatory variable doesn’t really matter or the model is not capturing the true relationship between the explanatory variables and the outcome). However, while the direction is relatively easy to interpret, magnitude is not, because what counts as a practically significant effect is dependent on the context of the analysis.\n\n7.3.2.2 Standard Errors\nThe standard errors are the standard deviation of the estimated coefficients. This is a measure of the precision of the estimated association between the explanatory variables and the outcome. The larger the standard error, the less precise the coefficient estimate. Precisely measured coefficients suggest that the model is fitting the data well, and we can be confident that the association between the explanatory variables and the outcome is real, and not just a result of random variation in the data. Defining what counts as ‘precise’ in any analysis, much like the coefficient magnitude, is dependent on the context of the analysis. However, in general, a standard error of less than 0.5 is considered to be a good estimate, and a standard error of less than 0.25 is considered to be a very precise estimate.\nIt is important to recognise that a precise coefficient alone cannot be treated as evidence of a causal effect between the explanatory variable and the outcome but it is, at least, a good indication that the explanatory variable is a good predictor of the outcome. In order to establish a causal relationship, we may consider methods that are more robust to the presence of confounding or omitted variables, the gold standard being randomised controlled trials, but with a strong theoretical basis and a good understanding of the data, we can make a good case for a causal relationship based on observational data."
  },
  {
    "objectID": "linear_regression_r.html#next-steps",
    "href": "linear_regression_r.html#next-steps",
    "title": "7  Linear Regression",
    "section": "\n7.4 Next Steps",
    "text": "7.4 Next Steps"
  },
  {
    "objectID": "linear_regression_r.html#resources",
    "href": "linear_regression_r.html#resources",
    "title": "7  Linear Regression",
    "section": "\n7.5 Resources",
    "text": "7.5 Resources\n\nMLU Explain: Linear Regression\nStatQuest: Linear Regression\nRegression & Other Stories\nModernDive: Statistical Inference via Data Science\nA User’s Guide to Statistical Inference and Regression\n\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari Vehtari. 2020. Regression and Other Stories. Cambridge University Press."
  },
  {
    "objectID": "ml_workflow_r.html#data",
    "href": "ml_workflow_r.html#data",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "\n8.1 Data",
    "text": "8.1 Data\n\n# inspect the data\nglimpse(df)\n\nRows: 918\nColumns: 10\n$ age                &lt;dbl&gt; 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39,…\n$ sex                &lt;fct&gt; M, F, M, F, M, M, F, M, M, F, F, M, M, M, F, F, M, …\n$ resting_bp         &lt;dbl&gt; 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 1…\n$ cholesterol        &lt;dbl&gt; 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 2…\n$ fasting_bs         &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ resting_ecg        &lt;fct&gt; Normal, Normal, ST, Normal, Normal, Normal, Normal,…\n$ max_hr             &lt;dbl&gt; 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 14…\n$ angina             &lt;fct&gt; N, N, N, Y, N, N, N, N, Y, N, N, Y, N, Y, N, N, N, …\n$ heart_peak_reading &lt;dbl&gt; 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0…\n$ heart_disease      &lt;fct&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n\n\n\n8.1.1 Train/Test Split\nOne of the central tenets of machine learning is that the model should not be trained on the same data that it is evaluated on. This is because the model could learn spurious/random patterns and correlations in the training data, and this will harm the model’s ability to make good predictions on new, unseen data. There are many way of trying to resolve this, but the most simple approach is to split the data into a training and test set. The training set will be used to train the model, and the test set will be used to evaluate the model.\n\n# split data into train/test sets\ntrain_test_split &lt;-\n  rsample::initial_split(df,\n                         strata = heart_disease,\n                         prop = 0.7)\n\n# extract training and test sets\ntrain_df &lt;- rsample::training(train_test_split)\ntest_df &lt;- rsample::testing(train_test_split)\n\n\n8.1.2 Cross-Validation\nOn top of the train/test split, we will also use cross-validation to train our models. Cross-validation is a method of training a model on a subset of the data, and then evaluating the model on the remaining data. This process is repeated multiple times, and the average performance is used to evaluate the model. This helps make the training process more generalisable.\nFor more information on cross-validation and how it can be used to train models Kuhn and Silge (2023)’s Resampling Methods discusses cross-validation in detail, in the wider context of resampling methods for training machine learning models. This helps to provide context for the purpose of cross-validation, as well as discussing some of the strategies for cross-validation.\nWe will use 5-fold stratified cross-validation to train our models. This means that the training data will be split into 5 folds, ensuring that each fold has the same proportion of the target classes. Each fold will be used as a test set once, and the remaining folds will be used as training sets. This will be repeated 5 times, and the average performance will be used to evaluate the model.\n\n# create cross-validation folds\ntrain_folds &lt;- vfold_cv(train_df, v = 5, strata = heart_disease)\n\n# inspect the folds\ntrain_folds\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [513/129]&gt; Fold1\n2 &lt;split [513/129]&gt; Fold2\n3 &lt;split [514/128]&gt; Fold3\n4 &lt;split [514/128]&gt; Fold4\n5 &lt;split [514/128]&gt; Fold5\n\n\n\n8.1.3 Preprocessing\nThe purpose of preprocessing is to prepare the data for model fitting. This can take a number of different forms, but some of the most common preprocessing steps include:\n\nNormalising/Standardising data\nOne-hot encoding categorical variables\nRemoving outliers\nImputing missing values\n\nWe will build two different preprocessing recipes and see which performs better. The first will be a ‘basic’ recipe that one-hot encodes all categorical features, and removes any features that have strong correlation with another feature in the dataset. The second recipe will include both these steps but will also normalise the numeric features so that they have a mean of zero and a standard deviation of one.\n\n# specify simple preprocessing recipe\nbasic_recipe &lt;- recipe(heart_disease ~ ., data = train_df) |&gt;\n  # one-hot encode categorical variables\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;\n  # remove features that have strong correlation with another feature\n  step_corr(all_numeric_predictors(), threshold = .5)\n\n# specify scaled preprocessing recipe\nscaled_recipe &lt;- recipe(heart_disease ~ ., data = train_df) |&gt;\n  # normalise data so that it has mean zero and sd one\n  step_normalize(all_numeric_predictors()) |&gt; \n  # one-hot encode categorical variables\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;\n  # remove features that have strong correlation with another feature\n  step_corr(all_numeric_predictors(), threshold = .5)\n\n# inspect basic preprocessing recipe\nbasic_recipe |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 642 × 11\n     age resting_bp cholesterol max_hr heart_peak_reading heart_disease sex_M\n   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1    37        130         283     98                0   0                 1\n 2    39        120         339    170                0   0                 1\n 3    45        130         237    170                0   0                 0\n 4    48        120         284    120                0   0                 0\n 5    39        120         204    145                0   0                 1\n 6    42        115         211    137                0   0                 0\n 7    54        120         273    150                1.5 0                 0\n 8    43        100         223    142                0   0                 0\n 9    44        120         184    142                1   0                 1\n10    40        130         215    138                0   0                 1\n# ℹ 632 more rows\n# ℹ 4 more variables: fasting_bs_X1 &lt;dbl&gt;, resting_ecg_LVH &lt;dbl&gt;,\n#   resting_ecg_ST &lt;dbl&gt;, angina_Y &lt;dbl&gt;\n\n# inspect scaled preprocessing recipe\nscaled_recipe |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 642 × 11\n       age resting_bp cholesterol  max_hr heart_peak_reading heart_disease sex_M\n     &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1 -1.74       -0.133      0.753  -1.52               -0.833 0                 1\n 2 -1.53       -0.666      1.25    1.31               -0.833 0                 1\n 3 -0.900      -0.133      0.341   1.31               -0.833 0                 0\n 4 -0.585      -0.666      0.762  -0.656              -0.833 0                 0\n 5 -1.53       -0.666      0.0457  0.328              -0.833 0                 1\n 6 -1.21       -0.933      0.108   0.0134             -0.833 0                 0\n 7  0.0436     -0.666      0.663   0.525               0.604 0                 0\n 8 -1.11       -1.73       0.216   0.210              -0.833 0                 0\n 9 -1.00       -0.666     -0.133   0.210               0.125 0                 1\n10 -1.42       -0.133      0.144   0.0528             -0.833 0                 1\n# ℹ 632 more rows\n# ℹ 4 more variables: fasting_bs_X1 &lt;dbl&gt;, resting_ecg_LVH &lt;dbl&gt;,\n#   resting_ecg_ST &lt;dbl&gt;, angina_Y &lt;dbl&gt;"
  },
  {
    "objectID": "ml_workflow_r.html#model-training",
    "href": "ml_workflow_r.html#model-training",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "\n8.2 Model Training",
    "text": "8.2 Model Training\nThere are many different types of models that can be used for classification problems, and selecting the right model for the problem at hand can be difficult when you are first starting out with machine learning.\nSimple models like linear and logistic regressions are often a good place to start and can be used to get a better understanding of the data and the problem at hand, and can give you a good idea of baseline performance before building more complex models to improve performance. Another example of a simple model is K-Nearest Neighbours (KNN), which is a non-parametric model that can be used for both classification and regression problems.\nWe will fit a logistic regression and a KNN, as well as fitting a Random Forest model, which is a good example of a slightly more complex model that will often perform well on structured data.\n\n8.2.1 Model Selection\n\n# create a list of preprocessing recipes\npreprocessers &lt;-\n  list(\n    basic = basic_recipe,\n    scaled = scaled_recipe\n  )\n\n# create a list of candidate models\nmodels &lt;- list(\n  # logistic regression\n  log = \n    logistic_reg(\n      # tune regularisation parameters\n      penalty = tune(),\n      mixture = tune()\n    ) |&gt;\n    set_engine('glmnet') |&gt;\n    set_mode('classification'),\n  # k-nearest neighbours\n  knn = \n    nearest_neighbor(\n      # tune weighting function and number of neighbours\n      weight_func = tune(),\n      neighbors = tune()\n      ) |&gt;\n    set_engine('kknn') |&gt;\n    set_mode('classification'),\n  # random forest\n  rf =\n    rand_forest(\n      # number of trees\n      trees = 1000,\n      # tune number of features to consider at each split\n      # and minimum number of observations in a leaf\n      mtry = tune(),\n      min_n = tune()\n      ) |&gt;\n    set_engine('ranger') |&gt;\n    set_mode('classification')\n  )\n\n# combine these lists as a workflow set\nmodel_workflows &lt;-\n  workflow_set(\n    preproc = preprocessers,\n    models = models\n  )\n\n# specify the metrics on which the models will be evaluated\neval_metrics &lt;- metric_set(f_meas, accuracy)\n\nHaving defined the preprocessing recipes and candidate models, we can now train and tune the models. We will use the tune_grid() function to carry out hyperparameter tuning1, which will search over a grid of hyperparameter values to find the best performing model. We will use 5-fold cross-validation to train the models, and will evaluate the models using F1 score and accuracy.\n\n# train and tune models\ntrained_models &lt;- \n  model_workflows |&gt;\n  workflow_map(\n    # function to use for hyperparameter tuning\n    fn = 'tune_grid',\n    # cv folds for resampling\n    resamples = train_folds,\n    # grid of hyperparameter values to search over\n    grid = 10,\n    # metrics to evaluate model performance\n    metrics = eval_metrics,\n    verbose = TRUE,\n    seed = 123,\n    # control parameters for tuning\n    control = control_grid(\n      save_pred = TRUE,\n      parallel_over = 'everything',\n      save_workflow = TRUE)\n    )\n\n\n8.2.1.1 Comparing Model Performance\nOnce the models have been trained we can inspect the results to see which model performed best. We can also plot the performance of the models to get a better idea of how they performed.\n\n# inspect the best performing models\ntrained_models |&gt;\n  rank_results(select_best=TRUE) |&gt; \n  filter(.metric == 'f_meas') |&gt; \n  select(wflow_id, model, f1 = mean)\n\n# A tibble: 6 × 3\n  wflow_id   model               f1\n  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n1 basic_rf   rand_forest      0.786\n2 scaled_rf  rand_forest      0.784\n3 scaled_knn nearest_neighbor 0.771\n4 basic_knn  nearest_neighbor 0.771\n5 basic_log  logistic_reg     0.770\n6 scaled_log logistic_reg     0.770\n\n\n\nmetric_labels = c(\"accuracy\" = \"Accuracy\", \"f_meas\" = \"F1\")\n\n# plot performance\ntrained_models |&gt; \n  autoplot() +\n  facet_wrap(~ .metric, labeller = as_labeller(metric_labels)) +\n  scale_shape(guide = 'none') +\n  scwplot::scale_colour_qualitative(\n    labels = c(\"Logistic Regression\", \"KNN\", \"Random Forest\"),\n    palette = \"scw\"\n    ) +\n  guides(colour = guide_legend(override.aes = list(size=2, linewidth = .8)))\n\n# select best models and plot performance\ntrained_models |&gt; \n  autoplot(select_best = TRUE) +\n  facet_wrap(~ .metric, labeller = as_labeller(metric_labels)) +\n  labs(y = NULL) +\n  scale_shape(guide = 'none') +\n  scwplot::scale_colour_qualitative(\n    labels = c(\"Logistic Regression\", \"KNN\", \"Random Forest\"),\n    palette = \"scw\"\n    ) +\n  guides(colour = guide_legend(override.aes = list(size=2, linewidth = .8)))\n\n\n\n\n\n(a) All Models\n\n\n\n\n\n(b) Best Models\n\n\n\nFigure 8.1: Comparing accuracy and f1 score of Random Forest, KNN, and Logistic Regression models"
  },
  {
    "objectID": "ml_workflow_r.html#finalising-model",
    "href": "ml_workflow_r.html#finalising-model",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "\n8.3 Finalising Model",
    "text": "8.3 Finalising Model\n\n# save best performing model\nbest_results &lt;-\n   trained_models |&gt;  \n   extract_workflow_set_result('basic_rf') |&gt;  \n   select_best(metric = 'f_meas')\n\nbest_results\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     3    30 Preprocessor1_Model01"
  },
  {
    "objectID": "ml_workflow_r.html#fitting-model-on-test-data",
    "href": "ml_workflow_r.html#fitting-model-on-test-data",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "\n8.4 Fitting Model on Test Data",
    "text": "8.4 Fitting Model on Test Data\nHaving selected the best performing model, we can now fit the model to the test data.\n\n# fit model on test data\nrf_test_results &lt;-\n   trained_models |&gt;\n   # extract the best performing model\n   extract_workflow('basic_rf') |&gt;\n   finalize_workflow(best_results) |&gt;\n   # fit to test data and evaluate performance\n   last_fit(split = train_test_split, metrics=eval_metrics)"
  },
  {
    "objectID": "ml_workflow_r.html#model-evaluation",
    "href": "ml_workflow_r.html#model-evaluation",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "\n8.5 Model Evaluation",
    "text": "8.5 Model Evaluation\nFinally, we can inspect the performance of the model on the test data.\n\n# inspect model performance on evaluation metrics\ncollect_metrics(rf_test_results)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 f_meas   binary         0.835 Preprocessor1_Model1\n2 accuracy binary         0.848 Preprocessor1_Model1\n\n# inspect confusion matrix\nrf_test_results |&gt; \n  conf_mat_resampled(tidy=FALSE)\n\n    0   1\n0 106  25\n1  17 128\n\n\n\n# plot confusion matrix\nrf_test_results |&gt;\n  conf_mat_resampled(tidy=FALSE) |&gt; \n  autoplot(type='heatmap')\n\n\n\nFigure 8.2: Confusion matrix visualising model performance, comparing predicted and actual values"
  },
  {
    "objectID": "ml_workflow_r.html#next-steps",
    "href": "ml_workflow_r.html#next-steps",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "\n8.6 Next Steps",
    "text": "8.6 Next Steps\nIn this post, we have covered a simple but robust machine learning workflow. We have have used the tidymodels package to fit a logistic regression model, a k-nearest neighbours model, and a random forest model to the heart disease dataset. We used cross-validation to make the results more generalisable, and we used hyperparameter tuning to improve model performance.\nThe next steps would be to consider what strategies for cross-validation would be most appropriate for the model we are building, what hyperparameter tuning process would produce the best performance, and some more complex algorithms that might outperform the models we’ve built here."
  },
  {
    "objectID": "ml_workflow_r.html#resources",
    "href": "ml_workflow_r.html#resources",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "\n8.7 Resources",
    "text": "8.7 Resources\nThere are lots of great (and free) introductory resources for machine learning:\n\n\nMachine Learning University (Python)\nMLU Explain\n\nMachine Learning for Beginners (Python/R)\n\nFor a guided video course, Data Talks Club’s Machine Learning Zoomcamp (available on GitHub and YouTube) is well-paced and well-presented, covering a variety of machine learning methods and even covering some of the aspects that introductory course often skip over, like deployment and data engineering principles. However, while the ML Zoomcamp course is intended as an introduction to machine learning, it does assume a certain level of familiarity with programming (the course uses Python) and software engineering. The appendix section of the course is definitely helpful for bridging some of the gaps in the course, but it is still worth being aware of the way the course is structured.\nIf you are keen to learn more about machine learning but are particularly focused on (or already have experience in) R, the ML Zoomcamp may not be the best fit. If you want to learn about machine learning in R, Kuhn and Silge (2023) is a great place to start.\nIf you are looking for something that goes into greater detail about a wide range of machine learning methods, then there is no better resource than James et al. (2013)’s An Introduction to Statistical Learning, which is available in R and Python (and has an online course).\nFinally, if you are particularly interested in learning the mathematics that underpins machine learning, I would highly recommend Mathematics of Machine Learning, which is admittedly not free but is very, very good. If you want to learn about the mathematics of machine learning but are not comfortable enough tackling the Mathematics of ML book, the StatQuest) and 3Blue1Brown YouTube channels are both really accessible and well-presented.\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Springer. https://www.statlearning.com/.\n\n\nJordan, Jeremy. 2017. “Hyperparameter Tuning for Machine Learning Models.” https://www.jeremyjordan.me/hyperparameter-tuning/.\n\n\nKuhn, Max, and Julia Silge. 2023. Tidy Modeling with r. https://www.tmwr.org/."
  },
  {
    "objectID": "ml_workflow_r.html#footnotes",
    "href": "ml_workflow_r.html#footnotes",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "",
    "text": "For a more detailed discussion of hyperparameter tuning, and the different methods for tuning in tidymodels, Kuhn and Silge (2023)’s Model Tuning and the Dangers of Overfitting discussion is a good place to start. In addition, the AWS overview of Hyperparameter Tuning is a good resource if you are only looking for a brief overview. Finally, for an in-depth discussion about how hyperparameter tuning works, including the mathematics behind it, Jordan (2017) is thorough but easy to follow.↩︎"
  },
  {
    "objectID": "clustering_r.html#dimensionality-reduction",
    "href": "clustering_r.html#dimensionality-reduction",
    "title": "9  Unsupervised Learning",
    "section": "\n9.1 Dimensionality Reduction",
    "text": "9.1 Dimensionality Reduction\nOne of the most common goals of unsupervised learning is trying to reduce complicated datasets with many dimensions (columns/features/variables) down to a smaller, more easily understood dataset.\nThere are lots of use cases for this sort of process. One particularly common use case is visualising a complex dataset with many dimensions using just two dimensions.\nIt can also be a really useful step in the data preprocessing of building either a supervised or unsupervised machine learning model. When there are multiple related features in your model, multicollinearity1 can weaken the predictive performance of your model, or limit its capacity to identify a meaningful latent structure in the data. Dimensionality reduction attempts to squash these related features down to just one feature (dimension) and give a representation of those features that captures their contribution to the variance in the data.\n\n9.1.1 Principal Component Analysis\nThe most popular (and still very useful) approach to dimensionality reduction is a method called Principal Component Analysis (PCA). PCA reduces data to fewer dimensions by taking the centre point of the space the data exists in (often referred to as a hyperplane or a feature subspace), and finds the axes in that space that captures the data (or the variance in the data) in simpler terms (fewer dimensions). This involves taking the\nI find a simple way of thinking about this process is that it is about taking a “large set of correlated variables” and summarising them “with a smaller number of representative variables that collectively explain most of the variability in the original set” (James et al. 2023). When PCA reduces a dataset to fewer dimensions, the first principal components should capture most of the variance in the data.\nThere are lots of other methods for doing dimensionality reduction (some examples include Linear Discriminant Analysis, UMAP, t-SNE), we will stick with PCA in this guide, to show you what this process is doing, and how it can be useful in a variety of data science contexts.\nAlthough unsupervised methods do not have labels that they will try to predict, validating an unsupervised model still requires data, and using the same variables that the model has been trained on is a problematic way of evaluating performance, because a model that has been trained on the underlying structure of a set of variables should probably do quite a good job when given those same variables!\nTherefore, in this instance, we will use the IMD Deciles as our validation data. The public health data we are using has a number of variables that should contribute to deprivation (or that are negatively impacted by deprivation), so we should expect a good model to fit well to IMD deciles.\n\ntrain_test_split &lt;- \n  initial_split(public_health, strata = \"imd_quartile\", prop = .7)\n\n# extract training and test sets\ntrain_df &lt;- training(train_test_split)\ntest_df &lt;- testing(train_test_split)\n\n\ntrain_df |&gt; \n  select(-starts_with(\"area\"), -starts_with(\"imd\")) |&gt; \n  janitor::clean_names(case = \"title\", abbreviations = c(\"LTC\")) |&gt; \n  cor() |&gt; \n  ggcorrplot::ggcorrplot(\n    type = \"lower\", \n    lab = TRUE, \n    lab_col = \"#333333\",\n    ggtheme = scwplot::theme_scw(base_size = 10)) +\n  scwplot::scale_fill_diverging(palette = \"blue_green\")\n\n\n\nFigure 9.1: Correlation matrix for all numerical variables in the Public Health dataset\n\n\n\n\ndf_numeric_long &lt;- \n  train_df |&gt; \n  select(-starts_with(\"area\"), -imd_score, -imd_decile) |&gt; \n  pivot_longer(!imd_quartile, names_to = \"feature_names\", values_to = \"values\") \n\ndf_numeric_long |&gt;  \n  ggplot(mapping = aes(x = feature_names, y = values)) +\n  geom_boxplot() +\n  facet_wrap(~ feature_names, ncol = 4, scales = \"free\")\n\n\n\n\nThere’s a reasonable amount of variance with several variables.\n\npca_recipe &lt;-\n  recipe(imd_quartile ~ ., data = train_df) |&gt; \n  step_rm(all_nominal(), \"imd_score\", \"imd_decile\") |&gt; \n  step_zv(all_numeric_predictors()) |&gt; \n  bestNormalize::step_orderNorm(all_numeric_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())\n\nWe can borrow a method taken from the Tidy Modelling with R book, for plotting PCA results and validating model performance.\n\nplot_validation_results &lt;- function(recipe, data = train_df) {\n  recipe |&gt;\n    # estimate additional steps\n    prep() |&gt;\n    # process data\n    bake(new_data = data) |&gt;\n    # scatterplot matrix\n    ggplot(aes(x = .panel_x, y = .panel_y,\n               colour = as.factor(imd_quartile),\n               fill = as.factor(imd_quartile))) +\n    geom_point(shape = 21, size = 2, stroke = 1,\n               alpha = .7, colour = \"#333333\",\n               show.legend = FALSE) +\n    ggforce::geom_autodensity(alpha = .8, colour = \"#333333\") +\n    ggforce::facet_matrix(vars(-imd_quartile), layer.diag = 2) + \n    scwplot::scale_colour_qualitative(palette = \"scw\") +\n    scwplot::scale_fill_qualitative(palette = \"scw\")\n}\n\n\npca_recipe |&gt; \n  step_pca(all_numeric_predictors(), num_comp = 4) |&gt; \n  plot_validation_results()\n\n\n\nFigure 9.2: Validation results from the principal components analysis\n\n\n\n\npca_recipe |&gt; \n  step_pca(all_numeric_predictors(), num_comp = 4) |&gt; \n  prep() |&gt; \n  tidy(5) |&gt; \n  filter(component %in% paste0(\"PC\", 1:4)) |&gt; \n  mutate(\n    component = forcats::fct_inorder(component),\n    terms = snakecase::to_title_case(terms, abbreviations = \"LTC\"),\n    direction = case_when(\n      value &gt; 0 ~ \"Positive\",\n      value &lt; 0 ~ \"Negative\"\n      ),\n    abs_value = abs(value)\n  ) |&gt; \n  slice_max(abs_value, n = 10, by = component) |&gt; \n  arrange(component, abs_value) |&gt; \n  mutate(order = row_number()) |&gt; \n  ggplot(aes(x = reorder(terms, order), y = value, fill = direction)) +\n  geom_col(colour = \"#333333\", show.legend = FALSE) +\n  geom_hline(yintercept = 0, linewidth = 1, colour = \"#333333\") +\n  coord_flip() +\n  facet_wrap(vars(component), scales = \"free_y\") +\n  labs(x = NULL, y = \"Value of Contribution\") +\n  scale_y_continuous(breaks = seq(-.5, .75, .25)) +\n  scwplot::scale_fill_qualitative(palette = \"scw\") +\n  theme(\n    axis.text.y = element_text(hjust = 1),\n    axis.title.x = element_text(hjust = .4)\n    )\n\n\n\nFigure 9.3: Plotting the features that are most predictive of each PCA component\n\n\n\n\npca_recipe |&gt; \n  step_pca(all_numeric_predictors(), num_comp = 4) |&gt; \n  prep() |&gt; \n  juice() |&gt; \n  ggplot(aes(PC1, PC2, fill = as.factor(imd_quartile))) +\n  geom_point(shape = 21, size = 5, stroke = 1,\n               alpha = .8, colour = \"#333333\") +\n  scwplot::scale_fill_qualitative(palette = \"scw\")\n\n\n\nFigure 9.4: Plotting the distribution of the first two components on a two-dimensional plane"
  },
  {
    "objectID": "clustering_r.html#clustering",
    "href": "clustering_r.html#clustering",
    "title": "9  Unsupervised Learning",
    "section": "\n9.2 Clustering",
    "text": "9.2 Clustering\nClustering is the process of grouping data into “clusters” based on the underlying patterns that explain the variance across all features in the data.\nThere are a wide variety of clustering algorithms that can be grouped into several different subtypes. Some of the most common approaches include partition clustering, density clustering2, and hierarchical clustering3, and model-based clustering) however, we will focus on the most common clustering algorithm, and a type of partition clustering- K-Means.\n\n9.2.1 K-Means Clustering\nK-means is a clustering algorithm that assigns each observation to \\(k\\) unique clusters (\\(k\\) clusters, defined by the user), based on which cluster centroid (the centre point) is nearest.\n\nSpecify number of clusters (centroids)\nThose centroids are randomly placed in the data\nEach observation assigned to its nearest centroid, measured using Euclidean distance4\n\nCentre point (centroid) of each cluster is then calculated based on the observations in each cluster\nThe distance between observations in a cluster and their centroid (sum of squared Euclidean distances, or geometric mean) is measured\nCentroids are iteratively shifted and clusters recalculated until the distance within clusters is minimised5\n\n\nThis interactive visualisation by Naftali Harris does a really good job of demonstrating how K-Means clustering works on a variety of different data structures.\nThe random process that initialises the k-means algorithm means that fitting k-means clustering to a dataset multiple times will lead to different outcomes, and it is therefore often a good idea to fit k-means algorithms multiple times and average over the results.\nWhile k-means is a simple and relatively powerful algorithm, it is not without its drawbacks. K-means assumes that clusters are a certain shape (circular/elliptical), and where the internal structure of clusters is more complicated than that, k-means will not cope particularly well (and in this case density clustering algorithms will generally perform better).\n\n# create cross-validation folds\ntrain_folds &lt;- vfold_cv(train_df, v = 5)\n\n# inspect the folds\ntrain_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [166/42]&gt; Fold1\n2 &lt;split [166/42]&gt; Fold2\n3 &lt;split [166/42]&gt; Fold3\n4 &lt;split [167/41]&gt; Fold4\n5 &lt;split [167/41]&gt; Fold5\n\n\n\nkmeans_spec &lt;- \n  k_means(num_clusters = tune()) |&gt; \n  set_engine(engine = \"stats\", nstart = 1000)\n\ncluster_recipe &lt;- \n  recipe(~ ., data = train_df) |&gt; \n  step_rm(all_nominal(), starts_with(\"imd\"))\n\ncluster_wflow &lt;- workflow(cluster_recipe, kmeans_spec)\n\ncluster_grid &lt;- grid_regular(num_clusters(), levels = 10)\n\nWe will tune the model and measure the model performance using three different metrics:\n\n\nsse_within_total - Total Within-Clusters Sum of Squared Errors (WSS)\n\nsse_total - Total Sum of Squared Errors (TSS)\n\nsse_ratio - Ratio of WSS to TSS\n\n\nres &lt;- tune_cluster(\n  cluster_wflow,\n  resamples = train_folds,\n  grid = cluster_grid,\n  control = control_grid(save_pred = TRUE, extract = identity),\n  metrics = cluster_metric_set(sse_within_total, sse_total, sse_ratio)\n)\n\ncluster_metrics &lt;- \n  res |&gt; \n  collect_metrics() \n\ncluster_metrics |&gt; \n  arrange(mean, std_err) |&gt; \n  print(n = 30)\n\n# A tibble: 30 × 7\n   num_clusters .metric          .estimator        mean     n    std_err .config\n          &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;  \n 1           10 sse_ratio        standard        0.0837     5    1.19e-3 Prepro…\n 2            9 sse_ratio        standard        0.0916     5    1.62e-3 Prepro…\n 3            8 sse_ratio        standard        0.101      5    2.13e-3 Prepro…\n 4            7 sse_ratio        standard        0.114      5    2.28e-3 Prepro…\n 5            6 sse_ratio        standard        0.128      5    2.71e-3 Prepro…\n 6            5 sse_ratio        standard        0.148      5    2.57e-3 Prepro…\n 7            4 sse_ratio        standard        0.192      5    2.28e-3 Prepro…\n 8            3 sse_ratio        standard        0.267      5    2.77e-3 Prepro…\n 9            2 sse_ratio        standard        0.438      5    2.21e-3 Prepro…\n10            1 sse_ratio        standard        1          5    0       Prepro…\n11           10 sse_within_total standard    78742.         5    1.09e+3 Prepro…\n12            9 sse_within_total standard    86210.         5    1.11e+3 Prepro…\n13            8 sse_within_total standard    95103.         5    1.02e+3 Prepro…\n14            7 sse_within_total standard   106773.         5    1.03e+3 Prepro…\n15            6 sse_within_total standard   120080.         5    1.20e+3 Prepro…\n16            5 sse_within_total standard   138864.         5    1.51e+3 Prepro…\n17            4 sse_within_total standard   181114.         5    4.11e+3 Prepro…\n18            3 sse_within_total standard   251509.         5    7.87e+3 Prepro…\n19            2 sse_within_total standard   413156.         5    9.65e+3 Prepro…\n20            2 sse_total        standard   942242.         5    2.09e+4 Prepro…\n21            7 sse_total        standard   942242.         5    2.09e+4 Prepro…\n22            8 sse_total        standard   942242.         5    2.09e+4 Prepro…\n23            4 sse_total        standard   942242.         5    2.09e+4 Prepro…\n24            6 sse_total        standard   942242.         5    2.09e+4 Prepro…\n25            1 sse_total        standard   942242.         5    2.09e+4 Prepro…\n26            1 sse_within_total standard   942242.         5    2.09e+4 Prepro…\n27            5 sse_total        standard   942242.         5    2.09e+4 Prepro…\n28            9 sse_total        standard   942242.         5    2.09e+4 Prepro…\n29           10 sse_total        standard   942242.         5    2.09e+4 Prepro…\n30            3 sse_total        standard   942242.         5    2.09e+4 Prepro…\n\n\nDue to the way k-means clustering works, adding clusters will generally improve the performance of the model, so it is no surprise to see that the model performance, according to all three metrics, is more or less ordered by the number of clusters. This doesn’t mean you should just be looking to add as many clusters as possible though. If \\(k\\) clusters = \\(n\\) observations then the distance will be zero.\nSo choosing the number of clusters is a non-trivial decision, and this is where data science becomes a little more art than science. One of the common approaches to choosing the number of clusters is the “Elbow” method (sometimes referred to as a Scree Plot), which plots the WSS/TSS ratio produced by the model when it has a certain amount of clusters.\n\ncluster_metrics |&gt;\n  filter(.metric == \"sse_ratio\") |&gt;\n  ggplot(aes(x = num_clusters, y = mean)) +\n  geom_point(size = 3) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Clusters\", y = \"WSS/TSS Ratio\") +\n  scale_x_continuous(breaks = 1:10)\n\n\n\nFigure 9.5: Validation results from the principal components analysis\n\n\n\nIt is called the Elbow method because the goal is to identify the “elbow” in the line, where the marginal gains that each additional cluster is not great enough to justify its inclusion in the model.\nIn the above plot, the biggest drop is from one cluster to two, which is typical, but from two clusters, to three, and possibly even four, there is a greater drop in the WSS/TSS ratio than in the additional clusters after that point.\n\nkmeans_spec &lt;- \n  k_means(num_clusters = 4) |&gt; \n  set_engine(engine = \"stats\", nstart = 1000)\n\ncluster_wflow &lt;- workflow(cluster_recipe, kmeans_spec)\n\nkmeans_fit &lt;- cluster_wflow |&gt; \n  fit(train_df)\n\nkmeans_fit |&gt; \n  extract_centroids()\n\n# A tibble: 4 × 13\n  .cluster  low_income_children ltc_employment_gap ltc_working sickness_absence\n  &lt;fct&gt;                   &lt;dbl&gt;              &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;\n1 Cluster_1                22.6               13.4        60.7             1.40\n2 Cluster_2                18.7               12.1        63.9             1.39\n3 Cluster_3                13.5               10.5        68.2             1.27\n4 Cluster_4                17.8               10.2        68.4             1.09\n# ℹ 8 more variables: homelessness &lt;dbl&gt;, fuel_poverty &lt;dbl&gt;, loneliness &lt;dbl&gt;,\n#   low_birth_weight &lt;dbl&gt;, child_injury_admissions &lt;dbl&gt;,\n#   self_harm_admissions &lt;dbl&gt;, inactive_adults &lt;dbl&gt;, suicide_rate &lt;dbl&gt;\n\nkmeans_fit |&gt;\n  summary()\n\n        Length Class      Mode   \npre     3      stage_pre  list   \nfit     2      stage_fit  list   \npost    1      stage_post list   \ntrained 1      -none-     logical\n\n\n\nkmeans_fit |&gt; silhouette_avg(train_df, dist_fun = Rfast::Dist)\n\n# A tibble: 1 × 3\n  .metric        .estimator .estimate\n  &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt;\n1 silhouette_avg standard       0.386\n\n\n\n9.2.2 Validation"
  },
  {
    "objectID": "clustering_r.html#limitationsissues-with-unsupervised-learning",
    "href": "clustering_r.html#limitationsissues-with-unsupervised-learning",
    "title": "9  Unsupervised Learning",
    "section": "\n9.3 Limitations/Issues with Unsupervised Learning",
    "text": "9.3 Limitations/Issues with Unsupervised Learning"
  },
  {
    "objectID": "clustering_r.html#next-steps",
    "href": "clustering_r.html#next-steps",
    "title": "9  Unsupervised Learning",
    "section": "\n9.4 Next Steps",
    "text": "9.4 Next Steps"
  },
  {
    "objectID": "clustering_r.html#resources",
    "href": "clustering_r.html#resources",
    "title": "9  Unsupervised Learning",
    "section": "\n9.5 Resources",
    "text": "9.5 Resources\n\nTidy Modeling With R: Dimensionality Reduction\nIntroduction to Statistical Learning: Unsupervised Learning Code\nHands on Machine Learning With R: Dimensional Reduction\nHands on Machine Learning With R: Clustering\n\n\n\n\n\n\n\nÇetinkaya-Rundel, Mine, and Johanna Hardin. 2021. Introduction to Modern Statistics. OpenIntro. https://openintro-ims.netlify.app/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. 2023. An Introduction to Statistical Learning. Springer. https://www.statlearning.com/."
  },
  {
    "objectID": "clustering_r.html#footnotes",
    "href": "clustering_r.html#footnotes",
    "title": "9  Unsupervised Learning",
    "section": "",
    "text": "Multicollinearity occurs when explanatory/predictor variables in your model are correlated with each other (Çetinkaya-Rundel and Hardin 2021). It can reduce the accuracy of your model and bias the estimates.↩︎\nDensity clustering treats clustering as a problem of high- and low-density in the feature space. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) builds clusters from areas of high density in the feature space, and finds the cut-off point based on the areas of low density.↩︎\nHierarchical clustering is an approach to clustering that assumes the latent structure that defines the data is hierarchical in nature. It clusters data by nesting clusters and merging or splitting them iteratively. The most common hierarchical algorithm is Agglomerative Clustering, which takes a bottom-up approach, with each observation being a distinct cluster, and merges clusters iteratively.↩︎\nThe Euclidean distance between two points is the length of a straight line between them, and it can be calculated using the Cartesian coordinates of the points.↩︎\nThere are variations on this iterative process (which is called the Lloyd method), such as the MacQueen method and Hartigan-Wong.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aschwanden, Christie. 2015. “Science Isn’t Broken: It’s Just a\nHell of a Lot Harder Than We Give It Credit For.”\nFiveThirtyEight. https://fivethirtyeight.com/features/science-isnt-broken/.\n\n\n———. 2016. “Statisticians Found One Thing They Can Agree on: It’s\nTime to Stop Misusing p-Values.” FiveThirtyEight. https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/.\n\n\nBlackwell, Matthew. 2023. “A User’s Guide to Statistical Inference\nand Regression.” https://mattblackwell.github.io/gov2002-book/.\n\n\nÇetinkaya-Rundel, Mine, and Johanna Hardin. 2021. Introduction to\nModern Statistics. OpenIntro. https://openintro-ims.netlify.app/.\n\n\nFisher, Ronald A. 1956. “Statistical Methods and Scientific\nInference.”\n\n\nGelman, Andrew. 2016. “The Problems with p-Values Are Not Just\nwith p-Values.” The American Statistician 70. http://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari Vehtari. 2020.\nRegression and Other Stories. Cambridge University Press.\n\n\nGelman, Andrew, and Eric Loken. 2021. “The Statistical Crisis in\nScience.” American Scientist 102 (6): 460. https://doi.org/10.1511/2014.111.460.\n\n\nGelman, Andrew, and Hal Stern. 2006. “The Difference Between\n\"Significant\" and \"Not Significant\" Is Not Itself Statistically\nSignificant.” The American Statistician 60 (4): 328–31.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin,\nCharles Poole, Steven N. Goodman, and Douglas G. Altman. 2016.\n“Statistical Tests, p-Values, Confidence Intervals, and Power: A\nGuide to Misinterpretations.” European Journal of\nEpidemiology 31: 337–50.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2013. An Introduction to Statistical Learning. Springer. https://www.statlearning.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and\nJonathan Taylor. 2023. An Introduction to Statistical Learning.\nSpringer. https://www.statlearning.com/.\n\n\nJordan, Jeremy. 2017. “Hyperparameter Tuning for Machine Learning\nModels.” https://www.jeremyjordan.me/hyperparameter-tuning/.\n\n\nKuhn, Max, and Julia Silge. 2023. Tidy Modeling with r. https://www.tmwr.org/.\n\n\nLakens, Daniel. 2022. Improving Your Statistical Inferences. https://doi.org/10.5281/zenodo.6409077.\n\n\nLindeløv, Jonas Kristoffer. 2019. “Common Statistical Tests Are\nLinear Models.” https://lindeloev.github.io/tests-as-linear/.\n\n\nMcElreath, Richard. 2023. “None of the Above.” https://elevanth.org/blog/2023/07/17/none-of-the-above/.\n\n\nPoole, Charles. 2022. “The Statistical Arc of\nEpidemiology.” Presented at the \"What is the Value of the\nP-Value?\" Panel Discussion, Cosponsored by UNC TraCS, Duke University,\nand Wake Forest University CTSA Biostatistics, Epidemiology and Research\nDesign (BERD) Cores.\n\n\nPopper, Karl. 1935. The Logic of Scientific Discovery.\nRoutledge.\n\n\nPrytherch, Ben. 2022. DSCI 335: Inferential Reasoning in Data\nAnalysis. https://bookdown.org/csu_statistics/dsci_335_spring_2022/.\n\n\nSinikumpu, Suvi-Päivikki, Jari Jokelainen, Juha Auvinena, Markku\nTimonen, and Laura Huilaja. 2021. “Association Between\nPsychosocial Distress, Sexual Disorders, Self-Esteem and Quality of Life\nwith Male Androgenetic Alopecia: A Population-Based Study with Men at\nAge 46.” BMJ Open 11 (12).\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA\nStatement on p-Values: Context, Process, and Purpose.” The\nAmerican Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019.\n“Moving to a World Beyond \"p &lt; 0.05\".” The American\nStatistician 73 (sup1): 1–19. https://doi.org/10.1080/00031305.2019.1583913.\n\n\nZablotski, Yury. 2019. “Statistical Tests Vs. Linear\nRegression.” https://yury-zablotski.netlify.app/post/statistical-tests-vs-linear-regression/."
  },
  {
    "objectID": "version_control.html",
    "href": "version_control.html",
    "title": "Version Control",
    "section": "",
    "text": "Version control systems like Git are vital for the management of any codebase. Version control means that changes to the code (and anything else in the repository) can be tracked over time. This is a software engineering best-practice that has also been adopted in data science. It is an excellent ‘habit’ to learn when working with code, because it allows you to track changes, and therefore, revert to previous versions of the code if necessary. It also makes collaboration significantly easier, as it allows multiple people to work on the same codebase without creating conflicts.\nIf you are new to Git, a good place to start is the NHS-R Git Training. Their Introduction to Git will help you understand why version control is necessary, what Git & GitHub can offer, and how to navigate these tools.\nThese data science guides are designed to be used with Git because this is a very important skill for data science, and while it can be a bit challenging at first, it is incredibly valuable to learn, and everyone who wants to do some of the tasks in these guides should also be using Git to manage their work."
  }
]