[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Data Science Guides",
    "section": "",
    "text": "Welcome\nFor the Python  version of this book, click here.\nData Science Guides is a series of worked examples of common data science tasks in R & Python, to use as a space for learning how to implement these methods, and as a template for applying data science at NHS South, Central and West CSU (and beyond).\nThe intention of these guides is not to teach the reader to use R or Python, nor is the intention to teach the foundations of mathematics, statistics, and probability that underpin many of the methods used in data science. While these guides should be easy enough to follow for a relative beginner, teaching these fundamentals is beyond the scope of this book.\nInstead, the hope is that these guides serve as simple starting points for anyone looking to use a particular method, using R/Python, or for anyone looking to kick off a data science project with limited experience.\nThis book is freely available and licensed under CC BY-NC-SA 4.0."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "R Data Science Guides",
    "section": "Resources",
    "text": "Resources\nThere are lots of resources for learning some of the foundations and principles that are not available in these data science guides.\n\nLearning Statistics with R\nBeyond Multiple Linear Regression\nStatistical Rethinking\nTelling Stories with Data\nStatQuest"
  },
  {
    "objectID": "intro.html#choosing-between-r-python",
    "href": "intro.html#choosing-between-r-python",
    "title": "1  Introduction",
    "section": "\n1.1 Choosing Between R & Python",
    "text": "1.1 Choosing Between R & Python\nR and Python are two of the most popular programming languages in the world, and they are both used extensively in data science. However, there is a lot of debate about which language is better, and which language is more suitable for data science. A quick Google search for “R vs Python” will quickly point you in the direction of plenty of strongly-held opinions about why R or Python is better and why the other language is rubbish and a total waste of time, but I think these debates are a little worn out, and it doesn’t help newcomers to the field to see these dichotomous views of two of the most popular languages for data science out there.\nIn my view, as someone that started out with R before moving on to Python, and that is comfortable using either, I don’t think the most important decision is about which language you will learn. It’s the decision to learn any programming language at all. I’ve found that different people find different languages easier to get started with. Personally, I find that Python makes a little more sense to me than R. While I’ve been using R a lot longer, and am quite capable with it (perhaps more so than I am with Python), I tend to be able to learn methods/concepts easier with Python, because the logic that underpins Python methods just works for me. But that doesn’t mean that the logic underpinning Python is better. It just happens to be the case that it is well-aligned with the way that I think about programming. I think it is more important to find which language makes the most sense to you. Both are very strong on data science, as well as being strong in a number of other areas, and both are very popular. Playing around with them both and working out which one makes most sense to you, and which one gives you the best shot at maintaining your coding journey, is what matters.\nIt’s also worth noting that learning a second language, when you are confident with your first, is a lot easier. The hard part is learning the first language. If you crack that part, then you will be able to tackle the second, third, and fourth languages with a lot more ease and at a much faster pace. Learning R doesn’t mean you have to stick to R forever, and vice versa for Python. The important thing is that you’ve learned a coding language. Any coding language.\n\n1.1.1 Popularity\nBoth R & Python are widely used in data science, and both are very popular programming languages. Although there are a number of ways to measure the popularity of a programming language, every measure demonstrates that Python is one of, if not the most popular language in the world, and R is in and around the top ten. A common measure of the popularity of a programming language is the Popularity of Programming Language (PYPL) Index, which is a measure of popularity based on the number of times they are searched for on Google. The PYPL Index is updated monthly, and the data is available from the PYPL website.\nHere is how the top ten most popular programming languages looks in 2023 (based on an average PYPL Index in 2023):\n\n\n\n\nFigure 1.1: The global popularity of programming languages in 2023, according to the PYPL Index\n\n\n\nWhile Python is way out in front in first, R is also very high on the list in seventh place. This is particularly impressive given that every other language on the list is a general purpose programming language, while R is a statistical programming language. When we look at the PYPL Index in the UK, we see a similar picture, with Python even further out in front in first place, and R in in fifth place. This is a testament to the popularity of R within the niche that it occupies, of which data science is a part.\nWe can also look at how the popularity of R and Python has changed over time, in comparison with other languages that are tracked in the PYPL Index. Here is a plot of the PYPL Index over time, from 2005 to 2023:\n\n\n\n\nFigure 1.2: The global popularity of programming languages from 2005 to 2023, according to the PYPL Index\n\n\n\nAs you can see, Python has been the most popular programming language since 2018, and R has been in and around the top ten for about a decade. Both languages are extremely popular, and both are likely to remain popular for the foreseeable future.\nWhile the PYPL Index is a good measure of language popularity, there are other ways of measuring the most popular programming language, such as GitHub’s The State Of Open Source Software Report and Stack Overflow’s Developer Survey and Tag Trends.\n\n1.1.2 NHS R & Python Communities\nThere are already a sizeable number of people using both languages in the NHS, and there are communities that are already in place to support people using R or Python in the NHS.\nThe NHS-R community is a pretty active community of people that are using R in the NHS. You can find them on the NHS-R website, Twitter, and Slack. They also have a Github organisation and a YouTube channel which both contain tons of really useful resources.\nWith regards to Python, the equivalent community is the NHS Python Community for Healthcare (typically shortened to NHS-PyCom). The NHS Python Community also have a website, a Slack channel, and they have a Github organisation and a YouTube channel which are both worth checking out. The NHS Python Community is a little smaller than the NHS-R community, but it is still a very active community, and it is growing quickly, with both communities collaborating regularly to try and spread the growth of both languages in the NHS."
  },
  {
    "objectID": "good_science.html#a-scientific-approach-to-problem-solving",
    "href": "good_science.html#a-scientific-approach-to-problem-solving",
    "title": "2  Doing Good Science",
    "section": "2.1 A Scientific Approach to Problem Solving",
    "text": "2.1 A Scientific Approach to Problem Solving\nWhile it is important to learn the methods detailed in these guides (and more), the greatest emphasis should be placed on learning how to use the scientific method to approach problems. Approaching business problems scientifically, trying to build theories based on our observations about the subject being studied, is often overlooked, as this is in many ways the most complex part of the process. It is also less exciting than learning new ways to analyse data, so for someone that is new to data science, it is quite easy to deemphasise this and focus on doing the fun parts instead.\nThese issues are exacerbated by the fact that any data science education, whether formal or self-taught, is often focused on the methods and tools required for data science. There is less explicit focus on the “process”, but this is because the process is taught implicitly through the methods. Teaching the various methods and tools that a data scientist needs to know is time-consuming and complex, and in the process of learning and applying the methods, the student will also learn the scientific approach to problem solving. That said, once someone has mastered the methods, they are (relatively) east to apply to real-world problems, while the process of doing science the right way is more difficult, and requires careful consideration for every practicioner, no matter how experienced they are.\n\n2.1.1 Framing the Problem\nAll of this ties into the same questions that should be asked when identifying data science opportunities. But having identified these opportunities, it is important not to skip to identifying a cool method for solving the problem. Instead, focus on the problem itself, and then identify the method(s) that will be most appropriate for answering it.\nA simple, concise approach to framing the problem and maximising the value of data science is to use Tom Carpenter’s suggestions below:\n\nIf you follow these five steps detailed by Tom, you stand a good chance of approaching the problem in the right way, generating real business value, and really getting to the heart of the problem you are trying to solve.\n\n\n2.1.2 A Simple Framework for Data Science\nThe guides here obviously focus on the methods, but before diving into the code, I think it is important to highlight the importance of learning to implement these methods appropriately, and approaching data science in the “correct” way. This means understanding the problem, generating theories about how to solve it, identifying the data that is needed to test these theories, and building a robust model that is informed by your theory and that is appropriate for the data you have. Before jumping into the code and the iterative process of model building, it is important to take the time to think carefully about your theory, the variables that are relevant to the phenomenon you are studying and the causal mechanism that guides it (or the data generating process), and your expectations from your model. Only once you’ve thought carefully about all of these things should you start to think about the method(s) you should use.\nThe order of these steps is important, but the right order is dependent on context. In an ideal situation…\nFor anyone that is analytically minded, it is very easy to fall into the trap of hyperfocusing on the cool methodological approaches you could take to solving a problem. This is something that I regularly have to check myself on, because I find it so easy to get excited about the fancy methods I could apply to a particular situation. However, despite the fact that the methods are often the most fun part of the job, the fact remains that a very fancy model won’t get you very far if you haven’t spent a lot of time thinking carefully about the problem that model is trying to solve, gathering the data that is needed to solve it, and preparing the data in a way that maximises the model’s performance.\nThe easiest way to avoid falling into the same trap that I regularly slip into is to treat the methods as a means to an end. The goal should always be to build a solution to a specific problem, with a clear understanding of what “success” looks like in that context. And if you also get to make something fancy in the process? Well isn’t that exciting!\nData plays an increasingly important role in business decision-making nowadays, due to the technological advances that have made it possible to collect and analyse large amounts of data. This is not just true of tech companies, but also of traditional businesses in sectors such as retail, manufacturing, and healthcare. The NHS is increasingly embracing the role that data plays in driving decision-making, improving clinical services, and, ultimately, improving patient outcomes.\nBut as more data becomes available, the challenge of making good use of it becomes even more difficult. The old methods are still useful, but they are no longer sufficient. In order to make the most of the data that is available, the NHS and SCW need to be able to identify when there are opportunities to use data science to improve the services that they provide. Recognising these opportunities can be challenging. It requires not only a good understanding of what is possible with data science, but also a proactive approach to identifying when it is possible to do more with data than is currently being done.\nWhile the other guides in this project will hopefully give a better understanding of what is possible with data science, this particular guide is intended to help anyone frame business problems, whether internal or external, in terms of data science, and to identify opportunities to develop more advanced solutions to those problems."
  },
  {
    "objectID": "good_science.html#designing-impactful-research",
    "href": "good_science.html#designing-impactful-research",
    "title": "2  Doing Good Science",
    "section": "2.2 Designing Impactful Research",
    "text": "2.2 Designing Impactful Research"
  },
  {
    "objectID": "good_science.html#identifying-data-science-opportunities",
    "href": "good_science.html#identifying-data-science-opportunities",
    "title": "2  Doing Good Science",
    "section": "2.3 Identifying Data Science Opportunities",
    "text": "2.3 Identifying Data Science Opportunities\nA lot of the analytics work that is done at SCW tends to be descriptive in nature, and takes the form of regular reports, dashboards, and Excel spreadsheets summarising data. These descriptive approaches are and always will be valuable, but there is also a limit to what descriptive analytics can tell us. Descriptive analytics can tell us what has happened, but it cannot tell us why, nor can it tell us what will happen in the future. When a piece of work is concerned with explaining past events, or predicting future events, there is an opportunity to use some of the data science techniques that are covered in these guides.\nIdentifying these opportunities is not always easy, however, because requests for work will often come in the form of requests for a certain type of solution to address a business problem, with expectations of what that solution should look like. Customers and colleagues will often have a clear idea of what they want, because that have seen or used similar solutions in the past, and this limits the scope for identifying data science opportunities. If we always provide the exact solution that is requested, without considering whether there is a better way to solve the problem, we will miss out on opportunities to extract more value from our data products.\nIt is important to understand the business problem and the solution that is being requested, but it is also important to identify why a certain solution is being requested, and how the customer or colleague plans to use it. For example, a request for a dashboard that shows utilisation of a particular service in the past twelve months is not necessarily a request for a data science solution. It is possible that the reason for wanting this dashboard is to help plan how to meet future demand for that service, but it is also possible that the reason for wanting this dashboard is simply to understand how the service is being used. While a dashboard will offer valuable context, it is possible that a machine learning model that predicts future demand might be more useful.\n\n2.3.1 Asking the Right Questions\nDesigning data science solutions to business problems is a process of discovery. The best way to identify potential data science work is to ask questions.\nThere is no foolproof set of questions you can ask, and no perfect framework for designing data science solutions, but there are some questions that are more likely to lead to useful insights than others:\n\n2.3.1.1 Identifying the Problem\n\nWhat is the problem you are trying to solve?\nWhat is the business impact of solving this problem?\nWhat is the current solution to this problem?\n\n\n\n2.3.1.2 Considering Solutions\n\nWhat do you think the solution to this problem should look like?\nWhy do you think this approach is the best way to solve this problem?\nHow will you use the solution to this problem?\n\n\n\n2.3.1.3 Measuring Success\n\nHow will you know if the solution to this problem is successful?\nHow is success defined and measured for this problem?\n\n\n\n2.3.1.4 Identifying Feasible Approaches\n\nWhat data do you need to solve this problem?\nWhat data do you have that might be useful in solving this problem?\nWhen does this piece of work need to be completed by?\nHow often will this piece of work need to be repeated/updated?"
  },
  {
    "objectID": "good_science.html#next-steps",
    "href": "good_science.html#next-steps",
    "title": "2  Doing Good Science",
    "section": "2.4 Next Steps",
    "text": "2.4 Next Steps\nAlthough the data science guides focus, for the most part, on the implementation of various data science techniques, developing SCW’s data science capabilities is more than just upskilling analysts and equipping individuals with the tools to do this work. Perhaps a more important part of this process is helping a much wider group of SCW colleagues to think about data science, and to recognise opportunities to use data science to solve business problems. Hopefully the guides will help SCW colleagues to understand what is possible, and this guide will help them to identify opportunities to do more with data using these techniques."
  },
  {
    "objectID": "good_science.html#resources",
    "href": "good_science.html#resources",
    "title": "2  Doing Good Science",
    "section": "2.5 Resources",
    "text": "2.5 Resources\n\nTechniques for Problem Solving\nKey Skills for Aspiring Data Scientists: Problem Solving and the Scientific Method\nRichard Feynman on the Scientific Method"
  },
  {
    "objectID": "sql.html#packages",
    "href": "sql.html#packages",
    "title": "3  Importing Data from SQL",
    "section": "\n3.1 Packages",
    "text": "3.1 Packages\nThe R packages you need to interact with SQL are:\n\nDBI\nODBC\n\nI won’t import either package here because they are only needed for a handful of functions."
  },
  {
    "objectID": "sql.html#establish-sql-connection",
    "href": "sql.html#establish-sql-connection",
    "title": "3  Importing Data from SQL",
    "section": "\n3.2 Establish SQL Connection",
    "text": "3.2 Establish SQL Connection\n\ncon &lt;- DBI::dbConnect(\n  odbc::odbc(),\n  driver = 'SQL Server',\n  server = '{db-server}',\n  database = '{db-name}',\n  trustedconnection = TRUE\n)"
  },
  {
    "objectID": "sql.html#running-sql-query",
    "href": "sql.html#running-sql-query",
    "title": "3  Importing Data from SQL",
    "section": "\n3.3 Running SQL Query",
    "text": "3.3 Running SQL Query\n\ndf &lt;- DBI::dbGetQuery(\n  con,\n  'SELECT col_1,\n          col_2,\n          col_3\n  FROM    db_name.table_name'\n)\n\n\nhead(df)\n\ndplyr::glimpse(df)"
  },
  {
    "objectID": "sql.html#export-to-csv",
    "href": "sql.html#export-to-csv",
    "title": "3  Importing Data from SQL",
    "section": "\n3.4 Export to CSV",
    "text": "3.4 Export to CSV\n\nreadr::write_csv(df, here::here('path-to-save-df', 'df.csv'))"
  },
  {
    "objectID": "wrangling_r.html#r-packages-for-data-wrangling",
    "href": "wrangling_r.html#r-packages-for-data-wrangling",
    "title": "4  Wrangling Data",
    "section": "\n4.1 R Packages for Data Wrangling",
    "text": "4.1 R Packages for Data Wrangling\nData wrangling is a task that R is extremely well suited for, particularly thanks to the tidyverse ecosystem, which provides several packages that can be used to do a wide variety of data wrangling tasks, including:\n\n\nreadr - package for reading tabular/rectangular data (e.g. csv files).\n\nreadxl - package for reading Excel files.\n\ndplyr - set of functions for data manipulation (filtering rows, selecting columns, re-ordering rows, adding new variables with functions of existing variables, and collapsing many values down to a single summary).\n\ntidyr - set of functions for data transformation (changing the representation of a dataset from wide to long, or vice versa, and splitting and combining columns).\n\nstringr - tools for working with strings.\n\nlubridate - tools for dealing with dates and times.\n\nIn addition to the tidyverse packages, there are a number of other packages that come in handy when dealing with data, including:\n\n\nhere - package that simplifies how you access files on your local machine by using the relative path based on the location of your R project.\n\njanitor - set of tools for data cleaning.\n\nFinally, there are a number of packages that help you work with particularly large data sets:\n\n\nvroom - package that reads files quickly using the ALTREP frameowrk.\n\ndata.table - package that provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed.\n\narrow - package for using Apache Arrow, which handles in-memory and larger-than-memory data very quickly.\n\nsparklyr - R interface for Apache Spark.\n\nAlthough the tidyverse packages are very popular, they are not required to work with data in R. There will be equivalent ways to do things using only base R functions too. However, I think the tidyverse is particularly user-friendly, especially for those that are coming to R from SQL, so I will focus on data wrangling using a tidyverse workflow in this guide."
  },
  {
    "objectID": "wrangling_r.html#fingertips-data",
    "href": "wrangling_r.html#fingertips-data",
    "title": "4  Wrangling Data",
    "section": "\n4.2 Fingertips Data",
    "text": "4.2 Fingertips Data\nWe will use Public Health England’s Fingertips API as the data source for this guide, as it represents a good opportunity to give a brief overview of this data and how to use it.\nFirst, we can look at what is available. The fingertipsR::indicators_unique() function returns a list of all unique indicators from the Fingerips API, as well as their indicator IDs. We can use the View() function to open the data in a new window and take a closer look at what is available.\n\ninds &lt;- fingertipsR::indicators_unique()\n\n# View(inds)\n\nIf we were interested in a particular indicator, this would be a good way of finding the ID and getting the data. However, if we’re looking for a group of indicators about a particular topic, then the Fingertips profiles are going to be more useful.\n\nprofiles &lt;- fingertipsR::profiles()\n\n# View(profiles)\n\nFrom the profiles, we can identify topics that are of particular interest, for example the Public Health Outcomes Framework (profile ID = 19).\n\nphof_inds &lt;- fingertipsR::indicators(ProfileID=19)\n\n# View(phof_inds)\n\nThere are lots of indicators available from this profile. We can also check the area types that the indicators in the Public Health Outcomes Framework are available for, using either the fingerprintsR::area_types() function, with the ProfileID argument set to the profile we are interested in, or the fingertipsR::indicator_area_types() function, with the IndicatorID argument set to one of the indicators that we are interested in.\n\nfingertipsR::area_types(ProfileID=19) |&gt;\n  dplyr::select(AreaTypeID, AreaTypeName)\n\n   AreaTypeID                             AreaTypeName\n1           6                 Government Office Region\n2         301 Lower tier local authorities (4/20-3/21)\n3         301 Lower tier local authorities (4/20-3/21)\n4         301 Lower tier local authorities (4/20-3/21)\n5         301 Lower tier local authorities (4/20-3/21)\n6         301 Lower tier local authorities (4/20-3/21)\n7         301 Lower tier local authorities (4/20-3/21)\n8         301 Lower tier local authorities (4/20-3/21)\n9         302 Upper tier local authorities (4/20-3/21)\n10        302 Upper tier local authorities (4/20-3/21)\n11        302 Upper tier local authorities (4/20-3/21)\n12        302 Upper tier local authorities (4/20-3/21)\n13         15                                  England\n14        501 Lower tier local authorities (post 4/23)\n15        501 Lower tier local authorities (post 4/23)\n16        501 Lower tier local authorities (post 4/23)\n17        501 Lower tier local authorities (post 4/23)\n18        501 Lower tier local authorities (post 4/23)\n19        502 Upper tier local authorities (post 4/23)\n20        502 Upper tier local authorities (post 4/23)\n21        502 Upper tier local authorities (post 4/23)\n22        502 Upper tier local authorities (post 4/23)\n\nfingertipsR::indicator_areatypes(IndicatorID=phof_inds$IndicatorID[1])\n\n# A tibble: 7 × 2\n  IndicatorID AreaTypeID\n        &lt;int&gt;      &lt;int&gt;\n1       90362        502\n2       90362        402\n3       90362        302\n4       90362        202\n5       90362        102\n6       90362         15\n7       90362          6\n\n\nThe list of area types is quite long, but from the indicator_area_types() function, we can see that there are six area types available for the first indicator in the list of profile indicators. The area types ending in 2 are all upper tier local authorities (the different types being different local authority boundaries from different points in time), while the other two area types refer to Government Office Regions and England as a whole.\nWe will use the most recent upper tier local authority boundaries, which have the AreaTypeID, 402.\nHaving identified the profile and area type we are interested in, we can now pull the data from the Fingertips API using the fingertipsR::fingertips_data() function.\n\n# pull wider determinants raw data from fingertips\nphof_raw &lt;-\n  fingertipsR::fingertips_data(\n    ProfileID = 19,\n    AreaTypeID = 302\n  )\n\n\ndplyr::glimpse(phof_raw)\n\nRows: 276,809\nColumns: 27\n$ IndicatorID                         &lt;int&gt; 90362, 90362, 90362, 90362, 90362,…\n$ IndicatorName                       &lt;chr&gt; \"A01a - Healthy life expectancy at…\n$ ParentCode                          &lt;chr&gt; NA, NA, \"E92000001\", \"E92000001\", …\n$ ParentName                          &lt;chr&gt; NA, NA, \"England\", \"England\", \"Eng…\n$ AreaCode                            &lt;chr&gt; \"E92000001\", \"E92000001\", \"E120000…\n$ AreaName                            &lt;chr&gt; \"England\", \"England\", \"North East …\n$ AreaType                            &lt;chr&gt; \"England\", \"England\", \"Region\", \"R…\n$ Sex                                 &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", …\n$ Age                                 &lt;chr&gt; \"All ages\", \"All ages\", \"All ages\"…\n$ CategoryType                        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Category                            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Timeperiod                          &lt;chr&gt; \"2009 - 11\", \"2009 - 11\", \"2009 - …\n$ Value                               &lt;dbl&gt; 63.02647, 64.03794, 59.71114, 60.7…\n$ LowerCI95.0limit                    &lt;dbl&gt; 62.87787, 63.88135, 59.19049, 60.3…\n$ UpperCI95.0limit                    &lt;dbl&gt; 63.17508, 64.19453, 60.23179, 61.1…\n$ LowerCI99.8limit                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ UpperCI99.8limit                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Count                               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Denominator                         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Valuenote                           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ RecentTrend                         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ComparedtoEnglandvalueorpercentiles &lt;chr&gt; \"Not compared\", \"Not compared\", \"W…\n$ ComparedtoRegionvalueorpercentiles  &lt;chr&gt; \"Not compared\", \"Not compared\", \"N…\n$ TimeperiodSortable                  &lt;int&gt; 20090000, 20090000, 20090000, 2009…\n$ Newdata                             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Comparedtogoal                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Timeperiodrange                     &lt;chr&gt; \"3y\", \"3y\", \"3y\", \"3y\", \"3y\", \"3y\"…\n\nphof_raw |&gt;\n  dplyr::distinct(IndicatorName) |&gt;\n  head()\n\n                                                  IndicatorName\n1                       A01a - Healthy life expectancy at birth\n2                               A01b - Life expectancy at birth\n3 A02b - Inequality in healthy life expectancy at birth ENGLAND\n4                                  A01b - Life expectancy at 65\n5      A02c - Inequality in healthy life expectancy at birth LA\n6                 A02a - Inequality in life expectancy at birth\n\n\nWith the raw data imported, we are now able to start the process of cleaning and wrangling the data into a format that is ready for analysis."
  },
  {
    "objectID": "wrangling_r.html#data-wrangling",
    "href": "wrangling_r.html#data-wrangling",
    "title": "4  Wrangling Data",
    "section": "\n4.3 Data Wrangling",
    "text": "4.3 Data Wrangling\nFirst, we will clean the column names using the janitor::clean_names() function. This will convert all column names to lower case, remove any spaces and replace them with underscores, and remove any special characters. This will make it easier to work with the data in the future.\n\nphof_raw &lt;-\n  phof_raw |&gt;\n  janitor::clean_names()\n\nWe’ve got a total of 168 indicators, which means some of the indicators from the Public Health Outcomes Framework are not available for upper tier local authorities. We can filter the data to just the indicators we are interested in (as well as filtering any rows where the value is missing), using the dplyr::filter() function. We can also use the dplyr::group_by() and dplyr::summarise() functions to get the mean value of the indicators across all areas in the data. Below is an example:\n\nphof_raw |&gt;\n  dplyr::filter(\n    indicator_id %in% c(90366, 20101, 11601) &\n    !is.na(value)) |&gt;\n    dplyr::group_by(indicator_name) |&gt;\n    dplyr::summarise(mean(value))\n\n# A tibble: 3 × 2\n  indicator_name                                                 `mean(value)`\n  &lt;chr&gt;                                                                  &lt;dbl&gt;\n1 A01b - Life expectancy at birth                                        80.2 \n2 B16 - Utilisation of outdoor space for exercise/health reasons         16.3 \n3 C04 - Low birth weight of term babies                                   2.87\n\n\nThe data pulled from the Fingertips API has a number of idiosyncracies that need addressing when wrangling the data. For example, the timeperiod_sortable column refers to the year that the data relates to, and it contains four superfluous zeros at the end of each year (i.e, 20230000). We can use the dplyr::mutate() function to create a year column which strips out the zeros from the timeperiod_sortable column (using the stringr::str_remove_all() function).\n\n# filter for the indicators of interest and wrangle data into tidy structure\nphof_raw |&gt;\n  dplyr::filter(indicator_id %in% c(90366, 20101, 11601) & area_code == \"E92000001\") |&gt;\n  dplyr::group_by(indicator_name, area_code, timeperiod_sortable) |&gt;\n  dplyr::summarise(value = mean(value)) |&gt;\n  dplyr::mutate(year = stringr::str_remove_all(timeperiod_sortable, \"0000\")) |&gt;\n  dplyr::select(indicator_name, area_code, year, value)\n\n# A tibble: 39 × 4\n# Groups:   indicator_name, area_code [3]\n   indicator_name                  area_code year  value\n   &lt;chr&gt;                           &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n 1 A01b - Life expectancy at birth E92000001 2001   78.4\n 2 A01b - Life expectancy at birth E92000001 2002   78.7\n 3 A01b - Life expectancy at birth E92000001 2003   79.0\n 4 A01b - Life expectancy at birth E92000001 2004   79.4\n 5 A01b - Life expectancy at birth E92000001 2005   79.6\n 6 A01b - Life expectancy at birth E92000001 2006   79.8\n 7 A01b - Life expectancy at birth E92000001 2007   80.1\n 8 A01b - Life expectancy at birth E92000001 2008   80.4\n 9 A01b - Life expectancy at birth E92000001 2009   80.7\n10 A01b - Life expectancy at birth E92000001 2010   81.0\n# ℹ 29 more rows\n\n\nWe can also remove the ID that appended to the beginning of each indicator name. We can use str_remove_all() again, using a regular expression to remove everything up to and including the dash in the indicator name (and the space immediately after).\n\nphof_raw |&gt;\n  dplyr::filter(indicator_id %in% c(90366, 20101, 11601)) |&gt;\n  dplyr::mutate(indicator_name = stringr::str_remove(indicator_name, \"^[^-]+- \")) |&gt;\n  dplyr::distinct(indicator_name)\n\n                                            indicator_name\n1                                 Life expectancy at birth\n2 Utilisation of outdoor space for exercise/health reasons\n3                          Low birth weight of term babies\n\n\nFinally, the data is structured in a wide format, with each indicator having its own column. This is not a tidy structure, as each column should represent a variable, and each row should represent an observation. We can use the tidyr::pivot_wider() function to convert the data into a tidy structure, with each indicator having its own row. We can also use the dplyr::rename() function to rename the columns to something more meaningful, and the tidyr::drop_na() function to remove any rows where the value is missing.\n\nphof_raw |&gt;\n  dplyr::filter(indicator_id %in% c(90366, 20101, 11601)) |&gt;\n  dplyr::select(indicator_id, indicator_name, area_code, timeperiod_sortable, sex, value) |&gt;\n  tidyr::pivot_wider(\n    names_from = indicator_name,\n    values_from = value\n  ) |&gt;\n  dplyr::rename(\n      life_expectancy = `A01b - Life expectancy at birth`,\n      low_birth_weight = `C04 - Low birth weight of term babies`,\n      outdoor_space = `B16 - Utilisation of outdoor space for exercise/health reasons`\n  )\n\n# A tibble: 9,159 × 7\n   indicator_id area_code timeperiod_sortable sex    life_expectancy\n          &lt;int&gt; &lt;chr&gt;                   &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1        90366 E92000001            20010000 Male              76.2\n 2        90366 E92000001            20010000 Female            80.7\n 3        90366 E12000001            20010000 Male              74.7\n 4        90366 E12000002            20010000 Male              74.8\n 5        90366 E12000003            20010000 Male              75.5\n 6        90366 E12000004            20010000 Male              76.2\n 7        90366 E12000005            20010000 Male              75.6\n 8        90366 E12000006            20010000 Male              77.3\n 9        90366 E12000007            20010000 Male              76.0\n10        90366 E12000008            20010000 Male              77.4\n# ℹ 9,149 more rows\n# ℹ 2 more variables: outdoor_space &lt;dbl&gt;, low_birth_weight &lt;dbl&gt;\n\n\nWe can combine all of the above steps into a single function, which we can then apply to the raw data to get the data into a tidy structure.\n\n# function to wrangle data into tidy structure\nwrangle_phof_data &lt;- function(data) {\n  data |&gt;\n    dplyr::filter(indicator_id %in% c(90366, 20101, 11601)) |&gt;\n    dplyr::group_by(indicator_name, area_code, timeperiod_sortable) |&gt;\n    dplyr::summarise(value = mean(value)) |&gt;\n    dplyr::mutate(year = stringr::str_remove_all(timeperiod_sortable, \"0000\")) |&gt;\n    dplyr::select(indicator_name, area_code, year, value) |&gt;\n    tidyr::pivot_wider(\n      names_from = indicator_name,\n      values_from = value\n    ) |&gt;\n    dplyr::rename(\n      life_expectancy = `A01b - Life expectancy at birth`,\n      low_birth_weight = `C04 - Low birth weight of term babies`,\n      outdoor_space = `B16 - Utilisation of outdoor space for exercise/health reasons`\n    ) |&gt;\n    tidyr::drop_na()\n}\n\nphof &lt;- wrangle_phof_data(phof_raw)"
  },
  {
    "objectID": "wrangling_r.html#next-steps",
    "href": "wrangling_r.html#next-steps",
    "title": "4  Wrangling Data",
    "section": "\n4.4 Next Steps",
    "text": "4.4 Next Steps\nWe have successfully imported, cleaned, and wrangled Fingertips data into a tidy structure. We can now move on to the next step of the data science process, which is to explore the data and perform some analysis.\nWe could save the data to a CSV file at this point, using the readr::write_csv() function (for example, readr::write_csv(phof, here::here(\"data/phof.csv\"))), or we could perform the analysis on the dataframe object we have created."
  },
  {
    "objectID": "wrangling_r.html#resources",
    "href": "wrangling_r.html#resources",
    "title": "4  Wrangling Data",
    "section": "\n4.5 Resources",
    "text": "4.5 Resources\n\nHadley Wickham - Tidy Data\nData Transformation with {dplyr}\nData Tidying with {tidyr}\nTop 10 Must-Know {dplyr} Commands for Data Wrangling in R!\nData Wrangling with R\nData Wrangling Essentials: Comparisons in JavaScript, Python, SQL, R, and Excel\nR to Python: Data Wrangling Snippets"
  },
  {
    "objectID": "eda_r.html#inspecting-the-data",
    "href": "eda_r.html#inspecting-the-data",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.1 Inspecting the Data",
    "text": "5.1 Inspecting the Data\nThe first step when doing EDA is to inspect the data itself and get an idea of the structure of the dataset, the variable types, and the typical values of each variable. This gives a better understanding of exactly what data is being used and informs decisions both about the next steps in the exploratory process and any modelling choices.\nWe can use the head() and glimpse() functions to get a sense of the structure of the data. The head() function returns the first five rows of the data, and glimpse() returns a summary of the data, including the number of rows, the number of columns, the column names, the data type of each column, and the first few rows of the data. In addition to these two methods, we can use the distinct() function to get a list of all unique values of a particular variable. This is useful for discrete variables, such as the outcome variable, which can take on a limited number of values. For continuous variables (or any variables with a large number of unique values) the output of distinct() (a tibble) can be difficult to read, so we can use the unique() function to get a list of all unique values, which will be returned as a vector.\n\n# view first rows in the dataset\nhead(df)\n\n# A tibble: 6 × 10\n    age sex   resting_bp cholesterol fasting_bs resting_ecg max_hr angina\n  &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;        &lt;dbl&gt; &lt;fct&gt; \n1    40 M            140         289 0          Normal         172 N     \n2    49 F            160         180 0          Normal         156 N     \n3    37 M            130         283 0          ST              98 N     \n4    48 F            138         214 0          Normal         108 Y     \n5    54 M            150         195 0          Normal         122 N     \n6    39 M            120         339 0          Normal         170 N     \n# ℹ 2 more variables: heart_peak_reading &lt;dbl&gt;, heart_disease &lt;fct&gt;\n\n# overview of the data\nglimpse(df)\n\nRows: 918\nColumns: 10\n$ age                &lt;dbl&gt; 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39,…\n$ sex                &lt;fct&gt; M, F, M, F, M, M, F, M, M, F, F, M, M, M, F, F, M, …\n$ resting_bp         &lt;dbl&gt; 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 1…\n$ cholesterol        &lt;dbl&gt; 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 2…\n$ fasting_bs         &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ resting_ecg        &lt;fct&gt; Normal, Normal, ST, Normal, Normal, Normal, Normal,…\n$ max_hr             &lt;dbl&gt; 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 14…\n$ angina             &lt;fct&gt; N, N, N, Y, N, N, N, N, Y, N, N, Y, N, Y, N, N, N, …\n$ heart_peak_reading &lt;dbl&gt; 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0…\n$ heart_disease      &lt;fct&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n\n# unique values of the outcome variable\ndf |&gt;\n  distinct(heart_disease)\n\n# A tibble: 2 × 1\n  heart_disease\n  &lt;fct&gt;        \n1 0            \n2 1            \n\n# unique values of a continuous explanatory variable\nunique(df$cholesterol)\n\n  [1] 289 180 283 214 195 339 237 208 207 284 211 164 204 234 273 196 201 248\n [19] 267 223 184 288 215 209 260 468 188 518 167 224 172 186 254 306 250 177\n [37] 227 230 294 264 259 175 318 216 340 233 205 245 194 270 213 365 342 253\n [55] 277 202 297 225 246 412 265 182 218 268 163 529 100 206 238 139 263 291\n [73] 229 307 210 329 147  85 269 275 179 392 466 129 241 255 276 282 338 160\n [91] 156 272 240 393 161 228 292 388 166 247 331 341 243 279 198 249 168 603\n[109] 159 190 185 290 212 231 222 235 320 187 266 287 404 312 251 328 285 280\n[127] 192 193 308 219 257 132 226 217 303 298 256 117 295 173 315 281 309 200\n[145] 336 355 326 171 491 271 274 394 221 126 305 220 242 347 344 358 169 181\n[163]   0 236 203 153 316 311 252 458 384 258 349 142 197 113 261 310 232 110\n[181] 123 170 369 152 244 165 337 300 333 385 322 564 239 293 407 149 199 417\n[199] 178 319 354 330 302 313 141 327 304 286 360 262 325 299 409 174 183 321\n[217] 353 335 278 157 176 131"
  },
  {
    "objectID": "eda_r.html#summary-statistics",
    "href": "eda_r.html#summary-statistics",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.2 Summary Statistics",
    "text": "5.2 Summary Statistics\nSummary statistics are a quick and easy way to get a sense of the distribution, central tendency, and dispersion of the variables in the dataset. We can use the summary() function to get a summary of the data, including the mean and median values, the 1st and 3rd quartiles, and the minimum and maximum values of each numeric column. It also returns the count values for each factor column, and the number of NA values for each column.\nWhile the base summary() function is pretty effective and works right out of the box, the package skimr can provide a more detailed summary of the data, using the skim() function. If you are looking for a single function to capture the entire process of inspecting the data and computing summary statistics, skim() is the function for the job, giving you a wealth of information about the dataset as a whole and each variable in the data.\nIf we want to examine a particular variable, the functions mean(), median(), quantile(), min(), and max() will return the same information as the summary() function. We can also get a sense of dispersion by computing the standard deviation or variance of a variable. The sd() function returns the standard deviation of a variable, and the var() function returns the variance.\nFinally, we can use the count() function to get a count of the number of observations in each category of a discrete variable. Proportions can also be computed by dividing the count by the total number of observations. Using the group_by() function to group the data by a particular variable, and the mutate() function to add a new column to the data, we can compute the proportion as n/sum(n).\n\n# summary of the data\nsummary(df)\n\n      age        sex       resting_bp     cholesterol    fasting_bs\n Min.   :28.00   F:193   Min.   :  0.0   Min.   :  0.0   0:704     \n 1st Qu.:47.00   M:725   1st Qu.:120.0   1st Qu.:173.2   1:214     \n Median :54.00           Median :130.0   Median :223.0             \n Mean   :53.51           Mean   :132.4   Mean   :198.8             \n 3rd Qu.:60.00           3rd Qu.:140.0   3rd Qu.:267.0             \n Max.   :77.00           Max.   :200.0   Max.   :603.0             \n resting_ecg      max_hr      angina  heart_peak_reading heart_disease\n LVH   :188   Min.   : 60.0   N:547   Min.   :-2.6000    0:410        \n Normal:552   1st Qu.:120.0   Y:371   1st Qu.: 0.0000    1:508        \n ST    :178   Median :138.0           Median : 0.6000                 \n              Mean   :136.8           Mean   : 0.8874                 \n              3rd Qu.:156.0           3rd Qu.: 1.5000                 \n              Max.   :202.0           Max.   : 6.2000                 \n\n# more detailed summary of the data\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n918\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nsex\n0\n1\nFALSE\n2\nM: 725, F: 193\n\n\nfasting_bs\n0\n1\nFALSE\n2\n0: 704, 1: 214\n\n\nresting_ecg\n0\n1\nFALSE\n3\nNor: 552, LVH: 188, ST: 178\n\n\nangina\n0\n1\nFALSE\n2\nN: 547, Y: 371\n\n\nheart_disease\n0\n1\nFALSE\n2\n1: 508, 0: 410\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nage\n0\n1\n53.51\n9.43\n28.0\n47.00\n54.0\n60.0\n77.0\n▁▅▇▆▁\n\n\nresting_bp\n0\n1\n132.40\n18.51\n0.0\n120.00\n130.0\n140.0\n200.0\n▁▁▃▇▁\n\n\ncholesterol\n0\n1\n198.80\n109.38\n0.0\n173.25\n223.0\n267.0\n603.0\n▃▇▇▁▁\n\n\nmax_hr\n0\n1\n136.81\n25.46\n60.0\n120.00\n138.0\n156.0\n202.0\n▁▃▇▆▂\n\n\nheart_peak_reading\n0\n1\n0.89\n1.07\n-2.6\n0.00\n0.6\n1.5\n6.2\n▁▇▆▁▁\n\n\n\n\n# mean age\nmean(df$age)\n\n[1] 53.51089\n\n# median age\nmedian(df$age)\n\n[1] 54\n\n# min and max age\nmin(df$age)\n\n[1] 28\n\nmax(df$age)\n\n[1] 77\n\n# dispersion of age\nsd(df$age)\n\n[1] 9.432617\n\nvar(df$age)\n\n[1] 88.97425\n\n# heart disease count\ndf |&gt;\n  count(heart_disease)\n\n# A tibble: 2 × 2\n  heart_disease     n\n  &lt;fct&gt;         &lt;int&gt;\n1 0               410\n2 1               508\n\n# resting ecg count\ndf |&gt;\n  count(resting_ecg)\n\n# A tibble: 3 × 2\n  resting_ecg     n\n  &lt;fct&gt;       &lt;int&gt;\n1 LVH           188\n2 Normal        552\n3 ST            178\n\n# angina\ndf |&gt;\n  count(angina)\n\n# A tibble: 2 × 2\n  angina     n\n  &lt;fct&gt;  &lt;int&gt;\n1 N        547\n2 Y        371\n\n# cholesterol\ndf |&gt;\n  count(cholesterol)\n\n# A tibble: 222 × 2\n   cholesterol     n\n         &lt;dbl&gt; &lt;int&gt;\n 1           0   172\n 2          85     1\n 3         100     2\n 4         110     1\n 5         113     1\n 6         117     1\n 7         123     1\n 8         126     2\n 9         129     1\n10         131     1\n# ℹ 212 more rows\n\n# heart disease proportion\ndf |&gt; \n  group_by(resting_ecg) |&gt; \n  count(heart_disease) |&gt; \n  mutate(freq = n/sum(n))\n\n# A tibble: 6 × 4\n# Groups:   resting_ecg [3]\n  resting_ecg heart_disease     n  freq\n  &lt;fct&gt;       &lt;fct&gt;         &lt;int&gt; &lt;dbl&gt;\n1 LVH         0                82 0.436\n2 LVH         1               106 0.564\n3 Normal      0               267 0.484\n4 Normal      1               285 0.516\n5 ST          0                61 0.343\n6 ST          1               117 0.657"
  },
  {
    "objectID": "eda_r.html#data-visualisation",
    "href": "eda_r.html#data-visualisation",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.3 Data Visualisation",
    "text": "5.3 Data Visualisation\nWhile inspecting the data directly and using summary statistics to describe it is a good first step, data visualisation is a more effective way to explore the data. It allows us to quickly identify patterns and relationships in the data, and to identify any data quality issues that might not be immediately obvious without a visual representation of the data.\nWhen using data visualisation for exploratory purposes, the intent is generally to visualise the way data is distributed, both within and between variables. This can be done using a variety of different types of plots, including histograms, bar charts, box plots, scatter plots, and line plots. How variables are distributed can tell us a lot about the variable itself, and how variables are distributed relative to each other can tell us a lot about the potential relationship between the variables.\nIn this tutorial, we will use the ggplot2 package to create a series of data visualisations to explore the data in more detail. ggplot2 is an incredibly flexible and powerful package for creating data visualisations. While it can be a little difficult to make sense of the syntax at first, it is well worth the effort to learn how to use it. Learning how to use ggplot2 is beyond the scope of this tutorial, but there are a number of excellent resources available online, including the ggplot2 documentation and this ggplot2 cheatsheet.\n\n5.3.1 Visualising Data Distributions\nThe first step in the exploratory process is to visualise the data distributions of key variables in the dataset. This allows us to get a sense of the typical values and central tendency of the variable, as well as identifying any outliers or other data quality issues.\n\n5.3.1.1 Continuous Distributions\nFor continuous variables, we can use histograms to visualise the distribution of the data. We can use the geom_histogram() function to create a histogram of a continuous variable. The binwidth argument can be used to control the width of the bins in the histogram.\n\n# age distribution\ndf |&gt;\n  ggplot(aes(age)) +\n  geom_histogram(binwidth = 5, colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n# max hr distribution\ndf |&gt;\n  ggplot(aes(max_hr)) +\n  geom_histogram(binwidth = 10, colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n# cholesterol distribution\ndf |&gt;\n  ggplot(aes(cholesterol)) +\n  geom_histogram(binwidth = 25, colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n(a) Age\n\n\n\n\n\n(b) Maximum Heart Rate\n\n\n\n\n\n(c) Cholesterol\n\n\n\nFigure 5.1: Histograms plotting the distributions of relevant variables from the heart disease dataset.\n\n\n\n\n# filter zero values\ndf |&gt; \n  filter(cholesterol != 0) |&gt; \n  ggplot(aes(cholesterol)) +\n  geom_histogram(binwidth = 25, colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = \"Cholesterol\", y = NULL)\n\n\n\nFigure 5.2: The distribution of cholesterol readings, with zero values filtered out to better visualise the non-zero distribution.\n\n\n\nThe inflated zero values in the cholesterol distribution suggests that there may be an issue with data quality that needs addressing.\n\n5.3.1.2 Discrete Distributions\nWe can use bar charts to visualise the distribution of discrete variables. We can use the geom_bar() function to create a bar chart of a discrete variable.\n\n# sex distribution\ndf |&gt;\n  ggplot(aes(sex)) +\n  geom_bar(colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n\n# angina distribution\ndf |&gt;\n  ggplot(aes(angina)) + \n  geom_bar(colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n(a) Patient Sex\n\n\n\n\n\n(b) Angina\n\n\n\nFigure 5.3: Bar charts plotting discrete variables from the heart disease dataset.\n\n\n\n\n# heart disease distribution\ndf |&gt;\n  ggplot(aes(heart_disease)) +\n  geom_bar(colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  labs(x = \"Heart Disease\", y = NULL)\n\n\n\nFigure 5.4: The distribution of the outcome variable, heart disease.\n\n\n\n\n5.3.2 Comparing Distributions\nThere are a number of ways to compare the distributions of multiple variables. Bar charts can be used to visualise two discrete variables, while histograms and box plots are useful for comparing the distribution of a continuous variable across the groups of a discrete variable, and scatter plots are particularly useful for comparing the distribution of two continuous variables.\n\n5.3.2.1 Visualising Multiple Discrete Variables\nBar charts are an effective way to visualize the observed relationship (or association, at least) between a discrete explanatory variable and a discrete outcome (whether binary, ordinal, or categorical).\nWe can use the geom_bar() function to create bar charts, but the default behaviour is to display the bars as stacked bars, which is not necessarily ideal for visualising discrete variables (though I’d recommend playing around with this yourself to decide what works in each case)\nThe position argument controls how the bars are displayed. The default position = 'stack' argument will display the bars as stacked bars, while the position = 'dodge' argument will display the bars side-by-side, and the position = 'fill' argument will display the bars as a proportion of the total number of observations in each category.\nFinally, the fill argument splits the bars by a particular variable and display them in different colours.\n\n# heart disease by sex\ndf |&gt;\n  ggplot(aes(heart_disease, fill = sex)) +\n  geom_bar(position = 'dodge', colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Heart Disease\", y = NULL)\n\n# resting ecg\ndf |&gt;\n  ggplot(aes(heart_disease, fill = resting_ecg)) +\n  geom_bar(position = 'dodge', colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Heart Disease\", y = NULL)\n\n# angina\ndf |&gt;\n  ggplot(aes(heart_disease, fill = angina)) +\n  geom_bar(position = 'dodge', colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Heart Disease\", y = NULL)\n\n# fasting bs\ndf |&gt;\n  ggplot(aes(heart_disease, fill = fasting_bs)) +\n  geom_bar(position = 'dodge', colour = \"#333333\", linewidth = 1) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Heart Disease\", y = NULL)\n\n\n\n\n\n(a) Patient Sex\n\n\n\n\n\n(b) Resting ECG\n\n\n\n\n\n\n\n(c) Angina\n\n\n\n\n\n(d) Fasting Blood Sugar\n\n\n\nFigure 5.5: Bar charts plotting discrete variables from the heart disease dataset against the outcome variable, heart disease.\n\n\n\n\n5.3.2.2 Visualising A Continuous Variable Across Discrete Groups\nHistograms and box plots are useful for comparing the distribution of a continuous variable across the groups of a discrete variable.\n\n5.3.2.2.1 Histogram Plots\nWe can use the geom_histogram() function to create histograms. The fill and position arguments can be used to split the bars by a particular variable and display them in different colours, as discussed above.\n\n# age distribution by heart disease\ndf |&gt;\n  ggplot(aes(age, fill = heart_disease)) +\n  geom_histogram(\n    binwidth = 5, position = 'dodge', \n    colour = \"#333333\", linewidth = 1\n    ) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = NULL, y = NULL)\n\n# filter zero values\ndf |&gt; \n  filter(cholesterol != 0) |&gt; \n  ggplot(aes(cholesterol, fill = heart_disease)) +\n  geom_histogram(\n    binwidth = 25, position = 'dodge', \n    colour = \"#333333\", linewidth = 1\n    ) +\n  geom_hline(yintercept = 0, colour = \"#333333\", linewidth = 1) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n(a) Patient Age by Heart Disease\n\n\n\n\n\n(b) Cholesterol (Zero Values Filtered) by Heart Disease\n\n\n\nFigure 5.6: Histograms plotting the distribution of continuous variables from the heart disease dataset against the outcome variable, heart disease.\n\n\n\nThe fact that there is a significantly larger proportion of positive heart disease cases in the zero cholesterol values further demonstrates the need to address this data quality issue.\n\n5.3.2.2.2 Box Plots\nBox plots visualize the characteristics of a continuous distribution over discrete groups. We can use the geom_boxplot() function to create box plots, and the fill() argument to split the box plots by a particular variable and display them in different colours.\nHowever, while box plots can be very useful, they are not always the most effective way of visualising this information, as explained here by Cedric Scherer. This guide uses box plots for the sake of simplicity, but it is worth considering other options when visualising distributions.\n\n# age & heart disease\ndf |&gt;\n  ggplot(aes(age, heart_disease)) +\n  geom_boxplot(size=0.8) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Heart Disease\")\n\n# age & heart disease, split by sex\ndf |&gt;\n  ggplot(aes(age, heart_disease, fill = sex)) +\n  geom_boxplot(size=0.8) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Heart Disease\")\n\n# max hr & heart disease\ndf |&gt;\n  ggplot(aes(max_hr, heart_disease)) +\n  geom_boxplot(size=0.8) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Maximum Heart Rate\", y = \"Heart Disease\")\n\n# max hr & heart disease, split by sex\ndf |&gt;\n  ggplot(aes(max_hr, heart_disease, fill = sex)) +\n  geom_boxplot(size=0.8) +\n  scale_fill_qualitative(palette = \"scw\") +\n  labs(x = \"Maximum Heart Rate\", y = \"Heart Disease\")\n\n\n\n\n\n(a) Patient Age\n\n\n\n\n\n(b) Patient Age, Split by Sex\n\n\n\n\n\n\n\n(c) Maximum Heart Rate\n\n\n\n\n\n(d) Maximum Heart Rate, Split by Sex\n\n\n\nFigure 5.7: Box plots visualising continuous distributions over the discrete outcome variable, heart disease.\n\n\n\n\n5.3.2.3 Visualising Multiple Discrete Variables\nScatter plots are an effective way to visualize how two continuous variables vary together. We can use the geom_point() function to create scatter plots, and the colour argument to split the scatter plots by a particular variable and display them in different colours.\n\n# age & resting bp\ndf |&gt; \n  ggplot(aes(age, resting_bp)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  labs(x = \"Patient Age\", y = \"Resting Blood Pressure\")\n  \n# filter zero values\ndf |&gt;\n  filter(resting_bp != 0) |&gt; \n  ggplot(aes(age, resting_bp)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  labs(x = \"Patient Age\", y = \"Resting Blood Pressure\")\n\n# age & cholesterol\ndf |&gt;\n  filter(cholesterol != 0) |&gt; \n  ggplot(aes(age, cholesterol)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  labs(x = \"Patient Age\", y = \"Cholesterol\")\n\n# age & max hr\ndf |&gt;\n  ggplot(aes(age, max_hr)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  labs(x = \"Patient Age\", y = \"Maximum Heart Rate\")\n\n\n\n\n\n(a) Patient Age & Resting Blood Pressure\n\n\n\n\n\n(b) Patient Age & Resting Blood Pressure (Zero Values Filtered)\n\n\n\n\n\n\n\n(c) Patient Age & Cholesterol (Zero Values Filtered)\n\n\n\n\n\n(d) Patient Age & Maximum Heart Rate\n\n\n\nFigure 5.8: Scatter plots visualising two continuous distributions together.\n\n\n\nThe scatter plot visualising age and resting blood pressure highlights another observation that needs to be removed due to data quality issues.\nIf there appears to be an association between the two continuous variables that you have plotted, as is the case with age and maximum heart rate in the above plot, you can also add a regression line to visualize the strength of that association. The geom_smooth() function can be used to add a regression line to a scatter plot. The method argument specifies the type of regression line to be added, and the se argument specifies whether or not to display the standard error of the regression line.\n\n# age & max hr\ndf |&gt;\n  ggplot(aes(age, max_hr)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  geom_smooth(method = lm, se = FALSE, size = 2, colour='#005EB8') +\n  labs(x = \"Patient Age\", y = \"Maximum Heart Rate\")\n\n\n\nFigure 5.9: Scatter plot visualising the distribution of patient age and maximum heart rate with a regression line fit to the data.\n\n\n\nYou can also include discrete variables by assigning the discrete groups different colours in the scatter plot, and if you add regression lines to these plots, separate regression lines will be fit to the discrete groups. This can be useful for visualising how the association between the two continuous variables varies across the discrete groups.\n\n# age & resting bp, split by heart disease\ndf |&gt;\n  filter(resting_bp != 0) |&gt; \n  ggplot(aes(age, resting_bp, colour = heart_disease)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_colour_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Resting Blood Pressure\")\n\n# age & cholesterol, split by heart disease (with regression line)\ndf |&gt;\n  filter(cholesterol!=0) |&gt; \n  ggplot(aes(age, cholesterol, colour = heart_disease)) +\n  geom_point(size = 3, alpha = 0.8) +\n  geom_smooth(method = lm, se = FALSE, size = 1.5) +\n  scale_colour_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Resting Blood Pressure\")\n\n# age & max hr, split by heart disease (with regression line)\ndf |&gt;\n  ggplot(aes(age, max_hr, colour = heart_disease)) +\n  geom_point(size = 3, alpha = 0.8)+\n  geom_smooth(method = lm, se = FALSE, size = 1.5) +\n  scale_colour_qualitative(palette = \"scw\") +\n  labs(x = \"Patient Age\", y = \"Maximum Heart Rate\")\n\n\n\n\n\n(a) Patient Age & Resting Blood Pressure\n\n\n\n\n\n(b) Patient Age & Cholesterol (Zero Values Filtered)\n\n\n\n\n\n(c) Patient Age & Maximum Heart Rate\n\n\n\nFigure 5.10: Scatter plots visualising continuous distributions together, with the data split and coloured by the discrete outcome variable, heart disease."
  },
  {
    "objectID": "eda_r.html#next-steps",
    "href": "eda_r.html#next-steps",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.4 Next Steps",
    "text": "5.4 Next Steps\nThere are many more visualisation techniques that you can use to explore your data, and you can find a comprehensive list of them on the ggplot2 function reference page. There are also a wide variety of ggplot extension packages that can be used to create more complex visualisations.\nThe next step in the data science process is to build a model to either explain or predict the outcome variable, heart disease. The exploratory work done here can help inform decisions about the choice of the model, and the choice of the variables that will be used to build the model. It will also help clean up the data, particularly the zero values in the cholesterol and resting blood pressure variables, to ensure that the model is built on the best possible data."
  },
  {
    "objectID": "eda_r.html#resources",
    "href": "eda_r.html#resources",
    "title": "5  Exploratory Data Analysis",
    "section": "\n5.5 Resources",
    "text": "5.5 Resources\nThere are a wealth of resources available to help you learn more about data visualisation.\n\nData Visualization: A Practical Introduction\nFundamentals of Data Visualization\nData Visualization with R\n{ggeasy} - Easy Access to {ggplot2} Commands"
  },
  {
    "objectID": "linear_regression_r.html#exploratory-data-analysis",
    "href": "linear_regression_r.html#exploratory-data-analysis",
    "title": "6  Linear Regression",
    "section": "\n6.1 Exploratory Data Analysis",
    "text": "6.1 Exploratory Data Analysis\nFirst we will carry out some exploratory data analysis (EDA) to get a feel for the data, and to see if there are any obvious relationships between the variables that we can use to inform our model.\nWe will use the dplyr package’s glimpse() function to get a quick overview of the data, before using the psych package’s describe() function to calculate summary statistics. The describe() function can compute a wide range of summary statistics, but we will only use a few of them here (the select() function below indicates the summary statistics that will be removed).\nThe knitr package’s kable() function is used to format the table output to make it easier to read in HTML format.\n\n# get a quick overview of the data\nglimpse(df)\n\nRows: 583\nColumns: 7\n$ area_code           &lt;chr&gt; \"E06000001\", \"E06000001\", \"E06000001\", \"E06000001\"…\n$ year                &lt;dbl&gt; 2015, 2016, 2017, 2018, 2015, 2016, 2017, 2018, 20…\n$ life_expectancy     &lt;dbl&gt; 78.73839, 79.07321, 79.08000, 78.81000, 77.78726, …\n$ imd_score           &lt;dbl&gt; 35.037, 35.037, 35.037, 35.037, 40.460, 40.460, 40…\n$ imd_decile          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 4, 4, 4, 4, 4,…\n$ physiological_score &lt;dbl&gt; 89.9, 88.8, 84.8, 84.6, 96.4, 95.8, 96.7, 96.2, 87…\n$ behavioural_score   &lt;dbl&gt; 84.9, 84.4, 82.7, 81.0, 80.3, 82.6, 84.5, 82.2, 96…\n\n\n\ndf |&gt; \n  select(!year) |&gt;\n  janitor::clean_names(case = \"title\", abbreviations = c(\"IMD\")) |&gt; \n  modelsummary::datasummary_skim(align = \"lccddddd\", output = \"gt\")\n\n\n\n\n\n\nTable 6.1:  Summary statistics \n  \n \n      Unique (#)\n      Missing (%)\n      Mean\n      SD\n      Min\n      Median\n      Max\n    \n\n\nLife Expectancy\n562\n0\n81.2 \n 1.6 \n76.5 \n 81.2 \n 86.0 \n\n\nIMD Score\n146\n0\n23.2 \n 8.0 \n 5.8 \n 23.0 \n 45.0 \n\n\nIMD Decile\n10\n0\n 5.4 \n 2.8 \n 1.0 \n  5.0 \n 10.0 \n\n\nPhysiological Score\n294\n0\n99.0 \n10.0 \n78.2 \n 97.9 \n125.9 \n\n\nBehavioural Score\n285\n0\n99.4 \n 8.7 \n72.4 \n100.6 \n120.0 \n\n\n\n\n\n\n\nHaving taken a quick look at the data, we can now start to explore how life expectancy, our outcome variable (the variable that we are trying to explain), varies with the other variables in our dataset (the explanatory variables). We will start by plotting how life expectancy is distributed by IMD score, which is a measure of deprivation. The expectation is that there will be a negative association between the two variables, which means that areas with higher levels of deprivation (i.e. lower IMD scores) will have lower life expectancies.\n\ndf |&gt;\n  ggplot(aes(life_expectancy, imd_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8') +\n  labs(x = \"Life Expectancy\", y = \"IMD Score\")\n\n\n\nFigure 6.1: Plotting the relationship between life expectancy and IMD scores\n\n\n\nThere is a clear negative association between life expectancy and IMD score, which is what we would expect. However, there are some outliers where life expectancy seems to be higher than would be expected given the level of deprivation. The plot seems to suggest that deprivation may have a ‘floor effect’ on life expectancy, i.e. that higher deprivation raises the floor of life expectancy, but that there are other factors that can raise life expectancy above the floor.\nWe can also plot the distribution of life expectancy by the Health Index scores for physiological and behavioural risk factors. These scores are index values that measure the prevalence of physiological and behavioural risk factors that are associated with poor health outcomes, against a baseline value of the national average for each risk factor in 2015. This baseline score is 100, and higher scores mean that an area has a higher prevalence of that risk factor than the national average in 2015, and lower scores mean that the area has lower prevalence. The expectation is that when the risk factor index scores increase, meaning that the risk factors are less prevalent compared against the national average, life expectancy will increase, while a decrease in the scores will be associated with a decrease in life expectancy.\n\ndf |&gt;\n  ggplot(aes(life_expectancy, physiological_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'grey50') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8') +\n  labs(x = \"Life Expectancy\", y = \"Physiological Index\")\n  \n\ndf |&gt;\n  ggplot(aes(life_expectancy, behavioural_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'grey50') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8') +\n  labs(x = \"Life Expectancy\", y = \"Behavioural Index\")\n\n\n\n\n\n(a) Physiological Risk Factor Score\n\n\n\n\n\n(b) Behavioural Risk Factor Score\n\n\n\nFigure 6.2: Plotting the relationship between life expectancy and the Health Index’s physiological and behavioural risk factor scores\n\n\n\nThe plots show that there is a clear negative association between life expectancy and both the physiological and behavioural risk factor scores, which is what we would expect. However, the plots also show that there are some outliers where life expectancy seems to be higher than would be expected given the level of risk factors. The plots seem to suggest that risk factors may have a ‘floor effect’ on life expectancy, i.e. that higher risk factors raise the floor of life expectancy, but that there are other factors that can raise life expectancy above the floor.\nThe above plots suggest there are strong correlations between life expectancy and the three independent variables. We can confirm this by calculating a correlation matrix for the variables in our dataset, using the correlation package.\n\ndf |&gt; \n  group_by(area_code) |&gt;\n  summarise_all(.funs = \"mean\") |&gt; \n  select(life_expectancy, imd_score, behavioural_score, physiological_score) |&gt;\n  janitor::clean_names(case = \"title\", abbreviations = c(\"IMD\")) |&gt; \n  modelsummary::datasummary_correlation(output = \"gt\")\n\n\n\n\n\n\nTable 6.2:  Correlations between life expectancy, deprivation, and socioeconomic\nfactors \n  \n \n      Life Expectancy\n      IMD Score\n      Behavioural Score\n      Physiological Score\n    \n\n\nLife Expectancy\n1\n.\n.\n.\n\n\nIMD Score\n-.84\n1\n.\n.\n\n\nBehavioural Score\n.91\n-.83\n1\n.\n\n\nPhysiological Score\n.60\n-.39\n.62\n1\n\n\n\n\n\n\n\nThere are strong correlations between life expectancy and the three explanatory variables, however, the correlation between IMD score and the two Health Index scores is also relatively strong, and in the case of behavioural risk factors, it is very strong. This could be an issue, as it could mean that the model is not able to distinguish between the effects of deprivation and the effects of risk factors on life expectancy. This is known as multicollinearity, and it can cause problems when interpreting the results of a regression model.\nWe can also visualise the correlation between our explanatory variables, to get a better sense of how they might be related to each other. We will start by plotting IMD score against the two Health Index scores, using scatter plots, before plotting the distributions of the scores by IMD decile, using density plots. We can use the ggridges package to plot the density plots, which can be useful for plotting how continuous variables are distributed across discrete groups.\n\ndf |&gt;\n  ggplot(aes(imd_score, physiological_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'grey50') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8') +\n  labs(x = \"IMD Score\", y = \"Physiological Index\")\n\ndf |&gt;\n  ggplot(aes(imd_score, behavioural_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'grey50') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8') +\n  labs(x = \"IMD Score\", y = \"Behavioural Index\")\n\ndf |&gt;\n  mutate(imd_decile = as.factor(imd_decile)) |&gt;\n  ggplot(aes(physiological_score, imd_decile)) +\n  ggridges::geom_density_ridges() +\n  labs(x = \"IMD Decile\", y = \"Physiological Index\")\n\ndf |&gt;\n  mutate(imd_decile = as.factor(imd_decile)) |&gt;\n  ggplot(aes(behavioural_score, imd_decile)) +\n  ggridges::geom_density_ridges() +\n  labs(x = \"IMD Decile\", y = \"Behavioural Index\")\n\n\n\n\n\n(a) IMD & Physiological Score\n\n\n\n\n\n(b) IMD & Behavioural Score\n\n\n\n\n\n\n\n(c) IMD Decile & Physiological Score\n\n\n\n\n\n(d) IMD Decile & Behavioural Score\n\n\n\nFigure 6.3: Plotting the relationship between IMD scores/deciles and the Health Index’s physiological and behavioural risk factor scores\n\n\n\nThe correlations are a little more obvious when visualised. The behavioural index scores have a negative linear association with IMD scores (and a positive linear association with IMD decile). This means that areas with higher level of deprivation also tend to have a higher prevalence of behavioural risk factors that are associated with poor health outcomes. The physiological index scores have a similar association with deprivation, but the relationship is not as strong as the behavioural risk factors, and there is significant variance around the linear trend."
  },
  {
    "objectID": "linear_regression_r.html#linear-regression",
    "href": "linear_regression_r.html#linear-regression",
    "title": "6  Linear Regression",
    "section": "\n6.2 Linear Regression",
    "text": "6.2 Linear Regression\nFirst we will transform each of the explanatory variables to make them a little easier to interpret (particularly with regard to the intercept). If we don’t transform the variables, the intercept will be based on zero values of each explanatory variable, which is not very meaningful. For example, a zero value of either risk factors variable is effectively meaningless because the ONS Health Index is centred around a baseline value of 100 (which is the average value for England in 2015, the first year for which the Health Index was calculated).\nThese transformations won’t impact the regression results, but will just make the results easier to interpret and explain, as the intercept is no longer based on zero values of each explanatory variable.\n\n# calculate the imd score mean and standard deviation\nimd_centre &lt;- mean(df$imd_score)\nimd_scale &lt;- sd(df$imd_score)\n\n# use 100 as the centre for the risk factors and calculate the standard deviation\nphysiological_centre &lt;- 100\nphysiological_scale &lt;- sd(df$physiological_score)\nbehavioural_centre &lt;- 100\nbehavioural_scale &lt;- sd(df$physiological_score)\n\n# transform the variables\ndf &lt;-\n  df |&gt;\n  mutate(\n    imd_transformed = (imd_score - imd_centre)/imd_scale,\n    physiological_transformed = \n      (physiological_score - physiological_centre)/physiological_scale,\n    behavioural_transformed = \n      (behavioural_score - behavioural_centre)/behavioural_scale\n  )\n\n# check the transformations\nglimpse(df)\n\nRows: 583\nColumns: 10\n$ area_code                 &lt;chr&gt; \"E06000001\", \"E06000001\", \"E06000001\", \"E060…\n$ year                      &lt;dbl&gt; 2015, 2016, 2017, 2018, 2015, 2016, 2017, 20…\n$ life_expectancy           &lt;dbl&gt; 78.73839, 79.07321, 79.08000, 78.81000, 77.7…\n$ imd_score                 &lt;dbl&gt; 35.037, 35.037, 35.037, 35.037, 40.460, 40.4…\n$ imd_decile                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 4, 4, 4,…\n$ physiological_score       &lt;dbl&gt; 89.9, 88.8, 84.8, 84.6, 96.4, 95.8, 96.7, 96…\n$ behavioural_score         &lt;dbl&gt; 84.9, 84.4, 82.7, 81.0, 80.3, 82.6, 84.5, 82…\n$ imd_transformed           &lt;dbl&gt; 1.4739156, 1.4739156, 1.4739156, 1.4739156, …\n$ physiological_transformed &lt;dbl&gt; -1.0087421, -1.1186051, -1.5181069, -1.53808…\n$ behavioural_transformed   &lt;dbl&gt; -1.5081194, -1.5580571, -1.7278454, -1.89763…\n\n\nHaving transformed the variables, we can now fit a linear regression model to the data. We will use the lm() function from the Base R stats package. Constructing a linear regression in R is very simple, and the syntax is relatively intuitive. The first argument in the function is the formula, which is the dependent variable on the left-hand side, and the independent variables on the right-hand side, separated by a ~ symbol. The second required argument is the data argument, which gives the lm() function a dataset to fit the regression to. There are a number of optional arguments that can be passed to the function to control the model fitting process, but we will use the default values for now.\nAfter fitting the regression model, we can call the summary() function to get a summary of the model results. This will give us the model coefficients, standard errors, t-statistics, p-values, and \\(R^2\\) value. However, in this instance we will use the sjPlot::tab_model() function to get a nice table of the model results which renders better in HTML.\n\n# fit linear regression\nmodels &lt;- list(\n  \"OLS (Raw)\" = \n    lm(life_expectancy ~ imd_score + physiological_score + \n         behavioural_score, data = df),\n  \"OLS (Normalised)\" =\n    lm(life_expectancy ~ imd_transformed +\n         physiological_transformed + behavioural_transformed, data = df)\n  )\n\n# get model results table\nmodels |&gt; \n  modelsummary::modelsummary(\n    estimate = \"estimate\",\n    statistic = \"conf.int\", \n    stars = TRUE,\n    coef_rename = c(\n      \"imd_score\" = \"IMD Score\",\n      \"imd_transformed\" = \"IMD Score\",\n      \"physiological_score\" = \"Physiological Score\",\n      \"physiological_transformed\" = \"Physiological Score\",\n      \"behavioural_score\" = \"Behavioural Score\",\n      \"behavioural_transformed\" = \"Behavioural Score\"\n      ),\n    gof_map = c(\"nobs\", \"r.squared\",  \"rmse\"),\n    output = \"gt\"\n  ) |&gt; \n  gt::tab_source_note(source_note = \"Source: PHE Fingertips/ONS Health Index\")\n\n\n\n\n\n\nTable 6.3:  Regression model estimating the effect of socioeconomic factors on\nlife expectancy \n  \n \n      OLS (Raw)\n      OLS (Normalised)\n    \n\n\n(Intercept)\n71.592***\n81.245***\n\n\n\n[70.367, 72.816]\n[81.193, 81.298]\n\n\nIMD Score\n-0.077***\n-0.623***\n\n\n\n[-0.089, -0.066]\n[-0.713, -0.533]\n\n\nPhysiological Score\n0.024***\n0.239***\n\n\n\n[0.017, 0.030]\n[0.173, 0.304]\n\n\nBehavioural Score\n0.091***\n0.908***\n\n\n\n[0.079, 0.103]\n[0.789, 1.026]\n\n\nNum.Obs.\n583\n583\n\n\nR2\n0.844\n0.844\n\n\nRMSE\n0.64\n0.64\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n\nSource: PHE Fingertips/ONS Health Index"
  },
  {
    "objectID": "linear_regression_r.html#evaluating-and-interpreting-regression-models",
    "href": "linear_regression_r.html#evaluating-and-interpreting-regression-models",
    "title": "6  Linear Regression",
    "section": "\n6.3 Evaluating and Interpreting Regression Models",
    "text": "6.3 Evaluating and Interpreting Regression Models\nWhen we fit a regression model, we have to consider both how well the model fits the data (evaluation) and the influence that each explanatory variable is having on the outcome (interpretation). The evaluation process is making sure that the model is doing a good enough job that we can reasonably draw some conclusions from it, while the interpretation process is understanding the conclusions we can draw. It’s important to understand the purpose that both of these processes serve, and to understand that the evaluation process is just kicking the tires, while what we are ultimately concerned with is the interpretation.\nThere are several elements to consider when interpreting the results of a linear regression model. Broadly speaking the focus should be on the magnitude and the precision of the model coefficients. The magnitude of the coefficients indicates the strength of the association between the explanatory variables and the outcome variable, while the precision of the coefficients indicates the uncertainty in the estimated coefficients. We use the regression model coefficients to understand the magnitude of the effect and standard errors (and confidence intervals) to understand the precision of the effect.\n\n6.3.1 Model Evaluation\nThere are a number of ways we might evaluate model performance, but the starting point should be the model’s \\(R^2\\) and the p-values of the model coefficients. These can help us consider whether the model fits the data and whether we have enough data to trust the conclusions we can draw from each explanatory variable.\n\n6.3.1.1 \\(R^2\\)\n\nThe \\(R^2\\) value is the proportion of the variance in the outcome that is explained by the model. It is often used as a measure of the goodness of fit of the model, and by extension the extent to which the specified model explains the outcome, but caution should be applied when using \\(R^2\\) this way, because \\(R^2\\) can be misleading and doesn’t always tell the whole story. One of the main problems with \\(R^2\\) when used in an inferential context is that adding more variables to the model will always increase the \\(R^2\\) value, even if the new variables are not actually related to the outcome. Adjusted \\(R^2\\) values are used to account for this problem, but they don’t resolve all issues with \\(R^2\\), so it is important to consider \\(R^2\\) as just one of a number of indicators of the model’s fit. If the \\(R^2\\) is high, this doesn’t necessarily mean that the model is a good fit for the data, but if the \\(R^2\\) is particularly low, this might be a good indication that the model is not a good fit for the data, and at the very least it is good reason to be cautious about the results of the analysis, and to take a closer look at the model and the data to understand why the \\(R^2\\) is so low.\nDefining high/low \\(R^2\\) values is dependent on the context of the analysis. The theory behind the model should inform our expectations about the \\(R^2\\) value, and we should also consider the number of explanatory variables in the model (adding variables to a regression will increase the \\(R^2\\)). If the theory suggests that the model should explain a small amount of the variance in the outcome, and the model has only a few explanatory variables, then a low \\(R^2\\) value might not be a cause for concern.\nThe Adjusted \\(R^2\\) value for the above regression model is 0.843, which means that the model explains 84.3% of the observed variance in life expectancy in our dataset. This is a relatively high \\(R^2\\) value.\n\n6.3.1.2 P-Values\nP-values are the probability of observing a coefficient as large as the estimated coefficient for a particular explanatory variable, given that the null hypothesis is true (i.e. that the coefficient is equal to zero), indicating that no effect is present. This can be a little tricky to understand at first, but a slightly crude way of thinking about this is that p-values are an estimate of the probability that the model coefficient you observe is actually distinguishable from zero.\nP-values are used to test the statistical significance of the coefficients. If the p-value is less than the significance level then the coefficient is statistically significant, and we can reject the null hypothesis that the coefficient is equal to zero. If the p-value is greater than the significance level, then the coefficient is not statistically significant, and we cannot reject the null hypothesis that the coefficient is equal to zero. The significance level is often set at 0.05, which means that we are willing to accept a 5% chance of incorrectly rejecting the null hypothesis. This means that if the p-value is less than 0.05, then there is a greater than 95% probability that the coefficient is not equal to zero. However, the significance level is a subjective decision, and can be set at any value. For example, if the significance level is set at 0.01, then we are willing to accept a 1% chance of incorrectly rejecting the null hypothesis.\nIn recent years p-values (and the wider concept of statistical significance) have been the subject of a great deal of discussion in the scientific community, because there is a belief among many practitioners that a lot of scientific research places too much weight in the p-value, treating it as the most important factor when interpreting a regression model. I think this perspective is correct, but I think dismissing p-values entirely is a mistake too. The real problem is not with p-values themselves, but with the goal of an analysis being the identification of a statistically significant effect.\nI think the best way to think about p-values is the position prescribed by Gelman, Hill, & Vehtari in Regression & Other Stories. Asking whether our coefficient is distinguishable from zero is the wrong question. As analysts, we should typically know enough about the context we are studying to build regression models with explanatory variables that will some effect. The question is how much. A statistically insignificant model coefficient is less a sign that a variable has no effect, and more a sign that there is insufficient data to detect a meaningful effect. If our model coefficients are not statistically significant, it is simply a good indication that we need more data. If the model coefficients are statistically significant, then our focus should be on interpreting the coefficients, not on the p-value itself.\n\n6.3.2 Model Interpretation\nModel interpretation is the process of understanding the influence that each explanatory variable has on the outcome. This involves understanding the direction, the magnitude, and the precision of the model coefficients. The coefficients themselves will tell us about direction and magnitude, while the standard errors and confidence intervals will tell us about precision.\n\n6.3.2.1 Model Coefficients\nThe model coefficients are the estimated values of the regression coefficients, or the estimated change in the outcome variable for a one unit change in the explanatory variable, while holding all other explanatory variables constant. The intercept is the estimated value of the outcome variable when all of the explanatory variables are equal to zero (or the baseline values if the variables are transformed).\nA regression’s coefficients are the most important part of interpreting the model. They specify the association between the explanatory variables and the outcome, telling us both the direction and the magnitude of the association. The direction of the association is indicated by the sign of the coefficient (positive = outcome increases with the explanatory variable, negative = outcome decreases with the explanatory variable). The magnitude of the association is indicated by the size of the coefficient (the larger the coefficient, the stronger the association). If the direction of the association is going in the wrong direction to that which we expect, this obviously indicates issues either in the theory or in the model, and if the magnitude of the effect is too small to be practically significant, then either the explanatory variable is not a good predictor of the outcome, or the model is not doing a good job of fitting the data (either the explanatory variable doesn’t really matter or the model is not capturing the true relationship between the explanatory variables and the outcome). However, while the direction is relatively easy to interpret, magnitude is not, because what counts as a practically significant effect is dependent on the context of the analysis.\n\n6.3.2.2 Standard Errors\nThe standard errors are the standard deviation of the estimated coefficients. This is a measure of the precision of the estimated association between the explanatory variables and the outcome. The larger the standard error, the less precise the coefficient estimate. Precisely measured coefficients suggest that the model is fitting the data well, and we can be confident that the association between the explanatory variables and the outcome is real, and not just a result of random variation in the data. Defining what counts as ‘precise’ in any analysis, much like the coefficient magnitude, is dependent on the context of the analysis. However, in general, a standard error of less than 0.5 is considered to be a good estimate, and a standard error of less than 0.25 is considered to be a very precise estimate.\nIt is important to recognise that a precise coefficient alone cannot be treated as evidence of a causal effect between the explanatory variable and the outcome but it is, at least, a good indication that the explanatory variable is a good predictor of the outcome. In order to establish a causal relationship, we may consider methods that are more robust to the presence of confounding or omitted variables, the gold standard being randomised controlled trials, but with a strong theoretical basis and a good understanding of the data, we can make a good case for a causal relationship based on observational data."
  },
  {
    "objectID": "linear_regression_r.html#next-steps",
    "href": "linear_regression_r.html#next-steps",
    "title": "6  Linear Regression",
    "section": "\n6.4 Next Steps",
    "text": "6.4 Next Steps"
  },
  {
    "objectID": "linear_regression_r.html#resources",
    "href": "linear_regression_r.html#resources",
    "title": "6  Linear Regression",
    "section": "\n6.5 Resources",
    "text": "6.5 Resources\n\nStatQuest: Linear Regression\nRegression & Other Stories\nStatistical Inference via Data Science\nA User’s Guide to Statistical Inference and Regression"
  },
  {
    "objectID": "ml_workflow_r.html#data",
    "href": "ml_workflow_r.html#data",
    "title": "7  End-to-End Machine Learning Workflow",
    "section": "\n7.1 Data",
    "text": "7.1 Data\n\n# inspect the data\nglimpse(df)\n\nRows: 918\nColumns: 10\n$ age                &lt;dbl&gt; 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39,…\n$ sex                &lt;fct&gt; M, F, M, F, M, M, F, M, M, F, F, M, M, M, F, F, M, …\n$ resting_bp         &lt;dbl&gt; 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 1…\n$ cholesterol        &lt;dbl&gt; 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 2…\n$ fasting_bs         &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ resting_ecg        &lt;fct&gt; Normal, Normal, ST, Normal, Normal, Normal, Normal,…\n$ max_hr             &lt;dbl&gt; 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 14…\n$ angina             &lt;fct&gt; N, N, N, Y, N, N, N, N, Y, N, N, Y, N, Y, N, N, N, …\n$ heart_peak_reading &lt;dbl&gt; 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0…\n$ heart_disease      &lt;fct&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n\n\n\n7.1.1 Train/Test Split\nOne of the central tenets of machine learning is that the model should not be trained on the same data that it is evaluated on. This is because the model could learn spurious/random patterns and correlations in the training data, and this will harm the model’s ability to make good predictions on new, unseen data. There are many way of trying to resolve this, but the most simple approach is to split the data into a training and test set. The training set will be used to train the model, and the test set will be used to evaluate the model.\n\n# set random seed to ensure reproducibility\nset.seed(123)\n\n# split data into train/test sets\ntrain_test_split &lt;-\n  rsample::initial_split(df,\n                         strata = heart_disease,\n                         prop = 0.7)\n\n# extract training and test sets\ntrain_df &lt;- rsample::training(train_test_split)\ntest_df &lt;- rsample::testing(train_test_split)\n\n\n7.1.2 Cross-Validation Folds\nOn top of the train/test split, we will also use cross-validation to train our models. Cross-validation is a method of training a model on a subset of the data, and then evaluating the model on the remaining data. This process is repeated multiple times, and the average performance is used to evaluate the model. This helps make the training process more generalisable.\nFor more information on cross-validation and how it can be used to train models the Resampling Methods section in Tidy Modeling with R discusses cross-validation in detail, in the wider context of resampling methods for training machine learning models. This helps to provide context for the purpose of cross-validation, as well as discussing some of the strategies for cross-validation.\nWe will use 5-fold stratified cross-validation to train our models. This means that the training data will be split into 5 folds, ensuring that each fold has the same proportion of the target classes. Each fold will be used as a test set once, and the remaining folds will be used as training sets. This will be repeated 5 times, and the average performance will be used to evaluate the model.\n\n# set random seed to ensure reproducibility\nset.seed(123)\n\n# create cross-validation folds\ntrain_folds &lt;- vfold_cv(train_df, v = 5, strata = heart_disease)\n\n# inspect the folds\ntrain_folds\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [513/129]&gt; Fold1\n2 &lt;split [513/129]&gt; Fold2\n3 &lt;split [514/128]&gt; Fold3\n4 &lt;split [514/128]&gt; Fold4\n5 &lt;split [514/128]&gt; Fold5\n\n\n\n7.1.3 Preprocessing\nThe purpose of preprocessing is to prepare the data for model fitting. This can take a number of different forms, but some of the most common preprocessing steps include:\n\nNormalizing/Standardizing data\nOne-hot encoding categorical variables\nRemoving outliers\nImputing missing values\n\nWe will build two different preprocessing recipes and see which performs better. The first will be a ‘basic’ recipe that one-hot encodes all categorical features, and removes any features that have strong correlation with another feature in the dataset. The second recipe will include both these steps but will also normalize the numeric features so that they have a mean of zero and a standard deviation of one.\n\n# specify simple preprocessing recipe\nbasic_recipe &lt;- recipe(heart_disease ~ ., data = train_df) |&gt;\n  # one-hot encode categorical variables\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;\n  # remove features that have strong correlation with another feature\n  step_corr(all_numeric_predictors(), threshold = .5)\n\n# specify scaled preprocessing recipe\nscaled_recipe &lt;- recipe(heart_disease ~ ., data = train_df) |&gt;\n  # normalize data so that it has mean zero and sd one\n  step_normalize(all_numeric_predictors()) |&gt; \n  # one-hot encode categorical variables\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;\n  # remove features that have strong correlation with another feature\n  step_corr(all_numeric_predictors(), threshold = .5)\n\n# inspect basic preprocessing recipe\nbasic_recipe |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 642 × 11\n     age resting_bp cholesterol max_hr heart_peak_reading heart_disease sex_M\n   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1    37        130         283     98                0   0                 1\n 2    39        120         339    170                0   0                 1\n 3    45        130         237    170                0   0                 0\n 4    48        120         284    120                0   0                 0\n 5    39        120         204    145                0   0                 1\n 6    42        115         211    137                0   0                 0\n 7    54        120         273    150                1.5 0                 0\n 8    43        100         223    142                0   0                 0\n 9    44        120         184    142                1   0                 1\n10    40        130         215    138                0   0                 1\n# ℹ 632 more rows\n# ℹ 4 more variables: fasting_bs_X1 &lt;dbl&gt;, resting_ecg_LVH &lt;dbl&gt;,\n#   resting_ecg_ST &lt;dbl&gt;, angina_Y &lt;dbl&gt;\n\n# inspect scaled preprocessing recipe\nscaled_recipe |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 642 × 11\n       age resting_bp cholesterol  max_hr heart_peak_reading heart_disease sex_M\n     &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1 -1.74       -0.133      0.753  -1.52               -0.833 0                 1\n 2 -1.53       -0.666      1.25    1.31               -0.833 0                 1\n 3 -0.900      -0.133      0.341   1.31               -0.833 0                 0\n 4 -0.585      -0.666      0.762  -0.656              -0.833 0                 0\n 5 -1.53       -0.666      0.0457  0.328              -0.833 0                 1\n 6 -1.21       -0.933      0.108   0.0134             -0.833 0                 0\n 7  0.0436     -0.666      0.663   0.525               0.604 0                 0\n 8 -1.11       -1.73       0.216   0.210              -0.833 0                 0\n 9 -1.00       -0.666     -0.133   0.210               0.125 0                 1\n10 -1.42       -0.133      0.144   0.0528             -0.833 0                 1\n# ℹ 632 more rows\n# ℹ 4 more variables: fasting_bs_X1 &lt;dbl&gt;, resting_ecg_LVH &lt;dbl&gt;,\n#   resting_ecg_ST &lt;dbl&gt;, angina_Y &lt;dbl&gt;"
  },
  {
    "objectID": "ml_workflow_r.html#model-training",
    "href": "ml_workflow_r.html#model-training",
    "title": "7  End-to-End Machine Learning Workflow",
    "section": "\n7.2 Model Training",
    "text": "7.2 Model Training\nThere are many different types of models that can be used for classification problems, and selecting the right model for the problem at hand can be difficult when you are first starting out with machine learning.\nSimple models like linear and logistic regressions are often a good place to start and can be used to get a better understanding of the data and the problem at hand, and can give you a good idea of baseline performance before building more complex models to improve performance. Another example of a simple model is K-Nearest Neighbours (KNN), which is a non-parametric model that can be used for both classification and regression problems.\nWe will fit a logistic regression and a KNN, as well as fitting a Random Forest model, which is a good example of a slightly more complex model that will often perform well on structured data.\n\n7.2.1 Model Selection\n\n# create a list of preprocessing recipes\npreprocessers &lt;-\n  list(\n    basic = basic_recipe,\n    scaled = scaled_recipe\n  )\n\n# create a list of candidate models\nmodels &lt;- list(\n  # logistic regression\n  log = \n    logistic_reg(\n      # tune regularization parameters\n      penalty = tune(),\n      mixture = tune()\n    ) |&gt;\n    set_engine('glmnet') |&gt;\n    set_mode('classification'),\n  # k-nearest neighbours\n  knn = \n    nearest_neighbor(\n      # tune weighting function and number of neighbours\n      weight_func = tune(),\n      neighbors = tune()\n      ) |&gt;\n    set_engine('kknn') |&gt;\n    set_mode('classification'),\n  # random forest\n  rf =\n    rand_forest(\n      # number of trees\n      trees = 1000,\n      # tune number of features to consider at each split\n      # and minimum number of observations in a leaf\n      mtry = tune(),\n      min_n = tune()\n      ) |&gt;\n    set_engine('ranger') |&gt;\n    set_mode('classification')\n  )\n\n# combine these lists as a workflow set\nmodel_workflows &lt;-\n  workflow_set(\n    preproc = preprocessers,\n    models = models\n  )\n\n# specify the metrics on which the models will be evaluated\neval_metrics &lt;- metric_set(f_meas, accuracy)\n\nHaving defined the preprocessing recipes and candidate models, we can now train and tune the models. We will use the tune_grid() function to tune the models, which will search over a grid of hyperparameter values to find the best performing model. We will use 5-fold cross-validation to train the models, and will evaluate the models using F1 score and accuracy.\nFor a more detailed discussion of hyperparameter tuning, and the different methods for tuning in tidymodels, the Model Tuning and the Dangers of Overfitting section of Tidy Modeling with R is a good place to start. In addition, the AWS overview of hyperparameter tuning is a good resource if you are only looking for a brief overview. Finally, for an in-depth discussion about how hyperparameter tuning works, including the mathematics behind it, Jeremy Jordan’s Hyperparameter Tuning for Machine Learning Models blogpost is thorough but easy to follow.\n\n# train and tune models\ntrained_models &lt;- \n  model_workflows |&gt;\n  workflow_map(\n    # function to use for hyperparameter tuning\n    fn = 'tune_grid',\n    # cv folds for resampling\n    resamples = train_folds,\n    # grid of hyperparameter values to search over\n    grid = 10,\n    # metrics to evaluate model performance\n    metrics = eval_metrics,\n    verbose = TRUE,\n    seed = 123,\n    # control parameters for tuning\n    control = control_grid(\n      save_pred = TRUE,\n      parallel_over = 'everything',\n      save_workflow = TRUE)\n    )\n\n\n7.2.1.1 Comparing Model Performance\nOnce the models have been trained we can inspect the results to see which model performed best. We can also plot the performance of the models to get a better idea of how they performed.\n\n# inspect the best performing models\ntrained_models |&gt;\n  rank_results(select_best=TRUE) |&gt; \n  filter(.metric == 'f_meas') |&gt; \n  select(wflow_id, model, f1 = mean)\n\n# A tibble: 6 × 3\n  wflow_id   model               f1\n  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n1 basic_rf   rand_forest      0.786\n2 scaled_rf  rand_forest      0.784\n3 scaled_knn nearest_neighbor 0.771\n4 basic_knn  nearest_neighbor 0.771\n5 basic_log  logistic_reg     0.770\n6 scaled_log logistic_reg     0.770\n\n\n\nmetric_labels = c(\"accuracy\" = \"Accuracy\", \"f_meas\" = \"F1\")\n\n# plot performance\ntrained_models |&gt; \n  autoplot() +\n  facet_wrap(~ .metric, labeller = as_labeller(metric_labels)) +\n  scale_shape(guide = 'none') +\n  scwplot::scale_colour_qualitative(\n    labels = c(\"Logistic Regression\", \"KNN\", \"Random Forest\"),\n    palette = \"scw\"\n    ) +\n  guides(colour = guide_legend(override.aes = list(size=2, linewidth = .8)))\n\n# select best models and plot performance\ntrained_models |&gt; \n  autoplot(select_best = TRUE) +\n  facet_wrap(~ .metric, labeller = as_labeller(metric_labels)) +\n  labs(y = NULL) +\n  scale_shape(guide = 'none') +\n  scwplot::scale_colour_qualitative(\n    labels = c(\"Logistic Regression\", \"KNN\", \"Random Forest\"),\n    palette = \"scw\"\n    ) +\n  guides(colour = guide_legend(override.aes = list(size=2, linewidth = .8)))\n\n\n\n\n\n(a) All Models\n\n\n\n\n\n(b) Best Models\n\n\n\nFigure 7.1: Comparing accuracy and f1 score of Random Forest, KNN, and Logistic Regression models"
  },
  {
    "objectID": "ml_workflow_r.html#finalising-model",
    "href": "ml_workflow_r.html#finalising-model",
    "title": "7  End-to-End Machine Learning Workflow",
    "section": "\n7.3 Finalising Model",
    "text": "7.3 Finalising Model\n\n# save best performing model\nbest_results &lt;-\n   trained_models |&gt;  \n   extract_workflow_set_result('basic_rf') |&gt;  \n   select_best(metric = 'f_meas')\n\nbest_results\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     3    30 Preprocessor1_Model01"
  },
  {
    "objectID": "ml_workflow_r.html#fitting-model-on-test-data",
    "href": "ml_workflow_r.html#fitting-model-on-test-data",
    "title": "7  End-to-End Machine Learning Workflow",
    "section": "\n7.4 Fitting Model on Test Data",
    "text": "7.4 Fitting Model on Test Data\nHaving selected the best performing model, we can now fit the model to the test data.\n\n# fit model on test data\nrf_test_results &lt;-\n   trained_models |&gt;\n   # extract the best performing model\n   extract_workflow('basic_rf') |&gt;\n   finalize_workflow(best_results) |&gt;\n   # fit to test data and evaluate performance\n   last_fit(split = train_test_split, metrics=eval_metrics)"
  },
  {
    "objectID": "ml_workflow_r.html#model-evaluation",
    "href": "ml_workflow_r.html#model-evaluation",
    "title": "7  End-to-End Machine Learning Workflow",
    "section": "\n7.5 Model Evaluation",
    "text": "7.5 Model Evaluation\nFinally, we can inspect the performance of the model on the test data.\n\n# inspect model performance on evaluation metrics\ncollect_metrics(rf_test_results)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 f_meas   binary         0.835 Preprocessor1_Model1\n2 accuracy binary         0.848 Preprocessor1_Model1\n\n# inspect confusion matrix\nrf_test_results |&gt; \n  conf_mat_resampled(tidy=FALSE)\n\n    0   1\n0 106  25\n1  17 128\n\n\n\n# plot confusion matrix\nrf_test_results |&gt;\n  conf_mat_resampled(tidy=FALSE) |&gt; \n  autoplot(type='heatmap')\n\n\n\nFigure 7.2: Confusion matrix visualising model performance, comparing predicted and actual values"
  },
  {
    "objectID": "ml_workflow_r.html#next-steps",
    "href": "ml_workflow_r.html#next-steps",
    "title": "7  End-to-End Machine Learning Workflow",
    "section": "\n7.6 Next Steps",
    "text": "7.6 Next Steps\nIn this post, we have covered a simple but robust machine learning workflow. We have have used the tidymodels package to fit a logistic regression model, a k-nearest neighbours model, and a random forest model to the heart disease dataset. We used cross-validation to make the results more generalizable, and we used hyperparameter tuning to improve model performance.\nThe next steps would be to consider what strategies for cross-validation would be most appropriate for the model we are building, what hyperparameter tuning process would produce the best performance, and some more complex algorithms that might outperform the models we’ve built here."
  },
  {
    "objectID": "ml_workflow_r.html#resources",
    "href": "ml_workflow_r.html#resources",
    "title": "7  End-to-End Machine Learning Workflow",
    "section": "\n7.7 Resources",
    "text": "7.7 Resources\nThere are lots of great (and free) introductory resources for machine learning:\n\n\nMachine Learning University (Python)\nMLU Explain\n\nMachine Learning for Beginners (Python/R)\n\nFor a guided video course, Data Talks Club’s Machine Learning Zoomcamp (available on Github and Youtube) is well-paced and well-presented, covering a variety of machine learning methods and even covering some of the aspects that introductory course often skip over, like deployment and data engineering principles. However, while the ML Zoomcamp course is intended as an introduction to machine learning, it does assume a certain level of familiarity with programming (the course uses Python) and software engineering. The appendix section of the course is definitely helpful for bridging some of the gaps in the course, but it is still worth being aware of the way the course is structured.\nIf you are keen to learn more about machine learning but are particularly focused on (or already have experience in) R, the ML Zoomcamp may not be the best fit. If you want to learn about machine learning in R, Max Kuhn & Julia Silge’s Tidy Modeling with R is a great place to start.\nIf you are looking for something that goes into greater detail about a wide range of machine learning methods, then there is no better resource than An Introduction to Statistical Learning (or its accompanying online course), by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\nFinally, if you are particularly interested in learning the mathematics that underpins machine learning, I would highly recommend Mathematics of Machine Learning, which is admittedly not free but is very, very good. If you want to learn about the mathematics of machine learning but are not comfortable enough tackling the Mathematics of ML book, the StatQuest and 3Blue1Brown Youtube channels are both really accessible and well-presented."
  },
  {
    "objectID": "installing_r.html#programming-environments",
    "href": "installing_r.html#programming-environments",
    "title": "Installing R",
    "section": "Programming Environments",
    "text": "Programming Environments\n\nIntegrated Development Environments (IDEs)\nIntegrated Development Environments (IDEs) are tools that can be used to write, run, and debug code. They are a great way to get started with a new language, because they can help you to avoid some of the more common mistakes that you might make when you are first learning how to code.\nGetting from an R installation to running your first lines of code is a little easier than doing the same with Python. The biggest reason for this is the monopoly that Posit (formerly RStudio) has over the tools for using R.\nRStudio is the best available Integrated Development Environment (IDE) for writing R code, and it is highly recommended that you stick to using RStudio when you first start using R. There are other IDEs and text editors that can handle R (for example, VS Code), but when you start out, it is easiest to stick to RStudio.\n\n\nPackage Management\nR has a built-in package management function, install.packages(), which can be used to install packages from CRAN. If you need to install a package from other sources, you can use the devtools package. For example, you can install packages from Github using the devtools::install_github() function.\n\n\nVirtual Environments\nVirtual environments are a way of creating isolated environments for your code. This means that you can have different versions of the same package installed in different virtual environments, and you can switch between them as needed. This is particularly useful when you are working on multiple projects that require different versions of the same package. It also means that you can have different versions of the same package installed on your local machine without having to worry about breaking your code.\nWhile it is possible to install packages in both R and Python without using virtual environments (and may be easier in the short term), it is recommended to use a virtual environment when building data science projects. This is because it helps to avoid problems that can occur when you have multiple versions of the same package installed on your local machine, and it also helps to ensure that your code will run on other people’s machines.\nThe best way to create virtual environments in R is to use the renv package. Virtual environments can be set up for R by running renv::init() in the console. This will create a new folder called renv in your project directory, and will create a new virtual environment for your project. You can then install packages in this virtual environment using the install.packages() function, or by using the renv::install() function."
  },
  {
    "objectID": "version_control.html",
    "href": "version_control.html",
    "title": "Version Control",
    "section": "",
    "text": "Version control systems like Git are vital for the management of any codebase. Version control means that changes to the code (and anything else in the repository) can be tracked over time. This is a software engineering best-practice that has also been adopted in data science. It is an excellent ‘habit’ to learn when working with code, because it allows you to track changes, and therefore, revert to previous versions of the code if necessary. It also makes collaboration significantly easier, as it allows multiple people to work on the same codebase without creating conflicts.\nIf you are new to Git, a good place to start is the NHS-R Git Training. Their Introduction to Git will help you understand why version control is necessary, what Git & GitHub can offer, and how to navigate these tools.\nThese data science guides are designed to be used with Git because this is a very important skill for data science, and while it can be a bit challenging at first, it is incredibly valuable to learn, and everyone who wants to do some of the tasks in these guides should also be using Git to manage their work."
  }
]