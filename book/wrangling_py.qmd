# Wrangling Data {#sec-wrangling}

{{< include _links.qmd >}}

```{python}
#| label: setup
#| cache: false
#| output: false
#| code-fold: true
#| code-summary: 'Setup Code (Click to Expand)'

# packages needed to run the code in this section
# !pip install pyjanitor pandas polars fingertips_py

import re

import pandas as pd
import polars as pl

from janitor import clean_names

# import fingertips_py as ftp
```

The first step in any analysis is to find the right data for the problem at hand, and to transform that data so that it is structured appropriately for the method of analysis. Finding the right data can be challenging, sometimes requiring getting data from multiple sources, and even then, the data might be a mess, with missing values, duplicates, formatting that makes it difficult to work with in R or Python, and a myriad of other problems. The process of transforming data to make it suitable for analysis is often referred to as data wrangling, or data munging. The data wrangling process can take many forms, from simple data cleaning to more complex transformations like combining multiple data sets, or creating new variables to enrich the data.

## Python Libraries for Data Wrangling

The Python ecosystem for data wrangling is not quite as rich as in R, but there is one library that is extremely dominant (pandas) and one library that is seeking to disrupt it (polars).

## Fingertips Data

We will use Public Health England's [Fingertips API] (for which there are packages available in [R][fingertipsR] and [Python][fingertips_py]) as the data source for this guide, as it represents a good opportunity to give a brief overview of this data and how to use it. Fingertips is a repository of public health data on a wide range of topics. The data included in this tutorial relates to the wider determinants of health. 

First, we can look at what is available.

Unfortunately, the `fingertips_py` package does seem to be a little less mature than the R equivalent, and there are a lot of errors running the functions we would need to import the data. While this is the case, we'll take a little shortcut, importing the data from a CSV that I've stored in the guides GitHub repository, using the Fingertips R package.

```{python}
#| label: import-data

phof_raw = pd.read_csv("https://raw.githubusercontent.com/NHS-South-Central-and-West/data-science-guides/main/data/phof.csv", low_memory=False)
```


```{python}
#| label: data-info

phof_raw.info()
```

```{python}
#| label: describe-data
#| column: page

phof_raw.describe()
```

```{python}
#| label: glimpse-data
#| column: page

phof_raw.head()
```

With the raw data imported, we are now able to start the process of cleaning and wrangling the data into a format that is ready for analysis.

## Data Wrangling

First, we will clean the column names using the pyjanitor's `clean_names` function. This will convert all column names to lower case and remove any spaces and special characters. This will make it easier to work with the data in the future.

```{python}
#| label: clean-names

phof_raw = (
    phof_raw
    .pipe(clean_names)
)
```


We've got a total of 168 indicators, which means some of the indicators from the Public Health Outcomes Framework are not available for upper tier local authorities. We can filter the data to just the indicators we are interested in (as well as filtering any rows where the value is missing), using the panda's `loc` function. We can also use the `group_by` and `mean` functions to get the mean value of the indicators across all areas in the data. Below is an example:

```{python}
#| label: summarise-values

(
  phof_raw
  .loc[(phof_raw['indicatorid'].isin([90366, 20101, 11601]))]
  .dropna(subset='value')
  .groupby('indicatorname')['value']
  .mean()
)
```

The data pulled from the Fingertips API has a number of idiosyncracies that need addressing when wrangling the data. For example, the `timeperiodsortable` column refers to the year that the data relates to, and it contains four superfluous zeros at the end of each year (i.e, 20230000). We can use the pandas `assign` function to create a `year` column which strips out the zeros from the `timeperiodsortable` column by dividing it by 10000, and dropping the decimal place by converting year to integer (using the `astype(int)` function).

```{python}
#| label: format-years
#| column: page

# filter for the indicators of interest and wrangle data into tidy structure
(
  phof_raw
  .loc[(
    phof_raw['indicatorid'].isin([90366, 20101, 11601])) 
    & (phof_raw['areacode'] == 'E92000001' 
    )]
  .assign(year=lambda x: (x['timeperiodsortable'] / 10000).astype(int))
  .groupby(['indicatorname', 'areacode', 'year'], as_index=False)['value']
  .mean()
)
```

We can also remove the ID that is appended to the beginning of each indicator name. We can use `str.replace` to remove this part of the string from the indicator name, using a regular expression to remove everything up to and including the dash (and the space immediately after).

```{python}
#| label: remove-inds-id
#| column: page

(
  phof_raw
  .loc[(phof_raw['indicatorid'].isin([90366, 20101, 11601]))]
  .assign(
    indicatorname = lambda x: x['indicatorname']
                              .str.replace('^[^-]+- ', '', regex=True))
)
```

Finally, the data is structured in a wide format, with each indicator having its own column. This is not a tidy structure, as each column should represent a variable, and each row should represent an observation. We can use the `pivot_table` function to convert the data into a tidy structure, with each indicator having its own row. Pandas will turn `areacode` and `timeperiodsortable` into the index of the dataframe, and in order to convert these variables back to columns we use `reset_index` and `rename_axis(None, axis=1)`. We can also use the `rename` function to rename the columns to something more meaningful, and the `dropna` function to remove any rows where the value is missing.

```{python}
#| label: pivot-wider
#| column: page

(
  phof_raw
  .loc[(phof_raw['indicatorid'].isin([90366, 20101, 11601]))]
  .groupby(
    ['indicatorname', 'areacode', 'timeperiodsortable'], 
    as_index=False
    )['value']
  .mean()
  .pivot_table(
    index=['areacode', 'timeperiodsortable'],
    columns='indicatorname',
    values='value'
    )
  .rename(
    columns={
      'A01b - Life expectancy at birth': 'life_expectancy',
      'C04 - Low birth weight of term babies': 'low_birth_weight', 
      'B16 - Utilisation of outdoor space for exercise or health reasons': 'outdoor_space'
      })
  .reset_index()
  .rename_axis(None, axis=1)
  .dropna()
)
```

We can combine all of the above steps into a single function, which we can then apply to the raw data to get the data into a tidy structure.

```{python}
#| label: function-wrangle-data
#| column: page

# function to wrangle data into tidy structure
def wrangle_phof_data(data):
  """
  Returns a tidy, formatted DataFrame with Public Health England data
  """
  phof_data = (
    data
    .loc[(data['indicatorid'].isin([90366, 20101, 11601]))]
    .assign(year=lambda x: (x['timeperiodsortable'] / 10000).astype(int))
    .groupby(
      ['indicatorname', 'areacode', 'year'], 
      as_index=False
      )['value']
    .mean()
    .pivot_table(
      index=['areacode', 'year'],
      columns='indicatorname',
      values='value'
      )
    .rename(
      columns={
        'A01b - Life expectancy at birth': 'life_expectancy',
        'C04 - Low birth weight of term babies': 'low_birth_weight', 
        'B16 - Utilisation of outdoor space for exercise or health reasons': 'outdoor_space'
        })
    .reset_index()
    .rename_axis(None, axis=1)
    .dropna()
    )
  
  return phof_data

phof = wrangle_phof_data(phof_raw)
phof.head()
```


## Next Steps

We have successfully imported, cleaned, and wrangled Fingertips data into a tidy structure. We can now move on to the next step of the data science process, which is to explore the data and perform some analysis.


We could save the data to a CSV file at this point, using the pandas `to_csv` function (for example, `phof.to_csv('data/phof.csv', index=False)`), or we could perform the analysis on the dataframe object we have created.

This is just one example of the data wrangling process, but in reality there is a vast array of ways you might transform data in the process of preparing it for an analysis. There is no set recipe for this process, but the goal is to transform data from whatever raw form it arrives in to something structured (typically this means tidy data) that lends itself to analysis.

## Resources

- [Hadley Wickham - Tidy Data]
- [Modern Polars]
- [Pandas Anti Patterns]
- [Minimally Sufficient Pandas]
- [Pandas Data Structures]
- [Pandas Series Introduction]
