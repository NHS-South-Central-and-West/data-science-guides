[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Data Science Guides",
    "section": "",
    "text": "Welcome\nFor the  version of this book, click here.\nData Science Guides is a series of worked examples of common data science tasks in R & Python, to use as a space for learning how to implement these methods, and as a template for applying data science at NHS South, Central and West CSU (and beyond).\nThe intention of these guides is not to teach the reader to use R or Python, nor is the intention to teach the foundations of mathematics, statistics, and probability that underpin many of the methods used in data science. While these guides should be easy enough to follow for a relative beginner, teaching these fundamentals is beyond the scope of this book.\nInstead, the hope is that these guides serve as simple starting points for anyone looking to use a particular method, using R/Python, or for anyone looking to kick off a data science project with limited experience.\nThis book is freely available and licensed under CC BY-NC-SA 4.0."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Python Data Science Guides",
    "section": "Resources",
    "text": "Resources\nThere are lots of resources for learning some of the foundations and principles that are not available in these data science guides.\n\nLearning Statistics with R\nBeyond Multiple Linear Regression\nStatistical Rethinking\nTelling Stories with Data\nStatQuest"
  },
  {
    "objectID": "intro.html#choosing-between-r-python",
    "href": "intro.html#choosing-between-r-python",
    "title": "1  Introduction",
    "section": "\n1.1 Choosing Between R & Python",
    "text": "1.1 Choosing Between R & Python\nR and Python are two of the most popular programming languages in the world, and they are both used extensively in data science. However, there is a lot of debate about which language is better, and which language is more suitable for data science. A quick Google search for “R vs Python” will quickly point you in the direction of plenty of strongly-held opinions about why R or Python is better and why the other language is rubbish and a total waste of time, but I think these debates are a little worn out, and it doesn’t help newcomers to the field to see these dichotomous views of two of the most popular languages for data science out there.\nIn my view, as someone that started out with R before moving on to Python, and that is comfortable using either, I don’t think the most important decision is about which language you will learn. It’s the decision to learn any programming language at all. I’ve found that different people find different languages easier to get started with. Personally, I find that Python makes a little more sense to me than R. While I’ve been using R a lot longer, and am quite capable with it (perhaps more so than I am with Python), I tend to be able to learn methods/concepts easier with Python, because the logic that underpins Python methods just works for me. But that doesn’t mean that the logic underpinning Python is better. It just happens to be the case that it is well-aligned with the way that I think about programming. I think it is more important to find which language makes the most sense to you. Both are very strong on data science, as well as being strong in a number of other areas, and both are very popular. Playing around with them both and working out which one makes most sense to you, and which one gives you the best shot at maintaining your coding journey, is what matters.\nIt’s also worth noting that learning a second language, when you are confident with your first, is a lot easier. The hard part is learning the first language. If you crack that part, then you will be able to tackle the second, third, and fourth languages with a lot more ease and at a much faster pace. Learning R doesn’t mean you have to stick to R forever, and vice versa for Python. The important thing is that you’ve learned a coding language. Any coding language.\n\n1.1.1 Popularity\nBoth R & Python are widely used in data science, and both are very popular programming languages. Although there are a number of ways to measure the popularity of a programming language, every measure demonstrates that Python is one of, if not the most popular language in the world, and R is in and around the top ten. A common measure of the popularity of a programming language is the Popularity of Programming Language (PYPL) Index, which is a measure of popularity based on the number of times they are searched for on Google. The PYPL Index is updated monthly, and the data is available from the PYPL website.\nHere is how the top ten most popular programming languages looks in 2023 (based on an average PYPL Index in 2023):\n\n\n\n\nFigure 1.1: The global popularity of programming languages in 2023, according to the PYPL Index\n\n\n\nWhile Python is way out in front in first, R is also very high on the list in seventh place. This is particularly impressive given that every other language on the list is a general purpose programming language, while R is a statistical programming language. When we look at the PYPL Index in the UK, we see a similar picture, with Python even further out in front in first place, and R in in fifth place. This is a testament to the popularity of R within the niche that it occupies, of which data science is a part.\nWe can also look at how the popularity of R and Python has changed over time, in comparison with other languages that are tracked in the PYPL Index. Here is a plot of the PYPL Index over time, from 2005 to 2023:\n\n\n\n\nFigure 1.2: The global popularity of programming languages from 2005 to 2023, according to the PYPL Index\n\n\n\nAs you can see, Python has been the most popular programming language since 2018, and R has been in and around the top ten for about a decade. Both languages are extremely popular, and both are likely to remain popular for the foreseeable future.\nWhile the PYPL Index is a good measure of language popularity, there are other ways of measuring the most popular programming language, such as GitHub’s The State Of Open Source Software Report and Stack Overflow’s Developer Survey and Tag Trends.\n\n1.1.2 NHS R & Python Communities\nThere are already a sizeable number of people using both languages in the NHS, and there are communities that are already in place to support people using R or Python in the NHS.\nThe NHS-R community is a pretty active community of people that are using R in the NHS. You can find them on the NHS-R website and Slack. They also have a Github organisation and a YouTube channel which both contain tons of really useful resources.\nWith regards to Python, the equivalent community is the NHS Python Community for Healthcare (typically shortened to NHS-Pycom). The NHS Python Community also have a website and a Slack channel, and they have a Github organisation and a YouTube channel which are both worth checking out. The NHS Python Community is a little smaller than the NHS-R community, but it is still a very active community, and it is growing quickly, with both communities collaborating regularly to try and spread the growth of both languages in the NHS."
  },
  {
    "objectID": "good_science.html#a-scientific-approach-to-problem-solving",
    "href": "good_science.html#a-scientific-approach-to-problem-solving",
    "title": "2  Doing Good Science",
    "section": "2.1 A Scientific Approach to Problem Solving",
    "text": "2.1 A Scientific Approach to Problem Solving\nWhile it is important to learn the methods detailed in these guides (and more), the greatest emphasis should be placed on learning how to use the scientific method to approach problems. Approaching business problems scientifically, trying to build theories based on our observations about the subject being studied, is often overlooked, as this is in many ways the most complex part of the process. It is also less exciting than learning new ways to analyse data, so for someone that is new to data science, it is quite easy to de-emphasise this and focus on doing the fun parts instead.\nThese issues are exacerbated by the fact that any data science education, whether formal or self-taught, is often focused on the methods and tools required for data science. There is less explicit focus on the “process”, but this is because the process is taught implicitly through the methods. Teaching the various methods and tools that a data scientist needs to know is time-consuming and complex, and in the process of learning and applying the methods, the student will also learn the scientific approach to problem solving. That said, once someone has mastered the methods, they are (relatively) east to apply to real-world problems, while the process of doing science the right way is more difficult, and requires careful consideration for every practitioner, no matter how experienced they are.\n\n2.1.1 Framing the Problem\nAll of this ties into the same questions that should be asked when identifying data science opportunities. But having identified these opportunities, it is important not to skip to identifying a cool method for solving the problem. Instead, focus on the problem itself, and then identify the method(s) that will be most appropriate for answering it.\nA simple, concise approach to framing the problem and maximising the value of data science is to use Tom Carpenter’s suggestions below:\n\nIf you follow these five steps detailed by Tom, you stand a good chance of approaching the problem in the right way, generating real business value, and really getting to the heart of the problem you are trying to solve.\n\n\n2.1.2 A Simple Framework for Data Science\nThe guides here obviously focus on the methods, but before diving into the code, I think it is important to highlight the importance of learning to implement these methods appropriately, and approaching data science in the “correct” way. This means understanding the problem, generating theories about how to solve it, identifying the data that is needed to test these theories, and building a robust model that is informed by your theory and that is appropriate for the data you have. Before jumping into the code and the iterative process of model building, it is important to take the time to think carefully about your theory, the variables that are relevant to the phenomenon you are studying and the causal mechanism that guides it (or the data generating process), and your expectations from your model. Only once you’ve thought carefully about all of these things should you start to think about the method(s) you should use.\nThe order of these steps is important, but the right order is dependent on context. In an ideal situation…\nFor anyone that is analytically minded, it is very easy to fall into the trap of hyperfocusing on the cool methodological approaches you could take to solving a problem. This is something that I regularly have to check myself on, because I find it so easy to get excited about the fancy methods I could apply to a particular situation. However, despite the fact that the methods are often the most fun part of the job, the fact remains that a very fancy model won’t get you very far if you haven’t spent a lot of time thinking carefully about the problem that model is trying to solve, gathering the data that is needed to solve it, and preparing the data in a way that maximises the model’s performance.\nThe easiest way to avoid falling into the same trap that I regularly slip into is to treat the methods as a means to an end. The goal should always be to build a solution to a specific problem, with a clear understanding of what “success” looks like in that context. And if you also get to make something fancy in the process? Well isn’t that exciting!\nData plays an increasingly important role in business decision-making nowadays, due to the technological advances that have made it possible to collect and analyse large amounts of data. This is not just true of tech companies, but also of traditional businesses in sectors such as retail, manufacturing, and healthcare. The NHS is increasingly embracing the role that data plays in driving decision-making, improving clinical services, and, ultimately, improving patient outcomes.\nBut as more data becomes available, the challenge of making good use of it becomes even more difficult. The old methods are still useful, but they are no longer sufficient. In order to make the most of the data that is available, the NHS and SCW need to be able to identify when there are opportunities to use data science to improve the services that they provide. Recognising these opportunities can be challenging. It requires not only a good understanding of what is possible with data science, but also a proactive approach to identifying when it is possible to do more with data than is currently being done.\nWhile the other guides in this project will hopefully give a better understanding of what is possible with data science, this particular guide is intended to help anyone frame business problems, whether internal or external, in terms of data science, and to identify opportunities to develop more advanced solutions to those problems."
  },
  {
    "objectID": "good_science.html#identifying-data-science-opportunities",
    "href": "good_science.html#identifying-data-science-opportunities",
    "title": "2  Doing Good Science",
    "section": "2.2 Identifying Data Science Opportunities",
    "text": "2.2 Identifying Data Science Opportunities\nA lot of the analytics work that is done at SCW tends to be descriptive in nature, and takes the form of regular reports, dashboards, and Excel spreadsheets summarising data. These descriptive approaches are and always will be valuable, but there is also a limit to what descriptive analytics can tell us. Descriptive analytics can tell us what has happened, but it cannot tell us why, nor can it tell us what will happen in the future. When a piece of work is concerned with explaining past events, or predicting future events, there is an opportunity to use some of the data science techniques that are covered in these guides.\nIdentifying these opportunities is not always easy, however, because requests for work will often come in the form of requests for a certain type of solution to address a business problem, with expectations of what that solution should look like. Customers and colleagues will often have a clear idea of what they want, because that have seen or used similar solutions in the past, and this limits the scope for identifying data science opportunities. If we always provide the exact solution that is requested, without considering whether there is a better way to solve the problem, we will miss out on opportunities to extract more value from our data products.\nIt is important to understand the business problem and the solution that is being requested, but it is also important to identify why a certain solution is being requested, and how the customer or colleague plans to use it. For example, a request for a dashboard that shows utilisation of a particular service in the past twelve months is not necessarily a request for a data science solution. It is possible that the reason for wanting this dashboard is to help plan how to meet future demand for that service, but it is also possible that the reason for wanting this dashboard is simply to understand how the service is being used. While a dashboard will offer valuable context, it is possible that a machine learning model that predicts future demand might be more useful.\n\n2.2.1 Asking the Right Questions\nDesigning data science solutions to business problems is a process of discovery. The best way to identify potential data science work is to ask questions.\nThere is no foolproof set of questions you can ask, and no perfect framework for designing data science solutions, but there are some questions that are more likely to lead to useful insights than others:\n\n2.2.1.1 Identifying the Problem\n\nWhat is the problem you are trying to solve?\nWhat is the business impact of solving this problem?\nWhat is the current solution to this problem?\n\n\n\n2.2.1.2 Considering Solutions\n\nWhat do you think the solution to this problem should look like?\nWhy do you think this approach is the best way to solve this problem?\nHow will you use the solution to this problem?\n\n\n\n2.2.1.3 Measuring Success\n\nHow will you know if the solution to this problem is successful?\nHow is success defined and measured for this problem?\n\n\n\n2.2.1.4 Identifying Feasible Approaches\n\nWhat data do you need to solve this problem?\nWhat data do you have that might be useful in solving this problem?\nWhen does this piece of work need to be completed by?\nHow often will this piece of work need to be repeated/updated?"
  },
  {
    "objectID": "good_science.html#next-steps",
    "href": "good_science.html#next-steps",
    "title": "2  Doing Good Science",
    "section": "2.3 Next Steps",
    "text": "2.3 Next Steps\nAlthough the data science guides focus, for the most part, on the implementation of various data science techniques, developing SCW’s data science capabilities is more than just upskilling analysts and equipping individuals with the tools to do this work. Perhaps a more important part of this process is helping a much wider group of SCW colleagues to think about data science, and to recognise opportunities to use data science to solve business problems. Hopefully the guides will help SCW colleagues to understand what is possible, and this guide will help them to identify opportunities to do more with data using these techniques."
  },
  {
    "objectID": "good_science.html#resources",
    "href": "good_science.html#resources",
    "title": "2  Doing Good Science",
    "section": "2.4 Resources",
    "text": "2.4 Resources\n\nTechniques for Problem Solving\nProblem Solving and the Scientific Method\nRichard Feynman on the Scientific Method"
  },
  {
    "objectID": "sql.html#packages",
    "href": "sql.html#packages",
    "title": "3  Importing Data from SQL",
    "section": "\n3.1 Packages",
    "text": "3.1 Packages\nThe Python packages you need to interact with SQL are:\n\npyodbc - accessing ODBC databases\nsqlalchemy - establishing a connection and interacts with pandas\npandas - storing, manipulating, and exporting dataframes\n\nI won’t import either package here because they are only needed for a handful of functions."
  },
  {
    "objectID": "sql.html#establish-sql-connection",
    "href": "sql.html#establish-sql-connection",
    "title": "3  Importing Data from SQL",
    "section": "\n3.2 Establish SQL Connection",
    "text": "3.2 Establish SQL Connection\nIn order to run a SQL query in Python, you need to set Python up so that it can interpret the specific SQL dialect and find the database that it is running the query on. The SQLAlchemy function create_engine() will give Python everything it needs to do this, but you also have to feed in the following parameters as a string (example given in code):\n\nSQL dialect (‘mssql’)\nPython library for interacting with the database (‘pyodbc’)\nDatabase location\nSQL driver (‘?driver=SQL+Server’)\n\n\nengine = sa.create_engine('mssql+pyodbc://{server-and-db-address}?driver=SQL+Server',echo = True)\n\nHaving specified a SQL engine, you can establish a connection.\n\nconn = engine.connect()"
  },
  {
    "objectID": "sql.html#running-sql-query",
    "href": "sql.html#running-sql-query",
    "title": "3  Importing Data from SQL",
    "section": "\n3.3 Running SQL Query",
    "text": "3.3 Running SQL Query\nYou have two ways of going about running a SQL query in a Python script. You can either write your query out explicitly in your Python script, or you can read in an external SQL query. If the query is particularly lengthy, it is better to store is as a .sql file and call it from Python, to make it easier to read your code, and to maintain both components.\n\n# open and read sql query\nquery = open('path-to-query\\query.sql', 'r')\n\n# read query in to pandas dataframe\ndf = pd.read_sql_query(query.read(),conn)\n\n# close sql query\nquery.close()\n\nYou can check that this process has worked as expected by inspecting your pandas dataframe.\n\ndf.head()"
  },
  {
    "objectID": "sql.html#export-to-csv",
    "href": "sql.html#export-to-csv",
    "title": "3  Importing Data from SQL",
    "section": "\n3.4 Export to CSV",
    "text": "3.4 Export to CSV\nFinally, you can export your dataframe using the pandas function to_csv(). If you want to retain the index column that pandas adds to the dataframe, simply change index=False to True.\n\ndf.to_csv('path-to-save-df\\df.csv', index=False)"
  },
  {
    "objectID": "eda_py.html#inspecting-the-data",
    "href": "eda_py.html#inspecting-the-data",
    "title": "4  Exploratory Data Analysis",
    "section": "4.1 Inspecting the Data",
    "text": "4.1 Inspecting the Data\nThe first step when doing EDA is to inspect the data itself and get an idea of the structure of the dataset, the variable types, and the typical values of each variable. This gives a better understanding of exactly what data is being used and informs decisions both about the next steps in the exploratory process and any modelling choices.\nWe can use the head() and info() functions to get a sense of the structure of the data. The head() function returns the first five rows of the data, and the info() method returns a summary of the data, including the number of rows, the number of columns, the column names, and the data type of each column. The info() method is particularly useful for identifying missing values, as it returns the number of non-null values in each column. If the number of non-null values is less than the number of rows, then there are missing values in the column.\nIn addition to these two methods, we can also use the nunique() method to count the number of unique values in each column, which helps identify categorical variables, and we can use the unique() method to get a list of the unique values in a column.\n\ndf.head()\n\n\n\n\n\n\n\n\nage\nsex\nresting_bp\ncholesterol\nfasting_bs\nresting_ecg\nmax_hr\nangina\nheart_peak_reading\nheart_disease\n\n\n\n\n0\n40\nM\n140\n289\n0\nNormal\n172\nN\n0.0\n0\n\n\n1\n49\nF\n160\n180\n0\nNormal\n156\nN\n1.0\n1\n\n\n2\n37\nM\n130\n283\n0\nST\n98\nN\n0.0\n0\n\n\n3\n48\nF\n138\n214\n0\nNormal\n108\nY\n1.5\n1\n\n\n4\n54\nM\n150\n195\n0\nNormal\n122\nN\n0.0\n0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 918 entries, 0 to 917\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   age                 918 non-null    int64   \n 1   sex                 918 non-null    category\n 2   resting_bp          918 non-null    int64   \n 3   cholesterol         918 non-null    int64   \n 4   fasting_bs          918 non-null    category\n 5   resting_ecg         918 non-null    category\n 6   max_hr              918 non-null    int64   \n 7   angina              918 non-null    category\n 8   heart_peak_reading  918 non-null    float64 \n 9   heart_disease       918 non-null    category\ndtypes: category(5), float64(1), int64(4)\nmemory usage: 41.1 KB\n\n\n\n# count unique values in each column\ndf.nunique()\n\nage                    50\nsex                     2\nresting_bp             67\ncholesterol           222\nfasting_bs              2\nresting_ecg             3\nmax_hr                119\nangina                  2\nheart_peak_reading     53\nheart_disease           2\ndtype: int64\n\n\n\n# unique values of the outcome variable\ndf.heart_disease.unique()\n\n[0, 1]\nCategories (2, int64): [0, 1]\n\n\n\n# unique values of a continuous explanatory variable\ndf.cholesterol.unique()\n\narray([289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 211, 164, 204,\n       234, 273, 196, 201, 248, 267, 223, 184, 288, 215, 209, 260, 468,\n       188, 518, 167, 224, 172, 186, 254, 306, 250, 177, 227, 230, 294,\n       264, 259, 175, 318, 216, 340, 233, 205, 245, 194, 270, 213, 365,\n       342, 253, 277, 202, 297, 225, 246, 412, 265, 182, 218, 268, 163,\n       529, 100, 206, 238, 139, 263, 291, 229, 307, 210, 329, 147,  85,\n       269, 275, 179, 392, 466, 129, 241, 255, 276, 282, 338, 160, 156,\n       272, 240, 393, 161, 228, 292, 388, 166, 247, 331, 341, 243, 279,\n       198, 249, 168, 603, 159, 190, 185, 290, 212, 231, 222, 235, 320,\n       187, 266, 287, 404, 312, 251, 328, 285, 280, 192, 193, 308, 219,\n       257, 132, 226, 217, 303, 298, 256, 117, 295, 173, 315, 281, 309,\n       200, 336, 355, 326, 171, 491, 271, 274, 394, 221, 126, 305, 220,\n       242, 347, 344, 358, 169, 181,   0, 236, 203, 153, 316, 311, 252,\n       458, 384, 258, 349, 142, 197, 113, 261, 310, 232, 110, 123, 170,\n       369, 152, 244, 165, 337, 300, 333, 385, 322, 564, 239, 293, 407,\n       149, 199, 417, 178, 319, 354, 330, 302, 313, 141, 327, 304, 286,\n       360, 262, 325, 299, 409, 174, 183, 321, 353, 335, 278, 157, 176,\n       131], dtype=int64)"
  },
  {
    "objectID": "eda_py.html#summary-statistics",
    "href": "eda_py.html#summary-statistics",
    "title": "4  Exploratory Data Analysis",
    "section": "4.2 Summary Statistics",
    "text": "4.2 Summary Statistics\nSummary statistics are a quick and easy way to get a sense of the distribution and central tendency of the variables in the dataset. We can use the describe() method to get a quick overview of every column in the dataset, including the row count, the mean and standard deviation, the minimum and maximum value, and quartiles of each variable.\n\n# summary of the data\ndf.describe()\n\n\n\n\n\n\n\n\nage\nresting_bp\ncholesterol\nmax_hr\nheart_peak_reading\n\n\n\n\ncount\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n\n\nmean\n53.510893\n132.396514\n198.799564\n136.809368\n0.887364\n\n\nstd\n9.432617\n18.514154\n109.384145\n25.460334\n1.066570\n\n\nmin\n28.000000\n0.000000\n0.000000\n60.000000\n-2.600000\n\n\n25%\n47.000000\n120.000000\n173.250000\n120.000000\n0.000000\n\n\n50%\n54.000000\n130.000000\n223.000000\n138.000000\n0.600000\n\n\n75%\n60.000000\n140.000000\n267.000000\n156.000000\n1.500000\n\n\nmax\n77.000000\n200.000000\n603.000000\n202.000000\n6.200000\n\n\n\n\n\n\n\nWhile the describe() function is pretty effective, the skimpy package can provide a more detailed summary of the data, using the skim() function. If you are looking for a single function to capture the entire process of inspecting the data and computing summary statistics, skim() is the function for the job, giving you a wealth of information about the dataset as a whole and each variable in the data.\nAnother package that provides a similar function is the [profiling][ydata-profiling] package, which can be used to generate a report containing a summary of the data, including the data types, missing values, and summary statistics. The ydata-profiling package is particularly useful for generating a report that can be shared with others, as it can be exported as an HTML file, however it’s a bit more resource-intensive than skimpy, so we will stick with skimpy for this tutorial.\n\n# more detailed summary of the data\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types               Categories                                        │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓ ┏━━━━━━━━━━━━━━━━━━━━━━━┓                                │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃ ┃ Categorical Variables ┃                                │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩ ┡━━━━━━━━━━━━━━━━━━━━━━━┩                                │\n│ │ Number of rows    │ 918    │ │ category    │ 5     │ │ sex                   │                                │\n│ │ Number of columns │ 10     │ │ int32       │ 4     │ │ fasting_bs            │                                │\n│ └───────────────────┴────────┘ │ float64     │ 1     │ │ resting_ecg           │                                │\n│                                └─────────────┴───────┘ │ angina                │                                │\n│                                                        │ heart_disease         │                                │\n│                                                        └───────────────────────┘                                │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┓  │\n│ ┃ column_name                ┃ NA   ┃ NA %    ┃ mean    ┃ sd    ┃ p0     ┃ p25   ┃ p75   ┃ p100   ┃ hist     ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━━━┩  │\n│ │ age                        │    0 │       0 │      54 │   9.4 │     28 │    47 │    60 │     77 │  ▁▄▅█▅▁  │  │\n│ │ resting_bp                 │    0 │       0 │     130 │    19 │      0 │   120 │   140 │    200 │     █▆▁  │  │\n│ │ cholesterol                │    0 │       0 │     200 │   110 │      0 │   170 │   270 │    600 │   ▃▂█▁   │  │\n│ │ max_hr                     │    0 │       0 │     140 │    25 │     60 │   120 │   160 │    200 │   ▃██▆▁  │  │\n│ │ heart_peak_reading         │    0 │       0 │    0.89 │   1.1 │   -2.6 │     0 │   1.5 │    6.2 │    █▆▃   │  │\n│ └────────────────────────────┴──────┴─────────┴─────────┴───────┴────────┴───────┴───────┴────────┴──────────┘  │\n│                                                    category                                                     │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                          ┃ NA       ┃ NA %          ┃ ordered              ┃ unique            ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩  │\n│ │ sex                                  │        0 │             0 │ False                │                 2 │  │\n│ │ fasting_bs                           │        0 │             0 │ False                │                 2 │  │\n│ │ resting_ecg                          │        0 │             0 │ False                │                 3 │  │\n│ │ angina                               │        0 │             0 │ False                │                 2 │  │\n│ │ heart_disease                        │        0 │             0 │ False                │                 2 │  │\n│ └──────────────────────────────────────┴──────────┴───────────────┴──────────────────────┴───────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\nIf we want to examine a particular variable, the functions mean(), median(), quantile(), min(), and max() will return the same information as the describe() function. We can also get a sense of dispersion by computing the standard deviation or variance of a variable. The std() function returns the standard deviation of a variable, and the var() function returns the variance.\n\n# mean & median age\ndf.age.mean(), df.age.median()\n\n(53.510893246187365, 54.0)\n\n\n\n# min and max age\ndf.age.min(), df.age.max()\n\n(28, 77)\n\n\n\n# dispersion of age\ndf.age.std(), df.age.var()\n\n(9.43261650673201, 88.9742541630732)\n\n\nFinally, we can use the value_counts() function to get a count of the number of observations in each category of a discrete variable.\n\n# heart disease count\ndf['heart_disease'].value_counts()\n\nheart_disease\n1    508\n0    410\nName: count, dtype: int64\n\n\n\n# resting ecg count\ndf['resting_ecg'].value_counts()\n\nresting_ecg\nNormal    552\nLVH       188\nST        178\nName: count, dtype: int64\n\n\n\n# angina\ndf['angina'].value_counts()\n\nangina\nN    547\nY    371\nName: count, dtype: int64\n\n\n\n# cholesterol\ndf['cholesterol'].value_counts()\n\ncholesterol\n0      172\n254     11\n223     10\n220     10\n230      9\n      ... \n392      1\n316      1\n153      1\n466      1\n131      1\nName: count, Length: 222, dtype: int64\n\n\nWe can also use the groupby() method to get the counts of each category in a categorical variable, grouped by another categorical variable.\n\ndf.groupby(['resting_ecg'])['heart_disease'].value_counts()\n\nresting_ecg  heart_disease\nLVH          1                106\n             0                 82\nNormal       1                285\n             0                267\nST           1                117\n             0                 61\nName: count, dtype: int64\n\n\nIn addition to the counts, we can also get the proportions of each category using the normalize=True argument.\n\ndf.groupby(['resting_ecg'])['heart_disease'].value_counts(normalize=True).round(3)\n\nresting_ecg  heart_disease\nLVH          1                0.564\n             0                0.436\nNormal       1                0.516\n             0                0.484\nST           1                0.657\n             0                0.343\nName: proportion, dtype: float64"
  },
  {
    "objectID": "eda_py.html#data-visualisation",
    "href": "eda_py.html#data-visualisation",
    "title": "4  Exploratory Data Analysis",
    "section": "4.3 Data Visualisation",
    "text": "4.3 Data Visualisation\nWhile inspecting the data directly and using summary statistics to describe it is a good first step, data visualisation is a more effective way to explore the data. It allows us to quickly identify patterns and relationships in the data, and to identify any data quality issues that might not be immediately obvious without a visual representation of the data.\nWhen using data visualisation for exploratory purposes, the intent is generally to visualise the way data is distributed, both within and between variables. This can be done using a variety of different types of plots, including histograms, bar charts, box plots, scatter plots, and line plots. How variables are distributed can tell us a lot about the variable itself, and how variables are distributed relative to each other can tell us a lot about the potential relationship between the variables.\nIn this tutorial, we will use the matplotlib and seaborn packages to create a series of data visualisations to explore the data in more detail. The seaborn package is a high-level data visualisation library that is built on top of matplotlib. Although data visualisation in Python is not as straightforward as it is in R, seaborn makes it much easier to create good quality and informative plots.\n\n4.3.1 Visualising Data Distributions\nThe first step in the exploratory process is to visualise the data distributions of key variables in the dataset. This allows us to get a sense of the typical values and central tendency of the variable, as well as identifying any outliers or other data quality issues.\n\n4.3.1.1 Continuous Distributions\nFor continuous variables, we can use histograms to visualise the distribution of the data. We can use the histplot() function to create a histogram of a continuous variable. The binwidth argument allows us to specify the width of the bins in the histogram.\n\n# age distribution\nsns.histplot(data=df, x='age', binwidth=5)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# max hr distribution\nsns.histplot(data=df, x='max_hr', binwidth=10)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# cholesterol distribution\nsns.histplot(data=df, x='cholesterol', binwidth=25)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# cholesterol distribution\nsns.histplot(data=df.loc[df.cholesterol!=0], x='cholesterol', binwidth=25)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nThe inflated zero values in the cholesterol distribution suggests that there may be an issue with data quality that needs addressing.\n\n\n4.3.1.2 Discrete Distributions\nWe can use bar plots to visualise the distribution of discrete variables. We can use the countplot() function to create a bar plot of a discrete variable.\n\n# heart disease distribution\nsns.countplot(data=df, x='heart_disease')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# sex distribution\nsns.countplot(data=df, x='sex')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# angina distribution\nsns.countplot(data=df, x='angina')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Comparing Distributions\nThere are a number of ways to compare the distributions of multiple variables. Bar plots can be used to visualise two discrete variables, while histograms and box plots are useful for comparing the distribution of a continuous variable across the groups of a discrete variable, and scatter plots are particularly useful for comparing the distribution of two continuous variables.\n\n4.3.2.1 Visualising Multiple Discrete Variables\nBar plots are an effective way to visualize the observed relationship (or association, at least) between a discrete explanatory variable and a discrete outcome (whether binary, ordinal, or categorical). We can use the countplot() function to create bar plots, and the hue argument to split the bars by a particular variable and display them in different colours.\n\n# heart disease by sex\nsns.countplot(data=df, x='heart_disease', hue='sex')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# heart disease by resting ecg\nsns.countplot(data=df, x='heart_disease', hue='resting_ecg')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# angina\nsns.countplot(data=df, x='heart_disease', hue='angina')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# fasting bs\nsns.countplot(data=df, x='heart_disease', hue='fasting_bs')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.3.2.2 Visualising A Continuous Variable Across Discrete Groups\nHistograms and box plots are useful for comparing the distribution of a continuous variable across the groups of a discrete variable.\n\n4.3.2.2.1 Histogram Plots\nWe can use the histplot() function to create a histogram of a continuous variable. The hue argument allows us to split the histogram by a particular variable and display them in different colours, while the multiple argument allows us to specify how the histograms should be displayed. The multiple argument can be set to stack to stack the histograms on top of each other, or dodge to display the histograms side-by-side.\n\n# age distribution by heart disease\nsns.histplot(data=df, x='age', hue='heart_disease', binwidth=5, multiple='dodge')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# cholesterol\nsns.histplot(data=df, x='cholesterol', hue='heart_disease', binwidth=25, multiple='dodge')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# filter zero values\nsns.histplot(\n    data=df.loc[df.cholesterol!=0],\n    x='cholesterol',\n    hue='heart_disease',\n    binwidth=25,\n    multiple='dodge')\n\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nThe fact that there is a significantly larger proportion of positive heart disease cases in the zero cholesterol values further demonstrates the need to address this data quality issue.\n\n\n4.3.2.2.2 Box Plots\nBox plots visualize the characteristics of a continuous distribution over discrete groups. We can use the boxplot() function to create box plots, and the hue argument to split the box plots by a particular variable and display them in different colours.\nHowever, while box plots can be very useful, they are not always the most effective way of visualising this information, as explained boxplots by Cedric Scherer. This guide uses box plots for the sake of simplicity, but it is worth considering other options when visualising distributions.\n\n# age & heart disease\nsns.boxplot(data=df, x='heart_disease', y='age')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# age & heart disease, split by sex\n# fig, ax = plt.subplots(figsize=(10,6))\nsns.boxplot(data=df, x='heart_disease', y='age', hue='sex')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# max hr & heart disease\nsns.boxplot(data=df, x='heart_disease', y='max_hr')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# max hr & heart disease, split by sex\n# fig, ax = plt.subplots(figsize=(10,6))\nsns.boxplot(data=df, x='heart_disease', y='max_hr', hue='sex')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4.3.2.3 Visualising Multiple Discrete Variables\nScatter plots are an effective way to visualize how two continuous variables vary together. We can use the scatterplot() function to create scatter plots, and the hue argument to split the scatter plots by a particular variable and display them in different colours.\n\n# age & resting bp\nsns.scatterplot(data=df, x='age', y='resting_bp')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# age & resting bp\nsns.scatterplot(data=df.loc[df.resting_bp!=0], x='age', y='resting_bp')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=df.loc[df.cholesterol!=0], x='age', y='cholesterol')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=df, x='age', y='max_hr')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nThe scatter plot visualising age and resting blood pressure highlights another observation that needs to be removed due to data quality issues.\nIf there appears to be an association between the two continuous variables that you have plotted, as is the case with age and maximum heart rate in the above plot, you can also add a regression line to visualize the strength of that association. The regplot() function can be used to add a regression line to a scatter plot. The ci argument specifies whether or not to display the confidence interval of the regression line.\n\n# age & max hr\nsns.regplot(data=df, x='age', y='max_hr', ci=None)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nYou can also include discrete variables by assigning the discrete groups different colours in the scatter plot, and if you add regression lines to these plots, separate regression lines will be fit to the discrete groups. This can be useful for visualising how the association between the two continuous variables varies across the discrete groups.\nThe lmplot() function can be used to create scatter plots with regression lines, and the hue argument can be used to split the scatter plots by a particular variable and display them in different colours.\n\n# age & resting bp, split by heart disease\nsns.scatterplot(data=df.loc[df.resting_bp!=0], x='age', y='resting_bp', hue='heart_disease')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# age & cholesterol, split by heart disease (with regression line)\nsns.lmplot(\n    data=df.loc[df.cholesterol!=0],\n    x='age', y='cholesterol',\n    hue='heart_disease',\n    ci=None,\n    height = 7,\n    aspect=1.3)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# age & max hr, split by heart disease (with regression line)\nsns.lmplot(\n    data=df,\n    x='age', y='max_hr',\n    hue='heart_disease',\n    ci=None,\n    height = 7,\n    aspect=1.3)\n\nplt.show()"
  },
  {
    "objectID": "eda_py.html#next-steps",
    "href": "eda_py.html#next-steps",
    "title": "4  Exploratory Data Analysis",
    "section": "4.4 Next Steps",
    "text": "4.4 Next Steps\nThere are many more visualisation techniques that you can use to explore your data. You can find plenty of inspiration for different approaches to visualising data in the seaborn and matplotlib documentation. There are also a number of other Python libraries that can be used to create visualisations, including plotly, bokeh, and altair.\nThe next step in the data science process is to build a model to either explain or predict the outcome variable, heart disease. The exploratory work done here can help inform decisions about the choice of the model, and the choice of the variables that will be used to build the model. It will also help clean up the data, particularly the zero values in the cholesterol and resting blood pressure variables, to ensure that the model is built on the best possible data."
  },
  {
    "objectID": "eda_py.html#resources",
    "href": "eda_py.html#resources",
    "title": "4  Exploratory Data Analysis",
    "section": "4.5 Resources",
    "text": "4.5 Resources\nThere are a wealth of resources available to help you learn more about data visualisation, and while the resources for producing visualisations in R are more extensive, there are still a number of good resources for producing visualisations in Python.\n\nScientific Visualization: Python + Matplotlib [PDF][GitHub]\nSeaborn Tutorials\nMatplotlib Cheatsheets\n\nWhile the following resources are R-based, they are still useful for learning about data visualisation principles:\n\nData Visualization: A Practical Introduction\nFundamentals of Data Visualization"
  },
  {
    "objectID": "linear_regression_py.html#exploratory-data-analysis",
    "href": "linear_regression_py.html#exploratory-data-analysis",
    "title": "5  Linear Regression",
    "section": "5.1 Exploratory Data Analysis",
    "text": "5.1 Exploratory Data Analysis\n\n# data summary\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 583 entries, 0 to 582\nData columns (total 7 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   area_code            583 non-null    object \n 1   year                 583 non-null    int64  \n 2   life_expectancy      583 non-null    float64\n 3   imd_score            583 non-null    float64\n 4   imd_decile           583 non-null    int64  \n 5   physiological_score  583 non-null    float64\n 6   behavioural_score    583 non-null    float64\ndtypes: float64(4), int64(2), object(1)\nmemory usage: 32.0+ KB\n\n\n\n# view first five rows\ndf.head()\n\n\n\n\n\n\n\n\narea_code\nyear\nlife_expectancy\nimd_score\nimd_decile\nphysiological_score\nbehavioural_score\n\n\n\n\n0\nE06000001\n2015\n78.738395\n35.037\n1\n89.9\n84.9\n\n\n1\nE06000001\n2016\n79.073210\n35.037\n1\n88.8\n84.4\n\n\n2\nE06000001\n2017\n79.080000\n35.037\n1\n84.8\n82.7\n\n\n3\nE06000001\n2018\n78.810000\n35.037\n1\n84.6\n81.0\n\n\n4\nE06000002\n2015\n77.787260\n40.460\n1\n96.4\n80.3\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nyear\nlife_expectancy\nimd_score\nimd_decile\nphysiological_score\nbehavioural_score\n\n\n\n\ncount\n583.000000\n583.000000\n583.000000\n583.000000\n583.000000\n583.000000\n\n\nmean\n2016.497427\n81.167021\n23.182043\n5.442539\n99.015952\n99.395026\n\n\nstd\n1.118223\n1.612812\n8.043172\n2.827766\n10.012470\n8.698003\n\n\nmin\n2015.000000\n76.535000\n5.846000\n1.000000\n78.200000\n72.400000\n\n\n25%\n2015.500000\n80.011722\n17.016000\n3.000000\n92.100000\n93.250000\n\n\n50%\n2016.000000\n81.210000\n22.965000\n5.000000\n97.900000\n100.600000\n\n\n75%\n2017.000000\n82.229125\n28.104000\n8.000000\n104.500000\n105.900000\n\n\nmax\n2018.000000\n86.050000\n45.039000\n10.000000\n125.900000\n120.000000\n\n\n\n\n\n\n\n\n# imd\nsns.scatterplot(x='life_expectancy', y='imd_score', data=df)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# physiological risk factors\nsns.scatterplot(x='life_expectancy', y='physiological_score', data=df)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# behavioural risk factors\nsns.scatterplot(x='life_expectancy', y='behavioural_score', data=df)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# correlations with life expectancy\ndf.corr(numeric_only=True).loc[:,'life_expectancy']\n\nyear                  -0.005660\nlife_expectancy        1.000000\nimd_score             -0.837260\nimd_decile             0.813546\nphysiological_score    0.581565\nbehavioural_score      0.887986\nName: life_expectancy, dtype: float64\n\n\n\n# correlation matrix\ncorr = df[[\n    'life_expectancy', 'imd_score',\n    'physiological_score', 'behavioural_score']].corr()\ncorr\n\n\n\n\n\n\n\n\nlife_expectancy\nimd_score\nphysiological_score\nbehavioural_score\n\n\n\n\nlife_expectancy\n1.000000\n-0.837260\n0.581565\n0.887986\n\n\nimd_score\n-0.837260\n1.000000\n-0.378271\n-0.807654\n\n\nphysiological_score\n0.581565\n-0.378271\n1.000000\n0.587748\n\n\nbehavioural_score\n0.887986\n-0.807654\n0.587748\n1.000000\n\n\n\n\n\n\n\n\n# plot correlation matrix\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    square=True, annot=True)\nplt.show()\n\n\n\n\n\n\n\n\nThere are strong correlations between life expectancy and the three independent variables, however, the correlation between IMD score and the two risk factor variables is also relatively strong, and in the case of behavioural risk factors, it is very strong. This could be an issue."
  },
  {
    "objectID": "linear_regression_py.html#linear-regression",
    "href": "linear_regression_py.html#linear-regression",
    "title": "5  Linear Regression",
    "section": "5.2 Linear Regression",
    "text": "5.2 Linear Regression\n\n# create model\nmodel = sm.OLS.from_formula(\n    'life_expectancy ~ imd_score + physiological_score + behavioural_score',\n    data=df)\n\n# fit model\nresults = model.fit()\n\n# print summary\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:        life_expectancy   R-squared:                       0.844\nModel:                            OLS   Adj. R-squared:                  0.843\nMethod:                 Least Squares   F-statistic:                     1042.\nDate:                Fri, 27 Oct 2023   Prob (F-statistic):          7.23e-233\nTime:                        17:48:11   Log-Likelihood:                -564.29\nNo. Observations:                 583   AIC:                             1137.\nDf Residuals:                     579   BIC:                             1154.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=======================================================================================\n                          coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept              71.5915      0.623    114.851      0.000      70.367      72.816\nimd_score              -0.0775      0.006    -13.583      0.000      -0.089      -0.066\nphysiological_score     0.0238      0.003      7.141      0.000       0.017       0.030\nbehavioural_score       0.0907      0.006     15.020      0.000       0.079       0.103\n==============================================================================\nOmnibus:                      171.707   Durbin-Watson:                   0.765\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              611.061\nSkew:                           1.343   Prob(JB):                    2.04e-133\nKurtosis:                       7.236   Cond. No.                     3.36e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.36e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "linear_regression_py.html#evaluating-and-interpreting-regression-models",
    "href": "linear_regression_py.html#evaluating-and-interpreting-regression-models",
    "title": "5  Linear Regression",
    "section": "5.3 Evaluating and Interpreting Regression Models",
    "text": "5.3 Evaluating and Interpreting Regression Models\nWhen we fit a regression model, we have to consider both how well the model fits the data (evaluation) and the influence that each explanatory variable is having on the outcome (interpretation). The evaluation process is making sure that the model is doing a good enough job that we can reasonably draw some conclusions from it, while the interpretation process is understanding the conclusions we can draw. It’s important to understand the purpose that both of these processes serve, and to understand that the evaluation process is just kicking the tires, while what we are ultimately concerned with is the interpretation.\nThere are several elements to consider when interpreting the results of a linear regression model. Broadly speaking the focus should be on the magnitude and the precision of the model coeffficients. The magnitude of the coefficients indicates the strength of the association between the explanatory variables and the outcome variable, while the precision of the coefficients indicates the uncertainty in the estimated coefficients. We use the regression model coefficients to understand the magnitude of the effect and standard errors (and confidence intervals) to understand the precision of the effect.\n\n5.3.1 Model Evaluation\nThere are a number of ways we might evaluate model performance, but the starting point should be the model’s \\(R^2\\) and the p-values of the model coefficients. These can help us consider whether the model fits the data and whether we have enough data to trust the conclusions we can draw from each explanatory variable.\n\n5.3.1.1 \\(R^2\\)\nThe \\(R^2\\) value is the proportion of the variance in the outcome that is explained by the model. It is often used as a measure of the goodness of fit of the model, and by extension the extent to which the specified model explains the outcome, but caution should be applied when using \\(R^2\\) this way, because \\(R^2\\) can be misleading and doesn’t always tell the whole story. One of the main problems with \\(R^2\\) when used in an inferential context is that adding more variables to the model will always increase the \\(R^2\\) value, even if the new variables are not actually related to the outcome. Adjusted \\(R^2\\) values are used to account for this problem, but they don’t resolve all issues with \\(R^2\\), so it is important to consider \\(R^2\\) as just one of a number of indicators of the model’s fit. If the \\(R^2\\) is high, this doesn’t necessarily mean that the model is a good fit for the data, but if the \\(R^2\\) is particularly low, this might be a good indication that the model is not a good fit for the data, and at the very least it is good reason to be cautious about the results of the analysis, and to take a closer look at the model and the data to understand why the \\(R^2\\) is so low.\nDefining high/low \\(R^2\\) values is dependent on the context of the analysis. The theory behind the model should inform our expectations about the \\(R^2\\) value, and we should also consider the number of explanatory variables in the model (adding variables to a regression will increase the \\(R^2\\)). If the theory suggests that the model should explain a small amount of the variance in the outcome, and the model has only a few explanatory variables, then a low \\(R^2\\) value might not be a cause for concern.\nThe Adjusted \\(R^2\\) value for the above regression model is 0.843, which means that the model explains 84.3% of the observed variance in life expectancy in our dataset. This is a relatively high \\(R^2\\) value.\n\n\n5.3.1.2 P-Values\nP-values are the probability of observing a coefficient as large as the estimated coefficient for a particular explanatory variable, given that the null hypothesis is true (i.e. that the coefficient is equal to zero), indicating that no effect is present. This can be a little tricky to understand at first, but a slightly crude way of thinking about this is that p-values are an estimate of the probability that the model coefficient you observe is actually distinguishable from zero.\nP-values are used to test the statistical significance of the coefficients. If the p-value is less than the significance level then the coefficient is statistically significant, and we can reject the null hypothesis that the coefficient is equal to zero. If the p-value is greater than the significance level, then the coefficient is not statistically significant, and we cannot reject the null hypothesis that the coefficient is equal to zero. The significance level is often set at 0.05, which means that we are willing to accept a 5% chance of incorrectly rejecting the null hypothesis. This means that if the p-value is less than 0.05, then there is a greater than 95% probability that the coefficient is not equal to zero. However, the significance level is a subjective decision, and can be set at any value. For example, if the significance level is set at 0.01, then we are willing to accept a 1% chance of incorrectly rejecting the null hypothesis.\nIn recent years p-values (and the wider concept of statistical significance) have been the subject of a great deal of discussion in the scientific community, because there is a belief among many practitioners that a lot of scientific research places too much weight in the p-value, treating it as the most important factor when interpreting a regression model. I think this perspective is correct, but I think dismissing p-values entirely is a mistake too. The real problem is not with p-values themselves, but with the goal of an analysis being the identification of a statistically significant effect.\nI think the best way to think about p-values is the position prescribed by Gelman, Hill, and Vehtari (2020). Asking whether our coefficient is distinguishable from zero is the wrong question. As analysts, we should typically know enough about the context we are studying to build regression models with explanatory variables that will some effect. The question is how much. A statistically insignificant model coefficient is less a sign that a variable has no effect, and more a sign that there is insufficient data to detect a meaningful effect. If our model coefficients are not statistically significant, it is simply a good indication that we need more data. If the model coefficients are statistically significant, then our focus should be on interpreting the coefficients, not on the p-value itself.\n\n\n\n5.3.2 Model Interpretation\nModel interpretation is the process of understanding the influence that each explanatory variable has on the outcome. This involves understanding the direction, the magnitude, and the precision of the model coefficients. The coefficients themselves will tell us about direction and magnitude, while the standard errors and confidence intervals will tell us about precision.\n\n5.3.2.1 Model Coefficients\nThe model coefficients are the estimated values of the regression coefficients, or the estimated change in the outcome variable for a one unit change in the explanatory variable, while holding all other explanatory variables constant. The intercept is the estimated value of the outcome variable when all of the explanatory variables are equal to zero (or the baseline values if the variables are transformed).\nA regression’s coefficients are the most important part of interpreting the model. They specify the association between the explanatory variables and the outcome, telling us both the direction and the magnitude of the association. The direction of the association is indicated by the sign of the coefficient (positive = outcome increases with the explanatory variable, negative = outcome decreases with the explanatory variable). The magnitude of the association is indicated by the size of the coefficient (the larger the coefficient, the stronger the association). If the direction of the association is going in the wrong direction to that which we expect, this obviously indicates issues either in the theory or in the model, and if the magnitude of the effect is too small to be practically significant, then either the explanatory variable is not a good predictor of the outcome, or the model is not doing a good job of fitting the data (either the explanatory variable doesn’t really matter or the model is not capturing the true relationship between the explanatory variables and the outcome). However, while the direction is relatively easy to interpret, magnitude is not, because what counts as a practically significant effect is dependent on the context of the analysis.\n\n\n5.3.2.2 Standard Errors\nThe standard errors are the standard deviation of the estimated coefficients. This is a measure of the precision of the estimated association between the explanatory variables and the outcome. The larger the standard error, the less precise the coefficient estimate. Precisely measured coefficients suggest that the model is fitting the data well, and we can be confident that the association between the explanatory variables and the outcome is real, and not just a result of random variation in the data. Defining what counts as ‘precise’ in any analysis, much like the coefficient magnitude, is dependent on the context of the analysis. However, in general, a standard error of less than 0.5 is considered to be a good estimate, and a standard error of less than 0.25 is considered to be a very precise estimate.\nIt is important to recognise that a precise coefficient alone cannot be treated as evidence of a causal effect between the explanatory variable and the outcome but it is, at least, a good indication that the explanatory variable is a good predictor of the outcome. In order to establish a causal relationship, we may consider methods that are more robust to the presence of confounding or ommitted variables, the gold standard being randomised controlled trials, but with a strong theoretical basis and a good understanding of the data, we can make a good case for a causal relationship based on observational data."
  },
  {
    "objectID": "linear_regression_py.html#next-steps",
    "href": "linear_regression_py.html#next-steps",
    "title": "5  Linear Regression",
    "section": "5.4 Next Steps",
    "text": "5.4 Next Steps"
  },
  {
    "objectID": "linear_regression_py.html#resources",
    "href": "linear_regression_py.html#resources",
    "title": "5  Linear Regression",
    "section": "5.5 Resources",
    "text": "5.5 Resources\n\nMLU Explain: Linear Regression\nStatQuest: Linear Regression\nRegression & Other Stories\nModernDive: Statistical Inference via Data Science\nA User’s Guide to Statistical Inference and Regression\n\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari Vehtari. 2020. Regression and Other Stories. Cambridge University Press."
  },
  {
    "objectID": "version_control.html",
    "href": "version_control.html",
    "title": "Version Control",
    "section": "",
    "text": "Version control systems like Git are vital for the management of any codebase. Version control means that changes to the code (and anything else in the repository) can be tracked over time. This is a software engineering best-practice that has also been adopted in data science. It is an excellent ‘habit’ to learn when working with code, because it allows you to track changes, and therefore, revert to previous versions of the code if necessary. It also makes collaboration significantly easier, as it allows multiple people to work on the same codebase without creating conflicts.\nIf you are new to Git, a good place to start is the NHS-R Git Training. Their Introduction to Git will help you understand why version control is necessary, what Git & GitHub can offer, and how to navigate these tools.\nThese data science guides are designed to be used with Git because this is a very important skill for data science, and while it can be a bit challenging at first, it is incredibly valuable to learn, and everyone who wants to do some of the tasks in these guides should also be using Git to manage their work."
  }
]