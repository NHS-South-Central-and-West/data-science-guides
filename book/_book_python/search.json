[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Data Science Guides",
    "section": "",
    "text": "Welcome\nFor the  version of this book, click here.\nData Science Guides is a series of worked examples of common data science tasks in R & Python, to use as a space for learning how to implement these methods, and as a template for applying data science at NHS South, Central and West CSU (and beyond).\nThe intention of these guides is not to teach the reader to use R or Python, nor is the intention to teach the foundations of mathematics, statistics, and probability that underpin many of the methods used in data science. While these guides should be easy enough to follow for a relative beginner, teaching these fundamentals is beyond the scope of this book.\nInstead, the hope is that these guides serve as simple starting points for anyone looking to use a particular method, using R/Python, or for anyone looking to kick off a data science project with limited experience.\nThis book is freely available and licensed under CC BY-NC-SA 4.0."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Python Data Science Guides",
    "section": "Resources",
    "text": "Resources\nThere are lots of resources for learning some of the foundations and principles that are not available in these data science guides.\n\nLearning Statistics with R\nBeyond Multiple Linear Regression\nStatistical Rethinking\nTelling Stories with Data\nStatQuest"
  },
  {
    "objectID": "intro.html#choosing-between-r-python",
    "href": "intro.html#choosing-between-r-python",
    "title": "1  Introduction",
    "section": "\n1.1 Choosing Between R & Python",
    "text": "1.1 Choosing Between R & Python\nR and Python are two of the most popular programming languages in the world, and they are both used extensively in data science. However, there is a lot of debate about which language is better, and which language is more suitable for data science. A quick Google search for “R vs Python” will quickly point you in the direction of plenty of strongly-held opinions about why R or Python is better and why the other language is rubbish and a total waste of time, but I think these debates are a little worn out, and it doesn’t help newcomers to the field to see these dichotomous views of two of the most popular languages for data science out there.\nIn my view, as someone that started out with R before moving on to Python, and that is comfortable using either, I don’t think the most important decision is about which language you will learn. It’s the decision to learn any programming language at all. I’ve found that different people find different languages easier to get started with. Personally, I find that Python makes a little more sense to me than R. While I’ve been using R a lot longer, and am quite capable with it (perhaps more so than I am with Python), I tend to be able to learn methods/concepts easier with Python, because the logic that underpins Python methods just works for me. But that doesn’t mean that the logic underpinning Python is better. It just happens to be the case that it is well-aligned with the way that I think about programming. I think it is more important to find which language makes the most sense to you. Both are very strong on data science, as well as being strong in a number of other areas, and both are very popular. Playing around with them both and working out which one makes most sense to you, and which one gives you the best shot at maintaining your coding journey, is what matters.\nIt’s also worth noting that learning a second language, when you are confident with your first, is a lot easier. The hard part is learning the first language. If you crack that part, then you will be able to tackle the second, third, and fourth languages with a lot more ease and at a much faster pace. Learning R doesn’t mean you have to stick to R forever, and vice versa for Python. The important thing is that you’ve learned a coding language. Any coding language.\n\n1.1.1 Popularity\nBoth R & Python are widely used in data science, and both are very popular programming languages. Although there are a number of ways to measure the popularity of a programming language, every measure demonstrates that Python is one of, if not the most popular language in the world, and R is in and around the top ten. A common measure of the popularity of a programming language is the Popularity of Programming Language (PYPL) Index, which is a measure of popularity based on the number of times they are searched for on Google. The PYPL Index is updated monthly, and the data is available from the PYPL website.\nHere is how the top ten most popular programming languages looks in 2023 (based on an average PYPL Index in 2023):\n\n\n\n\nFigure 1.1: The global popularity of programming languages in 2023, according to the PYPL Index\n\n\n\nWhile Python is way out in front in first, R is also very high on the list in seventh place. This is particularly impressive given that every other language on the list is a general purpose programming language, while R is a statistical programming language. When we look at the PYPL Index in the UK, we see a similar picture, with Python even further out in front in first place, and R in in fifth place. This is a testament to the popularity of R within the niche that it occupies, of which data science is a part.\nWe can also look at how the popularity of R and Python has changed over time, in comparison with other languages that are tracked in the PYPL Index. Here is a plot of the PYPL Index over time, from 2005 to 2023:\n\n\n\n\nFigure 1.2: The global popularity of programming languages from 2005 to 2023, according to the PYPL Index\n\n\n\nAs you can see, Python has been the most popular programming language since 2018, and R has been in and around the top ten for about a decade. Both languages are extremely popular, and both are likely to remain popular for the foreseeable future.\nWhile the PYPL Index is a good measure of language popularity, there are other ways of measuring the most popular programming language, such as GitHub’s The State Of Open Source Software Report and Stack Overflow’s Developer Survey and Tag Trends.\n\n1.1.2 NHS R & Python Communities\nThere are already a sizeable number of people using both languages in the NHS, and there are communities that are already in place to support people using R or Python in the NHS.\nThe NHS-R community is a pretty active community of people that are using R in the NHS. You can find them on the NHS-R website and Slack. They also have a Github organisation and a YouTube channel which both contain tons of really useful resources.\nWith regards to Python, the equivalent community is the NHS Python Community for Healthcare (typically shortened to NHS-Pycom). The NHS Python Community also have a website and a Slack channel, and they have a Github organisation and a YouTube channel which are both worth checking out. The NHS Python Community is a little smaller than the NHS-R community, but it is still a very active community, and it is growing quickly, with both communities collaborating regularly to try and spread the growth of both languages in the NHS."
  },
  {
    "objectID": "good_science.html#a-scientific-approach-to-problem-solving",
    "href": "good_science.html#a-scientific-approach-to-problem-solving",
    "title": "2  Doing Good Science",
    "section": "2.1 A Scientific Approach to Problem Solving",
    "text": "2.1 A Scientific Approach to Problem Solving\nWhile it is important to learn the methods detailed in these guides (and more), the greatest emphasis should be placed on learning how to use the scientific method to approach problems. Approaching business problems scientifically, trying to build theories based on our observations about the subject being studied, is often overlooked, as this is in many ways the most complex part of the process. It is also less exciting than learning new ways to analyse data, so for someone that is new to data science, it is quite easy to de-emphasise this and focus on doing the fun parts instead.\nThese issues are exacerbated by the fact that any data science education, whether formal or self-taught, is often focused on the methods and tools required for data science. There is less explicit focus on the “process”, but this is because the process is taught implicitly through the methods. Teaching the various methods and tools that a data scientist needs to know is time-consuming and complex, and in the process of learning and applying the methods, the student will also learn the scientific approach to problem solving. That said, once someone has mastered the methods, they are (relatively) east to apply to real-world problems, while the process of doing science the right way is more difficult, and requires careful consideration for every practitioner, no matter how experienced they are.\n\n2.1.1 Framing the Problem\nAll of this ties into the same questions that should be asked when identifying data science opportunities. But having identified these opportunities, it is important not to skip to identifying a cool method for solving the problem. Instead, focus on the problem itself, and then identify the method(s) that will be most appropriate for answering it.\nA simple, concise approach to framing the problem and maximising the value of data science is to use Tom Carpenter’s suggestions below:\n\nIf you follow these five steps detailed by Tom, you stand a good chance of approaching the problem in the right way, generating real business value, and really getting to the heart of the problem you are trying to solve.\n\n\n2.1.2 A Simple Framework for Data Science\nThe guides here obviously focus on the methods, but before diving into the code, I think it is important to highlight the importance of learning to implement these methods appropriately, and approaching data science in the “correct” way. This means understanding the problem, generating theories about how to solve it, identifying the data that is needed to test these theories, and building a robust model that is informed by your theory and that is appropriate for the data you have. Before jumping into the code and the iterative process of model building, it is important to take the time to think carefully about your theory, the variables that are relevant to the phenomenon you are studying and the causal mechanism that guides it (or the data generating process), and your expectations from your model. Only once you’ve thought carefully about all of these things should you start to think about the method(s) you should use.\nThe order of these steps is important, but the right order is dependent on context. In an ideal situation…\nFor anyone that is analytically minded, it is very easy to fall into the trap of hyperfocusing on the cool methodological approaches you could take to solving a problem. This is something that I regularly have to check myself on, because I find it so easy to get excited about the fancy methods I could apply to a particular situation. However, despite the fact that the methods are often the most fun part of the job, the fact remains that a very fancy model won’t get you very far if you haven’t spent a lot of time thinking carefully about the problem that model is trying to solve, gathering the data that is needed to solve it, and preparing the data in a way that maximises the model’s performance.\nThe easiest way to avoid falling into the same trap that I regularly slip into is to treat the methods as a means to an end. The goal should always be to build a solution to a specific problem, with a clear understanding of what “success” looks like in that context. And if you also get to make something fancy in the process? Well isn’t that exciting!\nData plays an increasingly important role in business decision-making nowadays, due to the technological advances that have made it possible to collect and analyse large amounts of data. This is not just true of tech companies, but also of traditional businesses in sectors such as retail, manufacturing, and healthcare. The NHS is increasingly embracing the role that data plays in driving decision-making, improving clinical services, and, ultimately, improving patient outcomes.\nBut as more data becomes available, the challenge of making good use of it becomes even more difficult. The old methods are still useful, but they are no longer sufficient. In order to make the most of the data that is available, the NHS and SCW need to be able to identify when there are opportunities to use data science to improve the services that they provide. Recognising these opportunities can be challenging. It requires not only a good understanding of what is possible with data science, but also a proactive approach to identifying when it is possible to do more with data than is currently being done.\nWhile the other guides in this project will hopefully give a better understanding of what is possible with data science, this particular guide is intended to help anyone frame business problems, whether internal or external, in terms of data science, and to identify opportunities to develop more advanced solutions to those problems."
  },
  {
    "objectID": "good_science.html#identifying-data-science-opportunities",
    "href": "good_science.html#identifying-data-science-opportunities",
    "title": "2  Doing Good Science",
    "section": "2.2 Identifying Data Science Opportunities",
    "text": "2.2 Identifying Data Science Opportunities\nA lot of the analytics work that is done at SCW tends to be descriptive in nature, and takes the form of regular reports, dashboards, and Excel spreadsheets summarising data. These descriptive approaches are and always will be valuable, but there is also a limit to what descriptive analytics can tell us. Descriptive analytics can tell us what has happened, but it cannot tell us why, nor can it tell us what will happen in the future. When a piece of work is concerned with explaining past events, or predicting future events, there is an opportunity to use some of the data science techniques that are covered in these guides.\nIdentifying these opportunities is not always easy, however, because requests for work will often come in the form of requests for a certain type of solution to address a business problem, with expectations of what that solution should look like. Customers and colleagues will often have a clear idea of what they want, because that have seen or used similar solutions in the past, and this limits the scope for identifying data science opportunities. If we always provide the exact solution that is requested, without considering whether there is a better way to solve the problem, we will miss out on opportunities to extract more value from our data products.\nIt is important to understand the business problem and the solution that is being requested, but it is also important to identify why a certain solution is being requested, and how the customer or colleague plans to use it. For example, a request for a dashboard that shows utilisation of a particular service in the past twelve months is not necessarily a request for a data science solution. It is possible that the reason for wanting this dashboard is to help plan how to meet future demand for that service, but it is also possible that the reason for wanting this dashboard is simply to understand how the service is being used. While a dashboard will offer valuable context, it is possible that a machine learning model that predicts future demand might be more useful.\n\n2.2.1 Asking the Right Questions\nDesigning data science solutions to business problems is a process of discovery. The best way to identify potential data science work is to ask questions.\nThere is no foolproof set of questions you can ask, and no perfect framework for designing data science solutions, but there are some questions that are more likely to lead to useful insights than others:\n\n2.2.1.1 Identifying the Problem\n\nWhat is the problem you are trying to solve?\nWhat is the business impact of solving this problem?\nWhat is the current solution to this problem?\n\n\n\n2.2.1.2 Considering Solutions\n\nWhat do you think the solution to this problem should look like?\nWhy do you think this approach is the best way to solve this problem?\nHow will you use the solution to this problem?\n\n\n\n2.2.1.3 Measuring Success\n\nHow will you know if the solution to this problem is successful?\nHow is success defined and measured for this problem?\n\n\n\n2.2.1.4 Identifying Feasible Approaches\n\nWhat data do you need to solve this problem?\nWhat data do you have that might be useful in solving this problem?\nWhen does this piece of work need to be completed by?\nHow often will this piece of work need to be repeated/updated?"
  },
  {
    "objectID": "good_science.html#next-steps",
    "href": "good_science.html#next-steps",
    "title": "2  Doing Good Science",
    "section": "2.3 Next Steps",
    "text": "2.3 Next Steps\nAlthough the data science guides focus, for the most part, on the implementation of various data science techniques, developing SCW’s data science capabilities is more than just upskilling analysts and equipping individuals with the tools to do this work. Perhaps a more important part of this process is helping a much wider group of SCW colleagues to think about data science, and to recognise opportunities to use data science to solve business problems. Hopefully the guides will help SCW colleagues to understand what is possible, and this guide will help them to identify opportunities to do more with data using these techniques."
  },
  {
    "objectID": "good_science.html#resources",
    "href": "good_science.html#resources",
    "title": "2  Doing Good Science",
    "section": "2.4 Resources",
    "text": "2.4 Resources\n\nTechniques for Problem Solving\nProblem Solving and the Scientific Method\nRichard Feynman on the Scientific Method"
  },
  {
    "objectID": "sql.html#packages",
    "href": "sql.html#packages",
    "title": "3  Importing Data from SQL",
    "section": "\n3.1 Packages",
    "text": "3.1 Packages\nThe Python packages you need to interact with SQL are:\n\npyodbc - accessing ODBC databases\nsqlalchemy - establishing a connection and interacts with pandas\npandas - storing, manipulating, and exporting dataframes\n\nI won’t import either package here because they are only needed for a handful of functions."
  },
  {
    "objectID": "sql.html#establish-sql-connection",
    "href": "sql.html#establish-sql-connection",
    "title": "3  Importing Data from SQL",
    "section": "\n3.2 Establish SQL Connection",
    "text": "3.2 Establish SQL Connection\nIn order to run a SQL query in Python, you need to set Python up so that it can interpret the specific SQL dialect and find the database that it is running the query on. The SQLAlchemy function create_engine() will give Python everything it needs to do this, but you also have to feed in the following parameters as a string (example given in code):\n\nSQL dialect (‘mssql’)\nPython library for interacting with the database (‘pyodbc’)\nDatabase location\nSQL driver (‘?driver=SQL+Server’)\n\n\nengine = sa.create_engine('mssql+pyodbc://{server-and-db-address}?driver=SQL+Server',echo = True)\n\nHaving specified a SQL engine, you can establish a connection.\n\nconn = engine.connect()"
  },
  {
    "objectID": "sql.html#running-sql-query",
    "href": "sql.html#running-sql-query",
    "title": "3  Importing Data from SQL",
    "section": "\n3.3 Running SQL Query",
    "text": "3.3 Running SQL Query\nYou have two ways of going about running a SQL query in a Python script. You can either write your query out explicitly in your Python script, or you can read in an external SQL query. If the query is particularly lengthy, it is better to store is as a .sql file and call it from Python, to make it easier to read your code, and to maintain both components.\n\n# open and read sql query\nquery = open('path-to-query\\query.sql', 'r')\n\n# read query in to pandas dataframe\ndf = pd.read_sql_query(query.read(),conn)\n\n# close sql query\nquery.close()\n\nYou can check that this process has worked as expected by inspecting your pandas dataframe.\n\ndf.head()"
  },
  {
    "objectID": "sql.html#export-to-csv",
    "href": "sql.html#export-to-csv",
    "title": "3  Importing Data from SQL",
    "section": "\n3.4 Export to CSV",
    "text": "3.4 Export to CSV\nFinally, you can export your dataframe using the pandas function to_csv(). If you want to retain the index column that pandas adds to the dataframe, simply change index=False to True.\n\ndf.to_csv('path-to-save-df\\df.csv', index=False)"
  },
  {
    "objectID": "wrangling_py.html#python-libraries-for-data-wrangling",
    "href": "wrangling_py.html#python-libraries-for-data-wrangling",
    "title": "4  Wrangling Data",
    "section": "4.1 Python Libraries for Data Wrangling",
    "text": "4.1 Python Libraries for Data Wrangling\nThe Python ecosystem for data wrangling is not quite as rich as in R, but there is one library that is extremely dominant (pandas) and one library that is seeking to disrupt it (polars)."
  },
  {
    "objectID": "wrangling_py.html#fingertips-data",
    "href": "wrangling_py.html#fingertips-data",
    "title": "4  Wrangling Data",
    "section": "4.2 Fingertips Data",
    "text": "4.2 Fingertips Data\nWe will use Public Health England’s Fingertips API (for which there are packages available in R and Python) as the data source for this guide, as it represents a good opportunity to give a brief overview of this data and how to use it. Fingertips is a repository of public health data on a wide range of topics. The data included in this tutorial relates to the wider determinants of health.\nFirst, we can look at what is available.\nUnfortunately, the fingertips_py package does seem to be a little less mature than the R equivalent, and there are a lot of errors running the functions we would need to import the data. While this is the case, we’ll take a little shortcut, importing the data from a CSV that I’ve stored in the guides GitHub repository, using the Fingertips R package.\n\nphof_raw = pd.read_csv(\"https://raw.githubusercontent.com/NHS-South-Central-and-West/data-science-guides/main/data/phof.csv\", low_memory=False)\n\n\nphof_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 264329 entries, 0 to 264328\nData columns (total 27 columns):\n #   Column                               Non-Null Count   Dtype  \n---  ------                               --------------   -----  \n 0   IndicatorID                          264329 non-null  int64  \n 1   IndicatorName                        264329 non-null  object \n 2   ParentCode                           261229 non-null  object \n 3   ParentName                           261229 non-null  object \n 4   AreaCode                             264329 non-null  object \n 5   AreaName                             264329 non-null  object \n 6   AreaType                             264329 non-null  object \n 7   Sex                                  264329 non-null  object \n 8   Age                                  264329 non-null  object \n 9   CategoryType                         0 non-null       float64\n 10  Category                             0 non-null       float64\n 11  Timeperiod                           264329 non-null  object \n 12  Value                                249986 non-null  float64\n 13  LowerCI95.0limit                     243173 non-null  float64\n 14  UpperCI95.0limit                     243173 non-null  float64\n 15  LowerCI99.8limit                     56885 non-null   float64\n 16  UpperCI99.8limit                     56885 non-null   float64\n 17  Count                                187440 non-null  float64\n 18  Denominator                          198367 non-null  float64\n 19  Valuenote                            28742 non-null   object \n 20  RecentTrend                          42091 non-null   object \n 21  ComparedtoEnglandvalueorpercentiles  264329 non-null  object \n 22  Comparedtopercentiles                264329 non-null  object \n 23  TimeperiodSortable                   264329 non-null  int64  \n 24  Newdata                              72025 non-null   object \n 25  Comparedtogoal                       38885 non-null   object \n 26  Timeperiodrange                      264329 non-null  object \ndtypes: float64(9), int64(2), object(16)\nmemory usage: 54.5+ MB\n\n\n\nphof_raw.describe()\n\n\n\n\n\n\n\n\nIndicatorID\nCategoryType\nCategory\nValue\nLowerCI95.0limit\nUpperCI95.0limit\nLowerCI99.8limit\nUpperCI99.8limit\nCount\nDenominator\nTimeperiodSortable\n\n\n\n\ncount\n264329.000000\n0.0\n0.0\n249986.000000\n243173.000000\n243173.000000\n56885.000000\n56885.000000\n1.874400e+05\n1.983670e+05\n2.643290e+05\n\n\nmean\n72371.514560\nNaN\nNaN\n72.092017\n65.526956\n81.604363\n122.884874\n165.406557\n4.023386e+04\n1.390792e+07\n2.015224e+07\n\n\nstd\n30934.529754\nNaN\nNaN\n276.950447\n253.887549\n311.174288\n452.040131\n622.510687\n1.124600e+06\n1.361739e+09\n4.862563e+04\n\n\nmin\n10301.000000\nNaN\nNaN\n-36.400000\n-72.200000\n-5.200000\n0.000000\n0.000000\n-1.800000e+02\n2.544061e-01\n2.000000e+07\n\n\n25%\n41001.000000\nNaN\nNaN\n12.398315\n8.300000\n16.790000\n22.773030\n33.147850\n6.200000e+01\n7.900000e+02\n2.012000e+07\n\n\n50%\n90634.000000\nNaN\nNaN\n40.859605\n35.579380\n50.900000\n58.024690\n69.074819\n4.850000e+02\n4.798000e+03\n2.016000e+07\n\n\n75%\n92517.000000\nNaN\nNaN\n75.900000\n72.400000\n79.700120\n83.092446\n90.801542\n4.164250e+03\n1.181570e+05\n2.019000e+07\n\n\nmax\n93982.000000\nNaN\nNaN\n9110.634850\n8473.979610\n9782.239270\n8122.734550\n10179.121160\n1.942292e+08\n2.894740e+11\n2.023000e+07\n\n\n\n\n\n\n\n\nphof_raw.head()\n\n\n\n\n\n\n\n\nIndicatorID\nIndicatorName\nParentCode\nParentName\nAreaCode\nAreaName\nAreaType\nSex\nAge\nCategoryType\n...\nCount\nDenominator\nValuenote\nRecentTrend\nComparedtoEnglandvalueorpercentiles\nComparedtopercentiles\nTimeperiodSortable\nNewdata\nComparedtogoal\nTimeperiodrange\n\n\n\n\n0\n90362\nA01a - Healthy life expectancy at birth\nNaN\nNaN\nE92000001\nEngland\nEngland\nMale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNot compared\nNot compared\n20090000\nNaN\nNaN\n3y\n\n\n1\n90362\nA01a - Healthy life expectancy at birth\nNaN\nNaN\nE92000001\nEngland\nEngland\nFemale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNot compared\nNot compared\n20090000\nNaN\nNaN\n3y\n\n\n2\n90362\nA01a - Healthy life expectancy at birth\nE92000001\nEngland\nE06000001\nHartlepool\nCounties & UAs (from Apr 2023)\nMale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nWorse\nNot compared\n20090000\nNaN\nNaN\n3y\n\n\n3\n90362\nA01a - Healthy life expectancy at birth\nE92000001\nEngland\nE06000002\nMiddlesbrough\nCounties & UAs (from Apr 2023)\nMale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nWorse\nNot compared\n20090000\nNaN\nNaN\n3y\n\n\n4\n90362\nA01a - Healthy life expectancy at birth\nE92000001\nEngland\nE06000003\nRedcar and Cleveland\nCounties & UAs (from Apr 2023)\nMale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nWorse\nNot compared\n20090000\nNaN\nNaN\n3y\n\n\n\n\n5 rows × 27 columns\n\n\n\nWith the raw data imported, we are now able to start the process of cleaning and wrangling the data into a format that is ready for analysis."
  },
  {
    "objectID": "wrangling_py.html#data-wrangling",
    "href": "wrangling_py.html#data-wrangling",
    "title": "4  Wrangling Data",
    "section": "4.3 Data Wrangling",
    "text": "4.3 Data Wrangling\nFirst, we will clean the column names using the pyjanitor’s clean_names function. This will convert all column names to lower case and remove any spaces and special characters. This will make it easier to work with the data in the future.\n\nphof_raw = (\n    phof_raw\n    .pipe(clean_names)\n)\n\nWe’ve got a total of 168 indicators, which means some of the indicators from the Public Health Outcomes Framework are not available for upper tier local authorities. We can filter the data to just the indicators we are interested in (as well as filtering any rows where the value is missing), using the panda’s loc function. We can also use the group_by and mean functions to get the mean value of the indicators across all areas in the data. Below is an example:\n\n(\n  phof_raw\n  .loc[(phof_raw['indicatorid'].isin([90366, 20101, 11601]))]\n  .dropna(subset='value')\n  .groupby('indicatorname')['value']\n  .mean()\n)\n\nindicatorname\nA01b - Life expectancy at birth                                      80.199513\nB16 - Utilisation of outdoor space for exercise or health reasons    16.216824\nC04 - Low birth weight of term babies                                 2.870756\nName: value, dtype: float64\n\n\nThe data pulled from the Fingertips API has a number of idiosyncracies that need addressing when wrangling the data. For example, the timeperiodsortable column refers to the year that the data relates to, and it contains four superfluous zeros at the end of each year (i.e, 20230000). We can use the pandas assign function to create a year column which strips out the zeros from the timeperiodsortable column by dividing it by 10000, and dropping the decimal place by converting year to integer (using the astype(int) function).\n\n# filter for the indicators of interest and wrangle data into tidy structure\n(\n  phof_raw\n  .loc[(\n    phof_raw['indicatorid'].isin([90366, 20101, 11601])) \n    & (phof_raw['areacode'] == 'E92000001' \n    )]\n  .assign(year=lambda x: (x['timeperiodsortable'] / 10000).astype(int))\n  .groupby(['indicatorname', 'areacode', 'year'], as_index=False)['value']\n  .mean()\n)\n\n\n\n\n\n\n\n\nindicatorname\nareacode\nyear\nvalue\n\n\n\n\n0\nA01b - Life expectancy at birth\nE92000001\n2001\n78.448740\n\n\n1\nA01b - Life expectancy at birth\nE92000001\n2002\n78.700330\n\n\n2\nA01b - Life expectancy at birth\nE92000001\n2003\n78.987650\n\n\n3\nA01b - Life expectancy at birth\nE92000001\n2004\n79.358295\n\n\n4\nA01b - Life expectancy at birth\nE92000001\n2005\n79.614160\n\n\n5\nA01b - Life expectancy at birth\nE92000001\n2006\n79.817360\n\n\n6\nA01b - Life expectancy at birth\nE92000001\n2007\n80.082390\n\n\n7\nA01b - Life expectancy at birth\nE92000001\n2008\n80.355270\n\n\n8\nA01b - Life expectancy at birth\nE92000001\n2009\n80.744790\n\n\n9\nA01b - Life expectancy at birth\nE92000001\n2010\n80.982525\n\n\n10\nA01b - Life expectancy at birth\nE92000001\n2011\n81.154860\n\n\n11\nA01b - Life expectancy at birth\nE92000001\n2012\n81.276175\n\n\n12\nA01b - Life expectancy at birth\nE92000001\n2013\n81.283525\n\n\n13\nA01b - Life expectancy at birth\nE92000001\n2014\n81.337425\n\n\n14\nA01b - Life expectancy at birth\nE92000001\n2015\n81.339375\n\n\n15\nA01b - Life expectancy at birth\nE92000001\n2016\n81.419660\n\n\n16\nA01b - Life expectancy at birth\nE92000001\n2017\n81.565000\n\n\n17\nA01b - Life expectancy at birth\nE92000001\n2018\n81.270000\n\n\n18\nA01b - Life expectancy at birth\nE92000001\n2021\n80.748915\n\n\n19\nB16 - Utilisation of outdoor space for exercis...\nE92000001\n2011\n14.015031\n\n\n20\nB16 - Utilisation of outdoor space for exercis...\nE92000001\n2012\n15.329390\n\n\n21\nB16 - Utilisation of outdoor space for exercis...\nE92000001\n2013\n17.130101\n\n\n22\nB16 - Utilisation of outdoor space for exercis...\nE92000001\n2014\n17.907098\n\n\n23\nB16 - Utilisation of outdoor space for exercis...\nE92000001\n2015\n17.917663\n\n\n24\nC04 - Low birth weight of term babies\nE92000001\n2006\n3.018037\n\n\n25\nC04 - Low birth weight of term babies\nE92000001\n2007\n2.912639\n\n\n26\nC04 - Low birth weight of term babies\nE92000001\n2008\n2.894468\n\n\n27\nC04 - Low birth weight of term babies\nE92000001\n2009\n2.919269\n\n\n28\nC04 - Low birth weight of term babies\nE92000001\n2010\n2.853172\n\n\n29\nC04 - Low birth weight of term babies\nE92000001\n2011\n2.844595\n\n\n30\nC04 - Low birth weight of term babies\nE92000001\n2012\n2.798094\n\n\n31\nC04 - Low birth weight of term babies\nE92000001\n2013\n2.822148\n\n\n32\nC04 - Low birth weight of term babies\nE92000001\n2014\n2.864100\n\n\n33\nC04 - Low birth weight of term babies\nE92000001\n2015\n2.774469\n\n\n34\nC04 - Low birth weight of term babies\nE92000001\n2016\n2.786173\n\n\n35\nC04 - Low birth weight of term babies\nE92000001\n2017\n2.819327\n\n\n36\nC04 - Low birth weight of term babies\nE92000001\n2018\n2.864822\n\n\n37\nC04 - Low birth weight of term babies\nE92000001\n2019\n2.902918\n\n\n38\nC04 - Low birth weight of term babies\nE92000001\n2020\n2.856152\n\n\n39\nC04 - Low birth weight of term babies\nE92000001\n2021\n2.774210\n\n\n\n\n\n\n\nWe can also remove the ID that is appended to the beginning of each indicator name. We can use str.replace to remove this part of the string from the indicator name, using a regular expression to remove everything up to and including the dash (and the space immediately after).\n\n(\n  phof_raw\n  .loc[(phof_raw['indicatorid'].isin([90366, 20101, 11601]))]\n  .assign(\n    indicatorname = lambda x: x['indicatorname']\n                              .str.replace('^[^-]+- ', '', regex=True))\n)\n\n\n\n\n\n\n\n\nindicatorid\nindicatorname\nparentcode\nparentname\nareacode\nareaname\nareatype\nsex\nage\ncategorytype\n...\ncount\ndenominator\nvaluenote\nrecenttrend\ncomparedtoenglandvalueorpercentiles\ncomparedtopercentiles\ntimeperiodsortable\nnewdata\ncomparedtogoal\ntimeperiodrange\n\n\n\n\n2984\n90366\nLife expectancy at birth\nNaN\nNaN\nE92000001\nEngland\nEngland\nMale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNot compared\nNot compared\n20010000\nNaN\nNaN\n3y\n\n\n2985\n90366\nLife expectancy at birth\nNaN\nNaN\nE92000001\nEngland\nEngland\nFemale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNot compared\nNot compared\n20010000\nNaN\nNaN\n3y\n\n\n2986\n90366\nLife expectancy at birth\nE92000001\nEngland\nE06000001\nHartlepool\nCounties & UAs (from Apr 2023)\nMale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNot compared\nNot compared\n20010000\nNaN\nNaN\n3y\n\n\n2987\n90366\nLife expectancy at birth\nE92000001\nEngland\nE06000002\nMiddlesbrough\nCounties & UAs (from Apr 2023)\nMale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNot compared\nNot compared\n20010000\nNaN\nNaN\n3y\n\n\n2988\n90366\nLife expectancy at birth\nE92000001\nEngland\nE06000003\nRedcar and Cleveland\nCounties & UAs (from Apr 2023)\nMale\nAll ages\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNot compared\nNot compared\n20010000\nNaN\nNaN\n3y\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n103354\n20101\nLow birth weight of term babies\nE92000001\nEngland\nE10000029\nSuffolk\nCounties & UAs (from Apr 2023)\nPersons\n&gt;=37 weeks gestational age at birth\nNaN\n...\n169.0\n6124.0\nNaN\nNo significant change\nSimilar\nNot compared\n20210000\nNaN\nNaN\n1y\n\n\n103355\n20101\nLow birth weight of term babies\nE92000001\nEngland\nE10000030\nSurrey\nCounties & UAs (from Apr 2023)\nPersons\n&gt;=37 weeks gestational age at birth\nNaN\n...\n277.0\n11536.0\nNaN\nNo significant change\nBetter\nNot compared\n20210000\nNaN\nNaN\n1y\n\n\n103356\n20101\nLow birth weight of term babies\nE92000001\nEngland\nE10000031\nWarwickshire\nCounties & UAs (from Apr 2023)\nPersons\n&gt;=37 weeks gestational age at birth\nNaN\n...\n128.0\n5474.0\nNaN\nNo significant change\nBetter\nNot compared\n20210000\nNaN\nNaN\n1y\n\n\n103357\n20101\nLow birth weight of term babies\nE92000001\nEngland\nE10000032\nWest Sussex\nCounties & UAs (from Apr 2023)\nPersons\n&gt;=37 weeks gestational age at birth\nNaN\n...\n138.0\n7854.0\nNaN\nNo significant change\nBetter\nNot compared\n20210000\nNaN\nNaN\n1y\n\n\n103358\n20101\nLow birth weight of term babies\nE92000001\nEngland\nE10000034\nWorcestershire\nCounties & UAs (from Apr 2023)\nPersons\n&gt;=37 weeks gestational age at birth\nNaN\n...\n117.0\n5069.0\nNaN\nNo significant change\nBetter\nNot compared\n20210000\nNaN\nNaN\n1y\n\n\n\n\n8847 rows × 27 columns\n\n\n\nFinally, the data is structured in a wide format, with each indicator having its own column. This is not a tidy structure, as each column should represent a variable, and each row should represent an observation. We can use the pivot_table function to convert the data into a tidy structure, with each indicator having its own row. Pandas will turn areacode and timeperiodsortable into the index of the dataframe, and in order to convert these variables back to columns we use reset_index and rename_axis(None, axis=1). We can also use the rename function to rename the columns to something more meaningful, and the dropna function to remove any rows where the value is missing.\n\n(\n  phof_raw\n  .loc[(phof_raw['indicatorid'].isin([90366, 20101, 11601]))]\n  .groupby(\n    ['indicatorname', 'areacode', 'timeperiodsortable'], \n    as_index=False\n    )['value']\n  .mean()\n  .pivot_table(\n    index=['areacode', 'timeperiodsortable'],\n    columns='indicatorname',\n    values='value'\n    )\n  .rename(\n    columns={\n      'A01b - Life expectancy at birth': 'life_expectancy',\n      'C04 - Low birth weight of term babies': 'low_birth_weight', \n      'B16 - Utilisation of outdoor space for exercise or health reasons': 'outdoor_space'\n      })\n  .reset_index()\n  .rename_axis(None, axis=1)\n  .dropna()\n)\n\n\n\n\n\n\n\n\nareacode\ntimeperiodsortable\nlife_expectancy\noutdoor_space\nlow_birth_weight\n\n\n\n\n11\nE06000001\n20120000\n79.607655\n21.131038\n3.222749\n\n\n12\nE06000001\n20130000\n79.075110\n9.837764\n2.555911\n\n\n13\nE06000001\n20140000\n78.846375\n18.596376\n2.989691\n\n\n14\nE06000001\n20150000\n78.738395\n11.314412\n1.794616\n\n\n31\nE06000002\n20110000\n78.344225\n16.792305\n3.448276\n\n\n...\n...\n...\n...\n...\n...\n\n\n3104\nE92000001\n20110000\n81.154860\n14.015031\n2.844595\n\n\n3105\nE92000001\n20120000\n81.276175\n15.329390\n2.798094\n\n\n3106\nE92000001\n20130000\n81.283525\n17.130101\n2.822148\n\n\n3107\nE92000001\n20140000\n81.337425\n17.907098\n2.864100\n\n\n3108\nE92000001\n20150000\n81.339375\n17.917663\n2.774469\n\n\n\n\n667 rows × 5 columns\n\n\n\nWe can combine all of the above steps into a single function, which we can then apply to the raw data to get the data into a tidy structure.\n\n# function to wrangle data into tidy structure\ndef wrangle_phof_data(data):\n  \"\"\"\n  Returns a tidy, formatted DataFrame with Public Health England data\n  \"\"\"\n  phof_data = (\n    data\n    .loc[(data['indicatorid'].isin([90366, 20101, 11601]))]\n    .assign(year=lambda x: (x['timeperiodsortable'] / 10000).astype(int))\n    .groupby(\n      ['indicatorname', 'areacode', 'year'], \n      as_index=False\n      )['value']\n    .mean()\n    .pivot_table(\n      index=['areacode', 'year'],\n      columns='indicatorname',\n      values='value'\n      )\n    .rename(\n      columns={\n        'A01b - Life expectancy at birth': 'life_expectancy',\n        'C04 - Low birth weight of term babies': 'low_birth_weight', \n        'B16 - Utilisation of outdoor space for exercise or health reasons': 'outdoor_space'\n        })\n    .reset_index()\n    .rename_axis(None, axis=1)\n    .dropna()\n    )\n  \n  return phof_data\n\nphof = wrangle_phof_data(phof_raw)\nphof.head()\n\n\n\n\n\n\n\n\nareacode\nyear\nlife_expectancy\noutdoor_space\nlow_birth_weight\n\n\n\n\n11\nE06000001\n2012\n79.607655\n21.131038\n3.222749\n\n\n12\nE06000001\n2013\n79.075110\n9.837764\n2.555911\n\n\n13\nE06000001\n2014\n78.846375\n18.596376\n2.989691\n\n\n14\nE06000001\n2015\n78.738395\n11.314412\n1.794616\n\n\n31\nE06000002\n2011\n78.344225\n16.792305\n3.448276"
  },
  {
    "objectID": "wrangling_py.html#next-steps",
    "href": "wrangling_py.html#next-steps",
    "title": "4  Wrangling Data",
    "section": "4.4 Next Steps",
    "text": "4.4 Next Steps\nWe have successfully imported, cleaned, and wrangled Fingertips data into a tidy structure. We can now move on to the next step of the data science process, which is to explore the data and perform some analysis.\nWe could save the data to a CSV file at this point, using the pandas to_csv function (for example, phof.to_csv('data/phof.csv', index=False)), or we could perform the analysis on the dataframe object we have created.\nThis is just one example of the data wrangling process, but in reality there is a vast array of ways you might transform data in the process of preparing it for an analysis. There is no set recipe for this process, but the goal is to transform data from whatever raw form it arrives in to something structured (typically this means tidy data) that lends itself to analysis."
  },
  {
    "objectID": "wrangling_py.html#resources",
    "href": "wrangling_py.html#resources",
    "title": "4  Wrangling Data",
    "section": "4.5 Resources",
    "text": "4.5 Resources\n\nHadley Wickham - Tidy Data\nModern Polars\nPandas Anti Patterns\nMinimally Sufficient Pandas\nPandas Data Structures\nPandas Series Introduction"
  },
  {
    "objectID": "eda_py.html#inspecting-the-data",
    "href": "eda_py.html#inspecting-the-data",
    "title": "5  Exploratory Data Analysis",
    "section": "5.1 Inspecting the Data",
    "text": "5.1 Inspecting the Data\nThe first step when doing EDA is to inspect the data itself and get an idea of the structure of the dataset, the variable types, and the typical values of each variable. This gives a better understanding of exactly what data is being used and informs decisions both about the next steps in the exploratory process and any modelling choices.\nWe can use the head() and info() functions to get a sense of the structure of the data. The head() function returns the first five rows of the data, and the info() method returns a summary of the data, including the number of rows, the number of columns, the column names, and the data type of each column. The info() method is particularly useful for identifying missing values, as it returns the number of non-null values in each column. If the number of non-null values is less than the number of rows, then there are missing values in the column.\nIn addition to these two methods, we can also use the nunique() method to count the number of unique values in each column, which helps identify categorical variables, and we can use the unique() method to get a list of the unique values in a column.\n\ndf.head()\n\n\n\n\n\n\n\n\nage\nsex\nresting_bp\ncholesterol\nfasting_bs\nresting_ecg\nmax_hr\nangina\nheart_peak_reading\nheart_disease\n\n\n\n\n0\n40\nM\n140\n289\n0\nNormal\n172\nN\n0.0\n0\n\n\n1\n49\nF\n160\n180\n0\nNormal\n156\nN\n1.0\n1\n\n\n2\n37\nM\n130\n283\n0\nST\n98\nN\n0.0\n0\n\n\n3\n48\nF\n138\n214\n0\nNormal\n108\nY\n1.5\n1\n\n\n4\n54\nM\n150\n195\n0\nNormal\n122\nN\n0.0\n0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 918 entries, 0 to 917\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   age                 918 non-null    int64   \n 1   sex                 918 non-null    category\n 2   resting_bp          918 non-null    int64   \n 3   cholesterol         918 non-null    int64   \n 4   fasting_bs          918 non-null    category\n 5   resting_ecg         918 non-null    category\n 6   max_hr              918 non-null    int64   \n 7   angina              918 non-null    category\n 8   heart_peak_reading  918 non-null    float64 \n 9   heart_disease       918 non-null    category\ndtypes: category(5), float64(1), int64(4)\nmemory usage: 41.1 KB\n\n\n\n# count unique values in each column\ndf.nunique()\n\nage                    50\nsex                     2\nresting_bp             67\ncholesterol           222\nfasting_bs              2\nresting_ecg             3\nmax_hr                119\nangina                  2\nheart_peak_reading     53\nheart_disease           2\ndtype: int64\n\n\n\n# unique values of the outcome variable\ndf.heart_disease.unique()\n\n[0, 1]\nCategories (2, int64): [0, 1]\n\n\n\n# unique values of a continuous explanatory variable\ndf.cholesterol.unique()\n\narray([289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 211, 164, 204,\n       234, 273, 196, 201, 248, 267, 223, 184, 288, 215, 209, 260, 468,\n       188, 518, 167, 224, 172, 186, 254, 306, 250, 177, 227, 230, 294,\n       264, 259, 175, 318, 216, 340, 233, 205, 245, 194, 270, 213, 365,\n       342, 253, 277, 202, 297, 225, 246, 412, 265, 182, 218, 268, 163,\n       529, 100, 206, 238, 139, 263, 291, 229, 307, 210, 329, 147,  85,\n       269, 275, 179, 392, 466, 129, 241, 255, 276, 282, 338, 160, 156,\n       272, 240, 393, 161, 228, 292, 388, 166, 247, 331, 341, 243, 279,\n       198, 249, 168, 603, 159, 190, 185, 290, 212, 231, 222, 235, 320,\n       187, 266, 287, 404, 312, 251, 328, 285, 280, 192, 193, 308, 219,\n       257, 132, 226, 217, 303, 298, 256, 117, 295, 173, 315, 281, 309,\n       200, 336, 355, 326, 171, 491, 271, 274, 394, 221, 126, 305, 220,\n       242, 347, 344, 358, 169, 181,   0, 236, 203, 153, 316, 311, 252,\n       458, 384, 258, 349, 142, 197, 113, 261, 310, 232, 110, 123, 170,\n       369, 152, 244, 165, 337, 300, 333, 385, 322, 564, 239, 293, 407,\n       149, 199, 417, 178, 319, 354, 330, 302, 313, 141, 327, 304, 286,\n       360, 262, 325, 299, 409, 174, 183, 321, 353, 335, 278, 157, 176,\n       131], dtype=int64)"
  },
  {
    "objectID": "eda_py.html#summary-statistics",
    "href": "eda_py.html#summary-statistics",
    "title": "5  Exploratory Data Analysis",
    "section": "5.2 Summary Statistics",
    "text": "5.2 Summary Statistics\nSummary statistics are a quick and easy way to get a sense of the distribution and central tendency of the variables in the dataset. We can use the describe() method to get a quick overview of every column in the dataset, including the row count, the mean and standard deviation, the minimum and maximum value, and quartiles of each variable.\n\n# summary of the data\ndf.describe()\n\n\n\n\n\n\n\n\nage\nresting_bp\ncholesterol\nmax_hr\nheart_peak_reading\n\n\n\n\ncount\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n\n\nmean\n53.510893\n132.396514\n198.799564\n136.809368\n0.887364\n\n\nstd\n9.432617\n18.514154\n109.384145\n25.460334\n1.066570\n\n\nmin\n28.000000\n0.000000\n0.000000\n60.000000\n-2.600000\n\n\n25%\n47.000000\n120.000000\n173.250000\n120.000000\n0.000000\n\n\n50%\n54.000000\n130.000000\n223.000000\n138.000000\n0.600000\n\n\n75%\n60.000000\n140.000000\n267.000000\n156.000000\n1.500000\n\n\nmax\n77.000000\n200.000000\n603.000000\n202.000000\n6.200000\n\n\n\n\n\n\n\nWhile the describe() function is pretty effective, the skimpy package can provide a more detailed summary of the data, using the skim() function. If you are looking for a single function to capture the entire process of inspecting the data and computing summary statistics, skim() is the function for the job, giving you a wealth of information about the dataset as a whole and each variable in the data.\nAnother package that provides a similar function is the [profiling][ydata-profiling] package, which can be used to generate a report containing a summary of the data, including the data types, missing values, and summary statistics. The ydata-profiling package is particularly useful for generating a report that can be shared with others, as it can be exported as an HTML file, however it’s a bit more resource-intensive than skimpy, so we will stick with skimpy for this tutorial.\n\n# more detailed summary of the data\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types               Categories                                        │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓ ┏━━━━━━━━━━━━━━━━━━━━━━━┓                                │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃ ┃ Categorical Variables ┃                                │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩ ┡━━━━━━━━━━━━━━━━━━━━━━━┩                                │\n│ │ Number of rows    │ 918    │ │ category    │ 5     │ │ sex                   │                                │\n│ │ Number of columns │ 10     │ │ int32       │ 4     │ │ fasting_bs            │                                │\n│ └───────────────────┴────────┘ │ float64     │ 1     │ │ resting_ecg           │                                │\n│                                └─────────────┴───────┘ │ angina                │                                │\n│                                                        │ heart_disease         │                                │\n│                                                        └───────────────────────┘                                │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┓  │\n│ ┃ column_name                ┃ NA   ┃ NA %    ┃ mean    ┃ sd    ┃ p0     ┃ p25   ┃ p75   ┃ p100   ┃ hist     ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━━━┩  │\n│ │ age                        │    0 │       0 │      54 │   9.4 │     28 │    47 │    60 │     77 │  ▁▄▅█▅▁  │  │\n│ │ resting_bp                 │    0 │       0 │     130 │    19 │      0 │   120 │   140 │    200 │     █▆▁  │  │\n│ │ cholesterol                │    0 │       0 │     200 │   110 │      0 │   170 │   270 │    600 │   ▃▂█▁   │  │\n│ │ max_hr                     │    0 │       0 │     140 │    25 │     60 │   120 │   160 │    200 │   ▃██▆▁  │  │\n│ │ heart_peak_reading         │    0 │       0 │    0.89 │   1.1 │   -2.6 │     0 │   1.5 │    6.2 │    █▆▃   │  │\n│ └────────────────────────────┴──────┴─────────┴─────────┴───────┴────────┴───────┴───────┴────────┴──────────┘  │\n│                                                    category                                                     │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                          ┃ NA       ┃ NA %          ┃ ordered              ┃ unique            ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩  │\n│ │ sex                                  │        0 │             0 │ False                │                 2 │  │\n│ │ fasting_bs                           │        0 │             0 │ False                │                 2 │  │\n│ │ resting_ecg                          │        0 │             0 │ False                │                 3 │  │\n│ │ angina                               │        0 │             0 │ False                │                 2 │  │\n│ │ heart_disease                        │        0 │             0 │ False                │                 2 │  │\n│ └──────────────────────────────────────┴──────────┴───────────────┴──────────────────────┴───────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\nIf we want to examine a particular variable, the functions mean(), median(), quantile(), min(), and max() will return the same information as the describe() function. We can also get a sense of dispersion by computing the standard deviation or variance of a variable. The std() function returns the standard deviation of a variable, and the var() function returns the variance.\n\n# mean & median age\ndf.age.mean(), df.age.median()\n\n(53.510893246187365, 54.0)\n\n\n\n# min and max age\ndf.age.min(), df.age.max()\n\n(28, 77)\n\n\n\n# dispersion of age\ndf.age.std(), df.age.var()\n\n(9.43261650673201, 88.9742541630732)\n\n\nFinally, we can use the value_counts() function to get a count of the number of observations in each category of a discrete variable.\n\n# heart disease count\ndf['heart_disease'].value_counts()\n\nheart_disease\n1    508\n0    410\nName: count, dtype: int64\n\n\n\n# resting ecg count\ndf['resting_ecg'].value_counts()\n\nresting_ecg\nNormal    552\nLVH       188\nST        178\nName: count, dtype: int64\n\n\n\n# angina\ndf['angina'].value_counts()\n\nangina\nN    547\nY    371\nName: count, dtype: int64\n\n\n\n# cholesterol\ndf['cholesterol'].value_counts()\n\ncholesterol\n0      172\n254     11\n223     10\n220     10\n230      9\n      ... \n392      1\n316      1\n153      1\n466      1\n131      1\nName: count, Length: 222, dtype: int64\n\n\nWe can also use the groupby() method to get the counts of each category in a categorical variable, grouped by another categorical variable.\n\ndf.groupby(['resting_ecg'])['heart_disease'].value_counts()\n\nresting_ecg  heart_disease\nLVH          1                106\n             0                 82\nNormal       1                285\n             0                267\nST           1                117\n             0                 61\nName: count, dtype: int64\n\n\nIn addition to the counts, we can also get the proportions of each category using the normalize=True argument.\n\ndf.groupby(['resting_ecg'])['heart_disease'].value_counts(normalize=True).round(3)\n\nresting_ecg  heart_disease\nLVH          1                0.564\n             0                0.436\nNormal       1                0.516\n             0                0.484\nST           1                0.657\n             0                0.343\nName: proportion, dtype: float64"
  },
  {
    "objectID": "eda_py.html#data-visualisation",
    "href": "eda_py.html#data-visualisation",
    "title": "5  Exploratory Data Analysis",
    "section": "5.3 Data Visualisation",
    "text": "5.3 Data Visualisation\nWhile inspecting the data directly and using summary statistics to describe it is a good first step, data visualisation is a more effective way to explore the data. It allows us to quickly identify patterns and relationships in the data, and to identify any data quality issues that might not be immediately obvious without a visual representation of the data.\nWhen using data visualisation for exploratory purposes, the intent is generally to visualise the way data is distributed, both within and between variables. This can be done using a variety of different types of plots, including histograms, bar charts, box plots, scatter plots, and line plots. How variables are distributed can tell us a lot about the variable itself, and how variables are distributed relative to each other can tell us a lot about the potential relationship between the variables.\nIn this tutorial, we will use the matplotlib and seaborn packages to create a series of data visualisations to explore the data in more detail. The seaborn package is a high-level data visualisation library that is built on top of matplotlib. Although data visualisation in Python is not as straightforward as it is in R, seaborn makes it much easier to create good quality and informative plots.\n\n5.3.1 Visualising Data Distributions\nThe first step in the exploratory process is to visualise the data distributions of key variables in the dataset. This allows us to get a sense of the typical values and central tendency of the variable, as well as identifying any outliers or other data quality issues.\n\n5.3.1.1 Continuous Distributions\nFor continuous variables, we can use histograms to visualise the distribution of the data. We can use the histplot() function to create a histogram of a continuous variable. The binwidth argument allows us to specify the width of the bins in the histogram.\n\n# age distribution\nsns.histplot(data=df, x='age', binwidth=5)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# max hr distribution\nsns.histplot(data=df, x='max_hr', binwidth=10)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# cholesterol distribution\nsns.histplot(data=df, x='cholesterol', binwidth=25)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# cholesterol distribution\nsns.histplot(data=df.loc[df.cholesterol!=0], x='cholesterol', binwidth=25)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nThe inflated zero values in the cholesterol distribution suggests that there may be an issue with data quality that needs addressing.\n\n\n5.3.1.2 Discrete Distributions\nWe can use bar plots to visualise the distribution of discrete variables. We can use the countplot() function to create a bar plot of a discrete variable.\n\n# heart disease distribution\nsns.countplot(data=df, x='heart_disease')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# sex distribution\nsns.countplot(data=df, x='sex')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# angina distribution\nsns.countplot(data=df, x='angina')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Comparing Distributions\nThere are a number of ways to compare the distributions of multiple variables. Bar plots can be used to visualise two discrete variables, while histograms and box plots are useful for comparing the distribution of a continuous variable across the groups of a discrete variable, and scatter plots are particularly useful for comparing the distribution of two continuous variables.\n\n5.3.2.1 Visualising Multiple Discrete Variables\nBar plots are an effective way to visualize the observed relationship (or association, at least) between a discrete explanatory variable and a discrete outcome (whether binary, ordinal, or categorical). We can use the countplot() function to create bar plots, and the hue argument to split the bars by a particular variable and display them in different colours.\n\n# heart disease by sex\nsns.countplot(data=df, x='heart_disease', hue='sex')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# heart disease by resting ecg\nsns.countplot(data=df, x='heart_disease', hue='resting_ecg')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# angina\nsns.countplot(data=df, x='heart_disease', hue='angina')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# fasting bs\nsns.countplot(data=df, x='heart_disease', hue='fasting_bs')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.3.2.2 Visualising A Continuous Variable Across Discrete Groups\nHistograms and box plots are useful for comparing the distribution of a continuous variable across the groups of a discrete variable.\n\n5.3.2.2.1 Histogram Plots\nWe can use the histplot() function to create a histogram of a continuous variable. The hue argument allows us to split the histogram by a particular variable and display them in different colours, while the multiple argument allows us to specify how the histograms should be displayed. The multiple argument can be set to stack to stack the histograms on top of each other, or dodge to display the histograms side-by-side.\n\n# age distribution by heart disease\nsns.histplot(data=df, x='age', hue='heart_disease', binwidth=5, multiple='dodge')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# cholesterol\nsns.histplot(data=df, x='cholesterol', hue='heart_disease', binwidth=25, multiple='dodge')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# filter zero values\nsns.histplot(\n    data=df.loc[df.cholesterol!=0],\n    x='cholesterol',\n    hue='heart_disease',\n    binwidth=25,\n    multiple='dodge')\n\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nThe fact that there is a significantly larger proportion of positive heart disease cases in the zero cholesterol values further demonstrates the need to address this data quality issue.\n\n\n5.3.2.2.2 Box Plots\nBox plots visualize the characteristics of a continuous distribution over discrete groups. We can use the boxplot() function to create box plots, and the hue argument to split the box plots by a particular variable and display them in different colours.\nHowever, while box plots can be very useful, they are not always the most effective way of visualising this information, as explained boxplots by Cedric Scherer. This guide uses box plots for the sake of simplicity, but it is worth considering other options when visualising distributions.\n\n# age & heart disease\nsns.boxplot(data=df, x='heart_disease', y='age')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# age & heart disease, split by sex\n# fig, ax = plt.subplots(figsize=(10,6))\nsns.boxplot(data=df, x='heart_disease', y='age', hue='sex')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# max hr & heart disease\nsns.boxplot(data=df, x='heart_disease', y='max_hr')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# max hr & heart disease, split by sex\n# fig, ax = plt.subplots(figsize=(10,6))\nsns.boxplot(data=df, x='heart_disease', y='max_hr', hue='sex')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.3.2.3 Visualising Multiple Discrete Variables\nScatter plots are an effective way to visualize how two continuous variables vary together. We can use the scatterplot() function to create scatter plots, and the hue argument to split the scatter plots by a particular variable and display them in different colours.\n\n# age & resting bp\nsns.scatterplot(data=df, x='age', y='resting_bp')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# age & resting bp\nsns.scatterplot(data=df.loc[df.resting_bp!=0], x='age', y='resting_bp')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=df.loc[df.cholesterol!=0], x='age', y='cholesterol')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=df, x='age', y='max_hr')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nThe scatter plot visualising age and resting blood pressure highlights another observation that needs to be removed due to data quality issues.\nIf there appears to be an association between the two continuous variables that you have plotted, as is the case with age and maximum heart rate in the above plot, you can also add a regression line to visualize the strength of that association. The regplot() function can be used to add a regression line to a scatter plot. The ci argument specifies whether or not to display the confidence interval of the regression line.\n\n# age & max hr\nsns.regplot(data=df, x='age', y='max_hr', ci=None)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nYou can also include discrete variables by assigning the discrete groups different colours in the scatter plot, and if you add regression lines to these plots, separate regression lines will be fit to the discrete groups. This can be useful for visualising how the association between the two continuous variables varies across the discrete groups.\nThe lmplot() function can be used to create scatter plots with regression lines, and the hue argument can be used to split the scatter plots by a particular variable and display them in different colours.\n\n# age & resting bp, split by heart disease\nsns.scatterplot(data=df.loc[df.resting_bp!=0], x='age', y='resting_bp', hue='heart_disease')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# age & cholesterol, split by heart disease (with regression line)\nsns.lmplot(\n    data=df.loc[df.cholesterol!=0],\n    x='age', y='cholesterol',\n    hue='heart_disease',\n    ci=None,\n    height = 7,\n    aspect=1.3)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# age & max hr, split by heart disease (with regression line)\nsns.lmplot(\n    data=df,\n    x='age', y='max_hr',\n    hue='heart_disease',\n    ci=None,\n    height = 7,\n    aspect=1.3)\n\nplt.show()"
  },
  {
    "objectID": "eda_py.html#next-steps",
    "href": "eda_py.html#next-steps",
    "title": "5  Exploratory Data Analysis",
    "section": "5.4 Next Steps",
    "text": "5.4 Next Steps\nThere are many more visualisation techniques that you can use to explore your data. You can find plenty of inspiration for different approaches to visualising data in the seaborn and matplotlib documentation. There are also a number of other Python libraries that can be used to create visualisations, including plotly, bokeh, and altair.\nThe next step in the data science process is to build a model to either explain or predict the outcome variable, heart disease. The exploratory work done here can help inform decisions about the choice of the model, and the choice of the variables that will be used to build the model. It will also help clean up the data, particularly the zero values in the cholesterol and resting blood pressure variables, to ensure that the model is built on the best possible data."
  },
  {
    "objectID": "eda_py.html#resources",
    "href": "eda_py.html#resources",
    "title": "5  Exploratory Data Analysis",
    "section": "5.5 Resources",
    "text": "5.5 Resources\nThere are a wealth of resources available to help you learn more about data visualisation, and while the resources for producing visualisations in R are more extensive, there are still a number of good resources for producing visualisations in Python.\n\nScientific Visualization: Python + Matplotlib [PDF][GitHub]\nSeaborn Tutorials\nMatplotlib Cheatsheets\n\nWhile the following resources are R-based, they are still useful for learning about data visualisation principles:\n\nData Visualization: A Practical Introduction\nFundamentals of Data Visualization"
  },
  {
    "objectID": "hypothesis_testing_py.html#the-motivation-for-hypothesis-testing",
    "href": "hypothesis_testing_py.html#the-motivation-for-hypothesis-testing",
    "title": "6  Hypothesis Testing",
    "section": "6.1 The Motivation for Hypothesis Testing",
    "text": "6.1 The Motivation for Hypothesis Testing\nA hypothesis is a statement about an effect that we expect to observe in our sample data, that is intended to generalise to the population. We do not know what the true effect in the population is, but we have a theory about what it might be, and we can test that theory using a hypothesis about what we will observe in the sample.\nOur hypothesis might relate to how the mean value of a certain quantity of interest (like height) differs from one group to another, or how we expect the value of one quantity in our sample to change as the value of another quantity changes (like the effect of weight on height). A hypothesis test is a statistical method for empirically evaluating a hypothesis, to determine whether what we observe in our sample data is likely to exist in the population.\nHypothesis testing is effectively a statistical method for testing the compatibility of the observed data against a relevant hypothesised effect, often the hypothesis of zero effect. The measure of that incompatibility is a test statistic, which quantifies the distance between the observed data and the expectations of the statistical test, given all the assumptions of the test (including the hypothesised effect size). For example, if testing the hypothesis that there is no effect, a distribution under the assumptions of the statistical test, including the null hypothesis, is generated, and the test statistic measures the distance between the observed data and the null distribution. A p-value is then calculated from this test statistic, representing the probability of a test statistic at least as extreme as the computed test statistic if all the assumptions of the statistical test are true, including the hypothesis that is being tested.\nThe most common approach to testing hypotheses is the Null Hypothesis Significance Testing (NHST) framework, which involves testing the hypothesis that there is no effect (the null hypothesis) and, where it is possible to reject the null, this can be used as evidence that something is going on, to which this may help present evidence for the theory developed by the researcher (the alternative hypothesis).\nHypothesis testing can be valuable for a variety of different use-cases, but they are not without their issues. Despite those issues, they remain a useful tool when you want to know whether the difference between two groups is real, or whether it’s a product of noise, or when you are interested in knowing whether an effect size is different from zero. When you have a need to answer questions with greater precision, I would argue that there is a need to go further than hypothesis testing, but in industry, where speed and simplicity are often of equal importance to precision2, hypothesis testing can serve real value.\n\n\n\n\n\n\nFurther Details\n\n\n\n\n\nThe goal of NHST is to “adjudicate between two complementary hypotheses” (Blackwell 2023), the null hypothesis (\\(H_0\\)) and the alternative/research hypothesis (\\(H_1\\)). The null hypothesis, which is the hypothesis that no effect exists in the population, is our starting assumption, and we require strong evidence to reject it. If we are able to find strong enough evidence to reject the null, this moves us one step closer to making a case for our alternative hypothesis.\nThe reasoning for using two complementary hypotheses is based on Popper (1935)’s The Logic of Scientific Discovery. Popper argues that it is impossible to confirm a hypothesis, because hypotheses are a form of inductive reasoning, which involves drawing conclusions about a general principle based on specific observations. We cannot confirm a hypothesis because we cannot prove that it is true by generalising from specific instances. However, we can falsify a hypothesis. If what we observe in our sample provides strong evidence that our hypothesis cannot be true, then we can conclude (with a degree of certainty) that the hypothesis is false For a better understanding about the null and alternative hypothesis, I would recommend Josh Starmer’s StatQuest videos, embedded below:\n\n\nWhen we carry out a hypothesis test, we are quantifying our confidence that what we observe in the sample did not occur by chance. If we are able to show that a effect is extremely unlikely to have occurred by chance, we can reject the null hypothesis. Rejecting the null doesn’t demonstrate that the observed effect can be explained by our theory, however, demonstrating that the observed data does not conform to our expectations given that all of our test’s assumptions, including the null hypothesis, are true, moves us one step closer to supporting our theory."
  },
  {
    "objectID": "hypothesis_testing_py.html#the-problems-with-hypothesis-testing",
    "href": "hypothesis_testing_py.html#the-problems-with-hypothesis-testing",
    "title": "6  Hypothesis Testing",
    "section": "6.2 The Problems With Hypothesis Testing",
    "text": "6.2 The Problems With Hypothesis Testing\nHypothesis testing serves an important purpose in science, both in academia and in industry. We use hypothesis testing to sift through noise in data to dry and draw conclusions, and this is a perfectly reasonable goal.\nHowever, the dominant framework for testing hypotheses, Null Hypothesis Significance Testing, is rife with issues. The primary problems with NHST are that it defines tests in terms of them being statistically significant or not, based on an arbitrary threshold value for p-values, and it frames p-values as the primary concern when interpreting a statistical model. While p-values are still relevant, they are not the whole story (nor the main part of the story).\nFurther, the importance afforded to p-values in the NHST framework encourages certain common misconceptions about them, and the framing that prioritises them also makes those misconceptions even more harmful. P-values are not the measure of the substantive importance of the effect; they are not the probability that the null hypothesis is true; and finally, a statistically significant p-value does not confirm the alternative hypothesis, just as a non-significant p-values does not confirm the null. They are a measure of how surprising the observed data would be if the hypothesis being tested (like the null hypothesis), and all other assumptions of the statistical test, are true.\nAlthough these problems are of real significance in academia, where precision is of the greatest importance,\n\n\n\n\n\n\nFurther Details\n\n\n\n\n\nThere are many complaints and criticisms that can be levelled at hypothesis testing. Here are some of the main ones.\n\nP-Values & Statistical Significance\nThe biggest problems with NHST relate to the misuse and misunderstanding of p-values, and their dichotomisation in order to make arbitrary judgements about statistical significance. The extent of the issues surrounding p-values and statistical significance are so extensive that it led the American Statistical Association to make an official statement addressing the misconceptions and clarifying the correct usage of p-values (Wasserstein and Lazar 2016). The statement makes the point that p-values do not measure the probability that a hypothesis is true, or that the data was produced by random chance alone; nor should they be interpreted as measuring the size of an effect or the importance of a result; and that by themselves p-values are not a good measure of the evidence for a model or a hypothesis (Wasserstein and Lazar 2016). Unfortunately, these are all very common misconceptions that find their way into scientific research. The misconceptions go much further than this, however, as illustrated by Greenland et al. (2016). Greenland et al. (2016) build on the wider discussion to include statistical tests, laying out a total of 25 different misconceptions about statistical testing, p-values (and confidence intervals) and statistical power3. They make the case that the biggest culprit is the arbitrary use of thresholds for defining statistical significance, which frame p-values in problematic terms, reduce a need for critical thinking about findings, and are far too easily abused.\nThis discussion has even permeated some of the media (Aschwanden 2015, 2016). FiveThirtyEight have written about this issue in some detail, and their discussion of the topic is perhaps more accessible than the more detailed discussions going on. Aschwanden (2015) is a particularly good summary of the discussion around p-values, and does a good job highlighting that the issues around p-values and the “replication crisis” are not evidence that science can’t be trusted, but rather that this stuff is extremely difficult, and that it takes a community of scientists working towards the truth, to try and achieve results.\n\n\nGarden of Forking Paths\nGelman and Loken (2021) add to the discussion by highlighting the role that the “garden of forking paths” approach to analysis, where there are many possible explanations for a statistically significant result, plays in the statistical crisis in science. It’s important to recognise how a scientific hypothesis can correspond to multiple statistical hypotheses (Gelman and Loken 2021), and to guard against this possibility. This effectively creates a “multiple comparisons” situation, where an analysis can uncover statistically significant results that can be used to support a theory by multiple “forking paths”. A “one-to-many mapping from scientific to statistical hypotheses” can create a situation that is equivalent to p-hacking but without the necessary intent (Gelman and Loken 2021).\nOne of the central questions that Gelman and Loken (2021) ask, is whether the same data-analysis decisions would have been made with a different data set. The garden of forking paths is driven by the fact that we devise scientific hypotheses using our common sense and domain knowledge, given the data we have, and we do not consider the myriad implicit choices that this involves. By not considering these implicit choices, we fail to account for the garden of forking paths.\n\n\nComparisons Are Not Enough\nI think that the biggest issue with p-values themselves is that we have placed far too much value in them. We use methods that are designed to compute p-values, we define “success” as the discovery of a statistically significant finding, and we give little thought to the design choices made in the process of carrying out these tests. This is partly driven by the frequency of misconceptions and misunderstandings of p-values, but it’s also a result of the methods we use.\nIf we are more precise about how we think about p-values, this should help us avoid making the mistake of thinking they are proving a hypothesis or that they are sufficient evidence for a theory, but if we are more precise about what a p-value really represents, this raises significant questions about the methods we use. What purpose does a test for “statistical significance” serve if we acknowledge that p-values are not enough? Gelman (2016) argue that p-values are not the real problem, because the real problem is null hypothesis significance testing, which they refer to as a “parody of falsificationism in which straw-man null hypothesis A is rejected and this is taken as evidence in favour of preferred alternative B.” The null hypothesis significance testing framework places far too much weight on the shoulders of p-values, and even if we are doing a better job of hypothesis testing (being particularly careful not to make mistakes about how we use p-values), what is it really worth? When we are treating p-values as a part of the wider toolkit, hypothesis tests become less useful."
  },
  {
    "objectID": "hypothesis_testing_py.html#avoiding-the-pitfalls",
    "href": "hypothesis_testing_py.html#avoiding-the-pitfalls",
    "title": "6  Hypothesis Testing",
    "section": "6.3 Avoiding the Pitfalls",
    "text": "6.3 Avoiding the Pitfalls\nThere has been plenty of debate among statisticians about the issues around hypothesis testing and p-values, and while there is no detail too minor for two academics to throw hands over, there is a general consensus that our approach to p-values is a problem. What has not been settled, however, is what the right approach is. That makes this section of this chapter a little trickier. However, I will attempt to give some sensible advice about how to avoid the greatest dangers pose by hypothesis testing and the use of p-values more broadly.\nA more general approach to hypothesis testing would frame things in terms of a test hypothesis, which is a hypothesised effect (including but not limited to zero effect – the null hypothesis) that is assumed to be true, and a statistical test is carried out that considers how well the observed date conforms to our expectations given that the test’s assumptions, including the test hypothesis, are true.\nPerhaps unsurprisingly, the primary way to improve how we do hypothesis testing is to approach p-values differently. The simplest solution is not to use statistical significance at all. This is an arbitrary threshold that gives us very little good information, and allows researchers to avoid thinking critically about their findings. Instead, we should report the p-value itself, and be clear about what it is that p-values actually tell us!\nBeyond framing hypothesis tests in more appropriate terms, a big part of the improvements that can be made is about transparency. Being transparent about results, about what those results mean, and about what they don’t mean, is important. Further, it is important to report all findings, and not pick and choose what we found based on which results support our theory (this becomes easier if we’re not in search of statistical significance).\nUltimately, hypothesis testing requires a careful, critical approach, and this applies to the development of the hypothesis, the design of the test, and the analysis of the findings.\n\n\n\n\n\n\nFurther Details\n\n\n\n\n\n\nRethink P-Values\nThe p-value is not a magic number. It does not give the probability that your research hypothesis is true, nor does it give the probability that the observed association is a product of random chance (Wasserstein, Schirm, and Lazar 2019). One of the biggest issues I have with p-values is that they feel, given the way we use them in NHST, like they should be the probability that our hypothesis is true. However, it is important to never let those urges win - the p-value is not a hypothesis probability.\nInstead, p-values are the degree to which the observed data are consistent with the pattern predicted by the research hypothesis, given the statistical test’s assumptions are not violated (Greenland et al. 2016)4.\nLakens (2022) recommends thinking about p-values as a statement about the probability of data, rather than a statement about the probability of a given hypothesis or theory. I think remembering this framing is a good way to avoid misconstruing exactly what p-values represent. Similarly, we can think about p-values as our confidence in what it is we observe in our data. How confident can we be that the data we observed is the product of random chance (and the assumptions that underpin our statistical model)? A p-value is an approximation of this.\nThe point is that none of these definitions (or more precisely, intuitive explanations) of p-values are claiming that p-values are the probability that the research hypothesis is true. They say nothing about a hypothesis, because they are not a hypothesis probability.\nFraming p-values appropriately is one part of the necessary rethink about p-values. The other is the need to place less weight in what p-values tell us. Having acknowledged all the things that p-values are not, this part should be easy to understand. Given that p-values are not a hypothesis probability, and they are really a statement about our data, they are far from sufficient (though I would argue they are at least necessary) for our understanding of our data. Instead, they are part of a wider toolkit. A toolkit that contains even more important tools, like effect sizes!\n\n\nStop Using Statistical Significance\nIn addition to the common misuse of p-values, the null hypothesis significance testing framework also defines an arbitrary threshold for what is considered a sufficiently “meaningful” result - statistical significance.\nThe commonly defined significance threshold (or the alpha (\\(\\alpha\\)) level) is a p-value of 0.05. A p-value of less than 0.05 is deemed statistically significant, while a p-value greater than 0.05 is insignificant. So two p-values, one that equals 0.499 and another that equals 0.501 are both deemed to offer something very different to each other. One is treated as a sign of success, and the other is filed away in the desk drawer, never to be seen again! The arbitrariness of this threshold has been best highlighted by Gelman and Stern (2006) when they demonstrated the fact that the difference between statistically significant and insignificant is not itself statistically significant.\nDefining whether a research finding is of real value based on such a meaningless threshold is clearly one of the contributing factors in the problems with hypothesis testing, and the simple solution is to just stop treating p-values as either statistically significant or not!\nIf you need further evidence of the problems with how we have been using statistical significance, look no further than the man credited with first developing the method, Ronald A Fisher. Fisher (1956) claimed that no researcher has a fixed level of significance that they are using to reject all hypotheses, regardless of their context. Ronald, I have some terrible, terrible news.\nWhere possible, do not use terms like statistical significance, and do not use a threshold (the \\(\\alpha\\) level) for significance. Report p-values instead. P-values should be “viewed as a continuous measure of the compatibility between the data and the entire model used to compute it, ranging from 0 for complete incompatibility to 1 for perfect compatibility, and in this sense may be viewed as measuring the fit of the model to the data.” (Greenland et al. 2016, 339)\n\n\nReport All Findings\nThis suggestion ties in closely with the previous two. If p-values are treated like a continuous measure of compatibility, and we are no longer using thresholds to define whether a result is good or bad, this should reduce the urge to report findings selectively, based only on those that say the right thing.\nDo not carry out multiple tests and only show the significant results. Be open and transparent about your process and what was tested. Only reporting significant results is problematic for many reasons. There is no reason to favour statistical significant results. Instead, we should either report all our findings, as this is the most transparent approach, or we should select the substantively meaningful results, taking great care to avoid only reporting those that confirm our theory5.\nI think this principle is easier to follow in academia, where extensive reporting of findings is encouraged. I don’t think there is a downside to open and transparent reporting in academia (ignoring the potential risks posed to publication). However, in industry, there is often a need to be as clear and concise as possible. Reporting all results could potentially cause confusion, and make statistical findings much harder for non-technical audiences to understand. Therefore I think the advice for industry should be that reporting results should not be defined by the statistical significance of the results. We should report the results that matter, that contribute the most to our understanding of a quantity of interest, and that help others understand the findings. Communication of findings in industry should focus on communicating the key results from the analysis, and that should not be defined by significance.\n\n\nBuild Scientific Models\nFinally, it is important to think carefully about the implications of any hypothesis, and to understand that there is a difference between a scientific hypothesis and a statistical hypothesis6. Statistical models do not, on their own, allow scientific hypotheses\nThere are often many different statistical hypotheses that would be consistent with a particular scientific hypothesis. This is well explained and multiple examples are given by Gelman and Loken (2021).\nThe garden of forking paths may be a particularly big issue in hypothesis testing, but it doesn’t go away if we scrap NHST. It is important to think about the ways that any scientific hypothesis can be explained by multiple statistical hypotheses, and it’s also very important to think about the many choices made when building any statistical model, and what this might mean for our work. There are so many different assumptions that go into the process of building a statistical model, and when we fail to consider the breadth of assumptions that underpin our work, we can easily invalidate our findings. As discussed earlier in this chapter, we have to remember that p-values are assuming that all of our assumptions are true, and if any of them are not true, we are in hot water.\nWhat is needed is to build coherent, logically-specified scientific models from which we design our statistical analysis. McElreath (2023) makes this case, arguing that we need to build theory-driven scientific models that describe the causal relationship we are modelling before we build statistical models that attempt to test our scientific models.\nBuilding scientific models doesn’t necessarily address all concerns around the “garden of forking paths” issue, but by doing so we are forced to be more thoughtful about our research, and we are more able to identify problems with our analysis."
  },
  {
    "objectID": "hypothesis_testing_py.html#building-statistical-tests",
    "href": "hypothesis_testing_py.html#building-statistical-tests",
    "title": "6  Hypothesis Testing",
    "section": "6.4 Building Statistical Tests",
    "text": "6.4 Building Statistical Tests\nA generalised framework for hypothesis testing, that discards of NHST’s problematic baggage (instead following a Fisherian approach (Fisher 1956)), would take the following steps:\n\nSpecify the test hypothesis (\\(H_0\\)) that assumes some effect size against which the observed effect size will be compared - this will often be the assumption of zero effect (the null hypothesis), but it can be anything7.\nGenerate the test distribution, defined by the data being analysed and the test being carried out, which represents our expectation of what we would observe if the statistical model’s assumptions, which include the test hypothesis, are all true.\nCompute the test statistic, which quantifies how extreme the observed data is, given the test distribution.\nCompute the p-value, representing the probability of observing a test statistic as large or larger than the observed test statistic if all of the assumptions of the statistical model, including the test hypothesis, are true.\n\nThe important takeaway is that the test statistic is a measure of the sample data, the test distribution is a measure of what we would expect to observe if the test hypothesis, and all other model assumptions, are true, and the statistical test is simply quantifying how compatible the observed data would be with the generated distribution.\nWhile there are many different statistical tests that are designed to test different data distributions, using different assumptions, the general framework is the same. Below are some examples of statistical tests that are commonly used.\n\n6.4.1 T-Tests\nThe general idea of a t-test is to calculate the statistical significance of the difference between two groups. What the groups represent depends on the type of t-test being carried out, whether it be comparing a single group of observed data against a null distribution that assumes the data was generated by chance, or comparing the difference between two observed groups against a null distribution that assumes the difference is zero.\nThere are several broad groups of t-tests:\n\nOne-sample t-test - comparing the sample mean of a single group against a “known mean”, generally testing whether a sample is likely to be generated by chance.\nPaired t-test - comparing means from the same group measured multiple times, like how someone responds to a survey at different points in time.\nTwo samples t-test - comparing the means of two groups, such as measuring the difference in blood glucose levels of two different groups.\n\nThere are several variations within these groups, for example a paired t-test generally assumes equal variance between the two groups, but this is often not true, so an unequal variance t-test can be used.\nFinally, when deciding on the nature of a t-test, you must also decide if the test should be one-tailed or two-tailed. A one-tailed t-test is used to test a hypothesis that includes a directional component (such as \\(x &gt; y\\)), while a two-tailed t-test is just testing whether there is a difference between two groups. A one-tailed t-test is appropriate when you believe the difference between two groups should be positive or negative, but if the only belief is that the groups are different, a two-tailed t-test is appropriate.\nA two-tailed t-test compares whether the sample mean(s) is different, whether larger or smaller, while a one-tailed t-test assumes the direction of the difference is known. For example, if it is known that Group A is either the same or bigger than Group B (or that Group A being smaller is not of interest), then a one-tailed t-test would be appropriate.\nI will simulate data from real-world examples to demonstrate one-sample, paired, and two-sample t-tests, however, these examples are not exhaustive, and there are many variations on t-test implementations.8\n\n6.4.1.1 One-Sample T-Test\nWe can carry out a one-sample t-test using IQ scores as our example, because there is plenty of data out there about IQ scores that we can base our simulations on. The average IQ score is said to be around 100, and scores are normally distributed with a standard deviation of 15, meaning that the majority of people have an IQ in the range of 85-115.\nIn a one-sample t-test we are interested in testing a sample mean against an “expected” mean. For example, perhaps we have a group of individuals that are viewed as “high achievers”, and we want to use IQ scores to test whether this group is meaningfully different to the rest of the population (despite being aware that IQ is not a particularly good test of either an individual’s intelligence or their potential).\nA one-sample t-test would allow us to do this because we are able to test the mean IQ score for our high achievers against the expected mean value if there was no real difference between them and the population, which would just be the mean IQ score in the population (100).\nLets first start by simulating our high achievers’ IQ scores, assuming that their mean score is 105 (perhaps some of the group studied IQ tests extensively and are now able to ace the test), with a standard deviation of 15.\n\n# mean and standard deviation\nmean, sd = 105, 15 \n\n# draw sample from normal distribution\nhigh_achievers = np.random.normal(mean, sd, 10)\n\nWe can take a look at the sample data we have simulated, to see how close it is to the true mean and standard deviation (because we are simulating the data it won’t be a perfect match).\n\n# compute summary statistics for our sample\nnp.mean(high_achievers)\nnp.std(high_achievers)\n\n11.27963405536393\n\n\nAnd we can visualise the data to get a better sense of what the group looks like.\n\nfig, ax = plt.subplots()\n\nsns.histplot(x=high_achievers, binwidth=5)\n\n# set plot labels and title\nplt.xlabel('IQ Score')\nplt.ylabel('')\n\nsns.despine()\nplt.show()\n\n\n\n\nIt is not clear what the underlying distribution of our sample data is here. We already know that the data was drawn from a normal distribution, but if we did not, we would need more observations to confirm this.\n\npg.ttest(high_achievers, y = 100, alternative = \"greater\")\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n1.568435\n9\ngreater\n0.075612\n[99.0, inf]\n0.495983\n1.576\n0.422705\n\n\n\n\n\n\n\nThe p-value of our one-sample t-test is 0.105. This means that we would expect to observe a sample mean at least as extreme as our observed mean (106), if the null that there is no difference between the high achiever group’s mean IQ score and the population mean IQ score is true, a little more than 10% of the time, or 1 in 10 times. That’s quite high, so I wouldn’t be confident that we are observing a meaningful difference.\nHowever, we know that there is a meaningful difference between our high achievers and the population, because we specified a difference when we simulated our sample data. If we were to conclude that the results we observe in our t-test do not lend any support to our hypothesis that the high achievers are better at IQ tests than the average person, this would be a false negative (or sometimes unhelpfully referred to as a Type 1 Error).\nIf we increase our sample size, maybe we will have more luck!\n\nlarger_sample = np.random.normal(mean, sd, 30)\n\npg.ttest(larger_sample, y = 100, alternative = \"greater\")\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n2.865017\n29\ngreater\n0.003839\n[103.31, inf]\n0.523078\n11.272\n0.875403\n\n\n\n\n\n\n\nIncreasing the sample size from 10 to 30 has decreased the probability of observing a test statistic at least as extreme as our sample mean if our data was generated by chance (and the assumptions of our statistical model are all met) from around 10% to just over 1.5%. That seems a lot more reasonable, and at this point I’d be more comfortable concluding that the high achiever’s IQ scores are meaningfully different to the average score in the population.\nOf course, our p-value doesn’t tell us anything about the substantive importance of the difference in IQ scores. We might be willing to conclude that there is a difference, but does it matter? Well, the answer is no because it’s IQ scores and IQ scores don’t matter.\nBut what if, for some truly baffling reason, we had reason to believe IQ scores are not nonsense? In that case, how do we know if the difference between the mean value in the high achievers group and the population is large enough to be of scientific and substantive relevance? This is not a question that significance tests can answer, and is instead something that our domain expertise should answer.\n\n\n6.4.1.2 Paired T-Test\nWhen respondents in surveys are asked to self-report certain details about themselves, this can invite bias. Take, for example, the difference in self-reported and interviewer-measured height among men in the UK. Although there are many proud short kings out there that will confidently declare their height, safe in the knowledge that good things come in small packages, there are plenty of men that wish they were a little bit taller (someone should write a song about that), and for whom a self-reported survey is an opportunity to dream big (literally). Any time that respondents are given the opportunity to report details about themselves that have any social baggage attached, there is a possibility that the responses will be subject to social desirability bias.\nIn the case of self-reported and measured male heights, the differences are not particularly large in absolute terms. It’s very possible that the difference is actually a product of some random variation, or measurement error. We can test this!\nUsing the mean values for reported and measured heights in 2016, calculating the standard deviation from the reported standard error9, and calculating the correlation between the two variables using the value for each variable from 2011-201610, we can simulate our sample data.\n\nmean, cov, n = [177.23, 175.71], [[1, .83], [.83,  1]], 100\n\nself_reported, measured = np.random.multivariate_normal(mean, cov, n).T\n\nWe have simulated the self-reported and measured height for 100 men, given the mean, standard deviation, and correlation as detailed in Table 1 of the above analysis, and carried out a paired t-test that compares the difference between the two simulated groups.\n\npg.ttest(self_reported, measured, alternative=\"greater\")\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n11.5537\n198\ngreater\n3.089721e-24\n[1.32, inf]\n1.63394\n9.509e+20\n1.0\n\n\n\n\n\n\n\nThe difference between the self-reported and measured heights has a p-value of 0.00719, suggesting there is a 0.7% probability of observing a t-statistic at least as extreme as we observe here, if the data was generated by chance and if the assumptions of our statistical model are valid.\nThe estimated difference between self-reported height and measured height is 1.4cm, with about 1cm difference from our estimate and our lower and upper confidence intervals, which suggests that the difference between self-reported height for men is larger than their measured heights. However, is 1.4cm significant enough to really care? I mean, none of this is important enough (or valid enough) to really care, but if we suspend disbelief for a moment, is a 1.4cm difference substantively meaningful? I’ll leave that up to you to decide.\n\n\n6.4.1.3 Two Samples T-Test\nSo we’ve established a very real, totally scientific over-reporting of male height across the UK, and some clever, entrepreneurial, and not at all exploitative pharmaceutical company has had the good sense to develop a medication that gives you an extra couple of inches in height.\nIn order to prove that it works it needs to go through testing (because we are particularly reckless, we don’t really care if it is safe in this example). An experiment is designed, with a treatment and control group that is representative of the wider male population, and the wonder drug is given to the treatment group.\nWe can compare the change in height for the two groups in the 6 months after the treatment group have taken the medication, hypothesising that the drug has had a positive effect on the treatment group, helping them grow by a whopping 1.5cm! This is compared against the average increase in height for the control group that is approximately zero, with a very small standard deviation. A two-sample t-test can help us shed some light on whether we can be confident that this difference did not occur by chance.\nWe simulate our treatment and control groups, with 20 observations in each, with the treatment group having a mean growth of 3cm and standard deviation of 3cm, and the control group having a mean growth of 0cm and a standard deviation of 0.5cm.\n\n# create an empty list for the simulated data\ndata = []\n\n# for loop to simulate 300 not bald observations\nfor i in range(300):\n  hair_status = \"Not Bald\"\n  education = np.random.choice(\n    [\"Basic\", \"Secondary\", \"Tertiary\"],\n    p=[0.039, 0.594, 0.367]\n    )\n\n  # collect all simulated variables into a dictionary\n  not_bald = { \"hair_status\": hair_status, \"education\": education }\n\n  # append dictionary to data\n  data.append(not_bald)\n\n# create pandas dataframe from simulated data\nbaldness_survey = pd.DataFrame(data)\n\n# draw samples from normal distribution\ntreatment, control = np.random.normal(3, .5, 20), np.random.normal(0, .5, 20)\n\nHaving simulated our treatment and control groups, we can compute a t-test to see if there is a difference in how much each group has grown, on average.\n\npg.ttest(x=treatment, y=control, paired=False, alternative='greater')\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n19.437624\n38\ngreater\n1.214921e-21\n[2.66, inf]\n6.146716\n1.766e+18\n1.0\n\n\n\n\n\n\n\nThe probability that we would observe a test statistic as large or larger than observed here is very, very small, and the results of the t-test estimate that the difference between the two groups is a little bit less than 4cm, with a lower confidence interval of just under 3cm, suggesting that this medication is having a positive difference on height in the treatment group!\n\n\n\n6.4.2 Chi-Squared Tests\nAlthough there are many ways that you may encounter chi-squared (\\(\\chi^2\\)) tests, the two most common are tests of independence and a goodness of fit tests. The chi-squared test of independence is a test of the association between two categorical variables, meaning whether the variables are related or not. It is testing whether they vary independently of each other. A chi-squared goodness of fit test is used to examine how well a theoretical distribution fits the distribution of a categorical variable. We will focus on the test of independence here, because I think this is the most common use case.\nA common use-case for chi-squared tests of independence is survey data, where individuals from different groups respond to a survey question that offers a finite number of discrete answers, like a scale from good to bad. A good example of categorical variables used in survey data is the impact of male baldness on 46 year old men, carried out by Sinikumpu et al. (2021). In this survey they grouped men by levels of baldness, and asked them various questions about their lives, from education to sex-life, to consider the impact that being bald has on men’s lives and well-being.\nWe will use the education question as the basis for simulating the data for our chi-squared test. The survey splits education-level in to three groups - basic, secondary, and tertiary - while I have simplified the question by turning the levels of baldness into a binary categorical variable.\n\n# create an empty list for the simulated data\ndata = []\n\n# for loop to simulate 300 not bald observations\nfor i in range(300):\n  hair_status = \"Not Bald\"\n  education = np.random.choice(\n    [\"Basic\", \"Secondary\", \"Tertiary\"],\n    p=[0.039, 0.594, 0.367]\n    )\n\n  # collect all simulated variables into a dictionary\n  not_bald = { \"hair_status\": hair_status, \"education\": education }\n\n  # append dictionary to data\n  data.append(not_bald)\n\n# for loop to simulate 600 bald observations\nfor i in range(600):\n  hair_status = \"Bald\"\n  education = np.random.choice(\n    [\"Basic\", \"Secondary\", \"Tertiary\"],\n    p=[0.03, 0.603, 0.367]\n    )\n\n  # collect all simulated variables into a dictionary\n  bald = { \"hair_status\": hair_status, \"education\": education }\n\n  # append dictionary to data\n  data.append(bald)\n\n# create pandas dataframe from simulated data\nbaldness_survey = pd.DataFrame(data)\n\nWe have simulated each of the different baldness categories individually, before combining them into a single dataset. I think there should be a more concise way of doing this, but I haven’t wrapped my head round how I should do that yet.\nWe can visualise the difference between the three education-levels in terms of proportion of baldness, to see if there is an obvious difference.\n\nfig, ax = plt.subplots()\n\nsns.histplot(\n  data = baldness_survey, \n  x = 'education', hue = 'hair_status',\n  multiple='fill'\n  )\n  \n# set plot labels and title\nplt.xlabel('Education Level')\nplt.ylabel('Proportion')\n\n\nsns.despine()\nplt.show()\n\n\n\n\nThere’s a small difference between people with a basic education, but for secondary and tertiary education it seems to be pretty evenly split. Our chi-squared test can test this empirically.\n\nexpected, observed, stats = pg.chi2_independence(baldness_survey, x='education', y='hair_status')\n\nstats.round(3)\n\n\n\n\n\n\n\n\ntest\nlambda\nchi2\ndof\npval\ncramer\npower\n\n\n\n\n0\npearson\n1.000\n5.075\n2.0\n0.079\n0.075\n0.510\n\n\n1\ncressie-read\n0.667\n5.028\n2.0\n0.081\n0.075\n0.506\n\n\n2\nlog-likelihood\n0.000\n4.950\n2.0\n0.084\n0.074\n0.499\n\n\n3\nfreeman-tukey\n-0.500\n4.904\n2.0\n0.086\n0.074\n0.496\n\n\n4\nmod-log-likelihood\n-1.000\n4.867\n2.0\n0.088\n0.074\n0.492\n\n\n5\nneyman\n-2.000\n4.821\n2.0\n0.090\n0.073\n0.488\n\n\n\n\n\n\n\nSo our p-value tells us that we should expect to observe data at least extreme as our simulated survey data a little less than 20% of the time, if the null hypothesis that there is no difference in education levels across levels of hair loss is true. This is a smaller p-value than Sinikumpu et al. (2021) observe (0.299), but that will be a consequence of the fact we are simulating the data using probabilistic samples, not replicating their results exactly.\nIf we were to fudge the numbers a little bit, so as to artificially increase the difference in education levels based on the amount of hair a 46 year old man has, what would our results look like?\n\n# Step 1, empty list `data`:\ndata = []\n\n# Step 2: for-loop:\nfor i in range(300):\n  # Step 3: simulate all real-world factors:\n  hair_status = \"Not Bald\"\n  education = np.random.choice(\n    [\"Basic\", \"Secondary\", \"Tertiary\"],\n    p=[0.15, 0.55, 0.3]\n    )\n\n  # Step 4: accumulate all factors in dictionary `d`:\n  not_bald = { \"hair_status\": hair_status, \"education\": education }\n\n  # Step 5: append `d` to `data`\n  data.append(not_bald)\n\n# Step 2: for-loop:\nfor i in range(600):\n  # Step 3: simulate all real-world factors:\n  hair_status = \"Bald\"\n  education = np.random.choice(\n    [\"Basic\", \"Secondary\", \"Tertiary\"],\n    p=[0.05, 0.6, 0.35]\n    )\n\n  # Step 4: accumulate all factors in dictionary `d`:\n  bald = { \"hair_status\": hair_status, \"education\": education }\n\n  # Step 5: append `d` to `data`\n  data.append(bald)\n\n# Step 6: create the DataFrame (outside of the for-loop)\nbaldness_survey_with_effect = pd.DataFrame(data)\n\nexpected, observed, stats = pg.chi2_independence(baldness_survey_with_effect, x='education', y='hair_status')\n\nstats.round(3)\n\n\n\n\n\n\n\n\ntest\nlambda\nchi2\ndof\npval\ncramer\npower\n\n\n\n\n0\npearson\n1.000\n25.238\n2.0\n0.0\n0.167\n0.997\n\n\n1\ncressie-read\n0.667\n24.650\n2.0\n0.0\n0.165\n0.996\n\n\n2\nlog-likelihood\n0.000\n23.791\n2.0\n0.0\n0.163\n0.995\n\n\n3\nfreeman-tukey\n-0.500\n23.400\n2.0\n0.0\n0.161\n0.994\n\n\n4\nmod-log-likelihood\n-1.000\n23.212\n2.0\n0.0\n0.161\n0.994\n\n\n5\nneyman\n-2.000\n23.406\n2.0\n0.0\n0.161\n0.994\n\n\n\n\n\n\n\nHaving increased the number of respondents with a basic education (and in turn decreasing those with secondary and tertiary educations) for the not bald group, our chi-squared test results suggest the data we observe is much more extreme. The p-value for this test is extremely small, but that’s not totally surprising. We did fake our data, after all."
  },
  {
    "objectID": "hypothesis_testing_py.html#moving-beyond-testing",
    "href": "hypothesis_testing_py.html#moving-beyond-testing",
    "title": "6  Hypothesis Testing",
    "section": "6.5 Moving Beyond Testing",
    "text": "6.5 Moving Beyond Testing\nHypothesis testing is still a useful tool when we are looking to make conclusions about data without overinterpreting noise (Gelman, Hill, and Vehtari 2020). Simple statistical tests of difference between groups or differences from a hypothesised value have plenty of utility, especially in industry, where getting reasonably reliable results quickly can often be more valuable than taking more time to get more precision.\nHowever, where more precision is needed, a different approach may be necessary. Testing a test hypothesis can only give us some sense of the degree to which our data is consistent with the hypothesis. This may move us closer to being able to make inferences, especially if the testing approach is robust, and sound scientific models have been developed for which the hypothesis test is a reasonable statistical representation, but answering meaningful questions usually involves trying to make some assertions not just about the existence of an effect or a difference, but also quantifying it too. Where this is the goal, estimation is a better choice. The estimation approach to statistical inference does not rely on a hypothesis, and instead focuses on a “quantity of interest”, for which we use the data to estimate (Prytherch 2022). While testing tries to give us a sense of whether or not what we observe in our sample is likely to exist in the population, the estimation approach tries to measure the effect size itself.\nAlthough estimating an effect size is a good start, it is also important to quantify the precision of that estimate as well. Estimating the size of an effect, a point estimate, is inherently uncertain. This uncertainty can be a strength, rather than a weakness. By incorporating uncertainty in to our estimates, we can say more about what our sample data is telling us, and we are better able to make inferences about the population.\nFurther, by acknowledging the uncertainty in the statistical modelling process, not just about the model’s point estimates, the process should become a more honest representation of the quantity of interest. Greenland et al. (2016) argue that the goal of statistical analysis is to evaluate the certainty of an effect size, and acknowledging and embracing that uncertainty should include every aspect of the process, including the conclusions we draw and the way we present our findings.\n\n\n\n\n\n\nFurther Details\n\n\n\n\n\n\nEstimate Meaningful Quantities\nThis is, I believe, the most important suggestion made in this chapter. I am borrowing a phrase used by the UNC Epidemiologist, Charles Poole, in a talk he gave about the value of p-values. He states that we should “stop testing null hypotheses, start estimating meaningful quantities” (Poole 2022). His (very firmly-held) position is that there is no reforming null hypothesis significance testing. Instead, our focus should be on estimation. Poole (2022) argues that wherever significance testing is of some utility, we should instead frame the results in terms of substantive results and estimation of effect size. This would still do what hypothesis testing is attempting to do, but it should be more robust (if done properly), and the framing itself offers a lot more.\nGelman, Hill, and Vehtari (2020) argue that we are generally capable of being interested in issues where effects exist, and that the presence of an effect is therefore not entirely interesting, in and of itself. Therefore, we should concern ourselves with the things that do matter, like the size of the effect, not the existence of the effect itself.\nAlthough I’m not going to take quite as strong a position as Poole (2022) or Gelman, Hill, and Vehtari (2020), I do think his position is pretty much correct. I am of the view that hypothesis testing is generally a sub-optimal approach when trying to better understand phenomena using data. I think there are still plenty of situations where a hypothesis test is sufficient, and in industry where there is often value to providing functionally useful (if relatively imprecise) conclusions quickly, it is reasonable to turn to testing. However, there are lots of situations where an estimate of the effect size, and the precision of that estimate, are both very valuable!\nIt is rare that we should only care that the effect exists. Generally speaking, this should be a prerequisite to the more important question of what the magnitude and direction of that effect is. Even where the knowledge of the existence of an effect is sufficient, it is hard for me to imagine a situation where knowing the magnitude and direction of the effect wouldn’t be useful, and wouldn’t allow us to estimate our confidence in that effect more effectively.\nTaking an estimation approach to statistical inference doesn’t necessarily alter the statistical methods used (though more on this at the end of this chapter), but it does require our adjusting how we approach these methods and what we look to get out of them.\nEstimation is not specifically about method. If statistical tests are used, then it requires taking the estimate of the effect size, or calculating it if the context means that any output does not offer an intuitive interpretation of effect. Other methods, like regression, can give a more direct measure of effect size (though there are plenty of regression models that also require work to produce more interpretable effect estimates).\nThe important change in approach is to focus on the importance of the estimated effect size, not on the p-value, or statistical significance!\n\n\nEmbrace Uncertainty\nWhen we move away from testing null hypotheses and focus instead on the estimation of effect sizes, we go from testing for the existence of an effect to estimating the magnitude (effect size) and the direction (whether the effect is positive or negative). However, a key part of this that is missing is the precision with which we are able to estimate the effect. It’s important to remember, in any statistical modelling, that we are estimating quantities, and therefore any point estimate is just our best effort at quantifying an unknown population parameter. The point estimate is probably wrong! It is important that we acknowledge the uncertainty in our estimations and account for them in the way we present model outputs11.\nHowever, the embrace of uncertainty is not limited to our uncertainty in our estimates. It is a wider issue. We have to accept that every part of the process of estimating meaningful quantities is done with a degree of uncertainty, and as such the way we think about statistical modelling and the way we present our results should account for the existence of variance. Uncertainty exists everywhere in statistical analysis and inference (Wasserstein, Schirm, and Lazar 2019).\nWasserstein, Schirm, and Lazar (2019) describe the use of “significance tests and dichotomized p-values” as an attempt by some practitioners to avoid dealing with uncertainty by escaping to a more simplistic world where results are either statistically significant or not. Perhaps a little uncharitable (though in an academic context I am more inclined to agree), but we’ve already discussed the importance of moving away from significance testing, so we should complete the journey and confront uncertainty head-on! We have to accept that findings are more uncertain than is often acknowledged. Our embrace of uncertainty should help us to seek out better methods, apply the necessary amount of care and critical thinking to our work, and draw inferences with the appropriate amount of confidence.\nOne way of acknowledging and working with this uncertainty is the use of confidence intervals when estimating effect sizes. Confidence intervals estimate a range of values that would be compatible with the data, given all of the statistical model’s assumptions are met. While confidence intervals are ultimately estimated from the p-value, and the typical 95% confidence interval corresponds to the p &gt; .05 threshold that I have warned against, they are useful because they frame results in terms of effect sizes, and they incorporate uncertainty in our estimates, helping to quantify the precision with which we are able to estimate effect size.\nConfidence intervals are a step in the right direction, because point estimates, while they are our best shot at the quantity of interest, are highly likely to be wrong. Estimating the interval within which we would expect the quantity of interest to fall, acknowledges that uncertainty, and there’s a better chance that the population parameter falls within the estimated range than bang on the point estimate. However, it’s important that we don’t treat confidence intervals as the fix for all uncertainty! Gelman (2016) states that “the solution is not to reform p-values or to replace them with some other statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty and embracing of variation”.\nUltimately, confidence intervals are valuable when estimating meaningful quantities, but there are very far from sufficient when it comes to embracing uncertainty. We have to acknowledge the uncertainty in the entire statistical modelling process, both when we are making decisions about that modelling process and when we present our findings to others."
  },
  {
    "objectID": "hypothesis_testing_py.html#next-steps",
    "href": "hypothesis_testing_py.html#next-steps",
    "title": "6  Hypothesis Testing",
    "section": "6.6 Next Steps",
    "text": "6.6 Next Steps\nThe goal for this chapter is that readers come away with an understanding of the significant limitations of null hypothesis significance testing and the more robust approaches to testing hypotheses, and the alternative approach of estimating meaningful effects. I hope that this chapter has helped set people up reasonably well when carrying out tests. Equally, I hope that they will consider the alternatives, recognising when it is defensible to carry out a statistical test and when estimating the effect size is necessary.\nI have taken a slightly more forgiving position than many of the statisticians that I respect, and from which I have learned so much, but I have deliberately done so because I think industry has slightly different incentives, and therefore slightly different requirements. I think that there are situations where the use of hypothesis testing is sufficient and defensible, and those situations are much more common in industry. The criticisms remain perfectly valid, but I hope I have given readers the tools to carry out more robust testing, and a good understanding of when to choose estimation instead, where necessary.\nWhile it is possible to use statistical tests for estimation, I also believe that the big wide world of estimation is better served by regression models. I think regression modelling is a better starting point than statistical tests because the structure of regression modelling places more emphasis on the effect size. Further, estimating effect sizes does not rule out the use of statistical tests, but it does require redirecting our focus and making sure to treat the magnitude and direction of effect size as the most important part of the analysis. The premise of statistical tests is that they test for the existence of an effect, and while packages like {infer} frame test outputs in a way that makes using statistical tests for these purposes, it does still require a little work to think about about the results of statistical tests in these terms. Further, I think that regression modelling gives a lot more flexibility when we are attempting to estimate an effect as precisely as possible, and the process and resulting outputs of a regression model lend themselves more naturally to a focus that steers away from hypothesis testing and towards estimating effects.\nIf it hasn’t been made clear before, I think it is important to stress the distinction between the null hypothesis testing framework and the statistical tests used to carry them out. The tests themselves are not really the problem. However, they are closely tied to null hypothesis testing. Ultimately, the statistical tests discussed in this chapter (and many more) are just special cases of linear models (Lindeløv 2019), and the results in both will be identical (Zablotski 2019) (if not the way the outputs are presented), so the same things can be achieved with either approach, but regression modelling offers more flexibility, allows for the layering of complexity/precision in estimates, and the way regression outputs are framed are generally a little more balanced. Treating regressions as the first point of call for inferential statistics is not a necessary prerequisite for “doing things the right way”, but doing things the right way becomes easier, because it is easier to frame everything in terms of the magnitude and direction of effects.\nThe good news? The next chapter is about modelling linear effects using regression!"
  },
  {
    "objectID": "hypothesis_testing_py.html#resources",
    "href": "hypothesis_testing_py.html#resources",
    "title": "6  Hypothesis Testing",
    "section": "6.7 Resources",
    "text": "6.7 Resources\n\n{infer}\nSeeing Theory\nThe Permutation Test\nThere is Only One Test\nMore Hypotheses, Less Trivia\nPermutation Tests\nPermutation Test as an Alternative to Two-Sample T-Test Using R\nPower Analysis\nRecommended Resources for A/B Testing\nMcElreath - None of the Above\nIntroduction to Scientific Programming and Simulations in R\nStatistical Inference is Only Mostly Wrong\nHypothesis Testing is Only Mostly Useless\nThe Idea of Permutation\nCoding for Data: Permutation and the T-Test\n\n\n\n\n\n\n\nAschwanden, Christie. 2015. “Science Isn’t Broken: It’s Just a Hell of a Lot Harder Than We Give It Credit For.” FiveThirtyEight. https://fivethirtyeight.com/features/science-isnt-broken/.\n\n\n———. 2016. “Statisticians Found One Thing They Can Agree on: It’s Time to Stop Misusing p-Values.” FiveThirtyEight. https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/.\n\n\nBlackwell, Matthew. 2023. “A User’s Guide to Statistical Inference and Regression.” https://mattblackwell.github.io/gov2002-book/.\n\n\nFisher, Ronald A. 1956. “Statistical Methods and Scientific Inference.”\n\n\nGelman, Andrew. 2016. “The Problems with p-Values Are Not Just with p-Values.” The American Statistician 70. http://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nGelman, Andrew, and Eric Loken. 2021. “The Statistical Crisis in Science.” American Scientist 102 (6): 460. https://doi.org/10.1511/2014.111.460.\n\n\nGelman, Andrew, and Hal Stern. 2006. “The Difference Between \"Significant\" and \"Not Significant\" Is Not Itself Statistically Significant.” The American Statistician 60 (4): 328–31.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. “Statistical Tests, p-Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31: 337–50.\n\n\nLakens, Daniel. 2022. Improving Your Statistical Inferences. https://doi.org/10.5281/zenodo.6409077.\n\n\nLindeløv, Jonas Kristoffer. 2019. “Common Statistical Tests Are Linear Models.” https://lindeloev.github.io/tests-as-linear/.\n\n\nMcElreath, Richard. 2023. “None of the Above.” https://elevanth.org/blog/2023/07/17/none-of-the-above/.\n\n\nPoole, Charles. 2022. “The Statistical Arc of Epidemiology.” Presented at the \"What is the Value of the P-Value?\" Panel Discussion, Cosponsored by UNC TraCS, Duke University, and Wake Forest University CTSA Biostatistics, Epidemiology and Research Design (BERD) Cores.\n\n\nPopper, Karl. 1935. The Logic of Scientific Discovery. Routledge.\n\n\nPrytherch, Ben. 2022. DSCI 335: Inferential Reasoning in Data Analysis. https://bookdown.org/csu_statistics/dsci_335_spring_2022/.\n\n\nSinikumpu, Suvi-Päivikki, Jari Jokelainen, Juha Auvinena, Markku Timonen, and Laura Huilaja. 2021. “Association Between Psychosocial Distress, Sexual Disorders, Self-Esteem and Quality of Life with Male Androgenetic Alopecia: A Population-Based Study with Men at Age 46.” BMJ Open 11 (12).\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019. “Moving to a World Beyond \"p &lt; 0.05\".” The American Statistician 73 (sup1): 1–19. https://doi.org/10.1080/00031305.2019.1583913.\n\n\nZablotski, Yury. 2019. “Statistical Tests Vs. Linear Regression.” https://yury-zablotski.netlify.app/post/statistical-tests-vs-linear-regression/."
  },
  {
    "objectID": "hypothesis_testing_py.html#footnotes",
    "href": "hypothesis_testing_py.html#footnotes",
    "title": "6  Hypothesis Testing",
    "section": "",
    "text": "I could stubbornly refuse to write about hypothesis tests, like covering my eyes and ears and screaming as someone asks me how to do a t-test properly, but something tells me that isn’t going to stop them happening. I would rather give someone all the necessary resources (or at least as much as is possible in a relatively limited guide like this), and hope it helps them improve the way they go about asking questions of data.↩︎\nThis is in contrast to academia, where it is rare that anything other than precision will matter.↩︎\nI won’t discuss all 25 misinterpretations that Greenland et al. (2016) discuss, because that would be overkill here. For anyone that is interested in learning more about the problems with p-values, I’d recommend going straight to the source and reading their paper.↩︎\nThe probability that a p-value represents are specific to the statistical model we are specifying. It’s important to remember that violations of our assumptions in specifying our model can bias the p-value and invalidate our results.↩︎\nI’m assuming that anyone following this advice has good intentions, and is not looking to cheat their way to the results they want. Someone wanting to achieve this is going to find a way to do it regardless.↩︎\nA scientific hypothesis reflects a theory about the world (for example, Drug \\(X\\) decreases the risk of Disease \\(Y\\)) while a statistical hypothesis reflect what is observed in the data (A one-unit increase in \\(x\\) will correspond with a one-unit decrease in \\(y\\)).↩︎\nIt could be a hypothesis that captures a consequence of a scientific hypothesis being true (as opposed to false, in the case of the null hypothesis), or it can test two scientific hypotheses against each other (instead of one against the possibility of nothing going on) (Prytherch 2022).↩︎\nIf you are unclear on some of the foundations that underpin what I’m discussing here, I’d recommend starting with StatQuest’s Statistics Fundamentals YouTube playlist, and for a more detailed introduction, I would give Answering Questions With Data a try.↩︎\nStandard deviation can be computed from standard error by multiplying the standard error by the square root of the sample size. The formula for this is as follows: \\[\\sigma = SE \\times \\sqrt{n}\\]↩︎\nUsing only five observations is less than ideal, but given that what we are doing is just illustrative, this will work fine.↩︎\nIt is also worth noting that while the guidance here is about the estimation approach to statistical inference, it can also be applied to hypothesis testing too.↩︎"
  },
  {
    "objectID": "linear_regression_py.html#exploratory-data-analysis",
    "href": "linear_regression_py.html#exploratory-data-analysis",
    "title": "7  Linear Regression",
    "section": "7.1 Exploratory Data Analysis",
    "text": "7.1 Exploratory Data Analysis\n\n# data summary\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 583 entries, 0 to 582\nData columns (total 7 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   area_code            583 non-null    object \n 1   year                 583 non-null    int64  \n 2   life_expectancy      583 non-null    float64\n 3   imd_score            583 non-null    float64\n 4   imd_decile           583 non-null    int64  \n 5   physiological_score  583 non-null    float64\n 6   behavioural_score    583 non-null    float64\ndtypes: float64(4), int64(2), object(1)\nmemory usage: 32.0+ KB\n\n\n\n# view first five rows\ndf.head()\n\n\n\n\n\n\n\n\narea_code\nyear\nlife_expectancy\nimd_score\nimd_decile\nphysiological_score\nbehavioural_score\n\n\n\n\n0\nE06000001\n2015\n78.738395\n35.037\n1\n89.9\n84.9\n\n\n1\nE06000001\n2016\n79.073210\n35.037\n1\n88.8\n84.4\n\n\n2\nE06000001\n2017\n79.080000\n35.037\n1\n84.8\n82.7\n\n\n3\nE06000001\n2018\n78.810000\n35.037\n1\n84.6\n81.0\n\n\n4\nE06000002\n2015\n77.787260\n40.460\n1\n96.4\n80.3\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nyear\nlife_expectancy\nimd_score\nimd_decile\nphysiological_score\nbehavioural_score\n\n\n\n\ncount\n583.000000\n583.000000\n583.000000\n583.000000\n583.000000\n583.000000\n\n\nmean\n2016.497427\n81.167021\n23.182043\n5.442539\n99.015952\n99.395026\n\n\nstd\n1.118223\n1.612812\n8.043172\n2.827766\n10.012470\n8.698003\n\n\nmin\n2015.000000\n76.535000\n5.846000\n1.000000\n78.200000\n72.400000\n\n\n25%\n2015.500000\n80.011722\n17.016000\n3.000000\n92.100000\n93.250000\n\n\n50%\n2016.000000\n81.210000\n22.965000\n5.000000\n97.900000\n100.600000\n\n\n75%\n2017.000000\n82.229125\n28.104000\n8.000000\n104.500000\n105.900000\n\n\nmax\n2018.000000\n86.050000\n45.039000\n10.000000\n125.900000\n120.000000\n\n\n\n\n\n\n\n\n# imd\nsns.scatterplot(x='life_expectancy', y='imd_score', data=df)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# physiological risk factors\nsns.scatterplot(x='life_expectancy', y='physiological_score', data=df)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# behavioural risk factors\nsns.scatterplot(x='life_expectancy', y='behavioural_score', data=df)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n# correlations with life expectancy\ndf.corr(numeric_only=True).loc[:,'life_expectancy']\n\nyear                  -0.005660\nlife_expectancy        1.000000\nimd_score             -0.837260\nimd_decile             0.813546\nphysiological_score    0.581565\nbehavioural_score      0.887986\nName: life_expectancy, dtype: float64\n\n\n\n# correlation matrix\ncorr = df[[\n    'life_expectancy', 'imd_score',\n    'physiological_score', 'behavioural_score']].corr()\ncorr\n\n\n\n\n\n\n\n\nlife_expectancy\nimd_score\nphysiological_score\nbehavioural_score\n\n\n\n\nlife_expectancy\n1.000000\n-0.837260\n0.581565\n0.887986\n\n\nimd_score\n-0.837260\n1.000000\n-0.378271\n-0.807654\n\n\nphysiological_score\n0.581565\n-0.378271\n1.000000\n0.587748\n\n\nbehavioural_score\n0.887986\n-0.807654\n0.587748\n1.000000\n\n\n\n\n\n\n\n\n# plot correlation matrix\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    square=True, annot=True)\nplt.show()\n\n\n\n\n\n\n\n\nThere are strong correlations between life expectancy and the three independent variables, however, the correlation between IMD score and the two risk factor variables is also relatively strong, and in the case of behavioural risk factors, it is very strong. This could be an issue."
  },
  {
    "objectID": "linear_regression_py.html#linear-regression",
    "href": "linear_regression_py.html#linear-regression",
    "title": "7  Linear Regression",
    "section": "7.2 Linear Regression",
    "text": "7.2 Linear Regression\n\n# create model\nmodel = sm.OLS.from_formula(\n    'life_expectancy ~ imd_score + physiological_score + behavioural_score',\n    data=df)\n\n# fit model\nresults = model.fit()\n\n# print summary\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:        life_expectancy   R-squared:                       0.844\nModel:                            OLS   Adj. R-squared:                  0.843\nMethod:                 Least Squares   F-statistic:                     1042.\nDate:                Fri, 27 Oct 2023   Prob (F-statistic):          7.23e-233\nTime:                        17:48:11   Log-Likelihood:                -564.29\nNo. Observations:                 583   AIC:                             1137.\nDf Residuals:                     579   BIC:                             1154.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=======================================================================================\n                          coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept              71.5915      0.623    114.851      0.000      70.367      72.816\nimd_score              -0.0775      0.006    -13.583      0.000      -0.089      -0.066\nphysiological_score     0.0238      0.003      7.141      0.000       0.017       0.030\nbehavioural_score       0.0907      0.006     15.020      0.000       0.079       0.103\n==============================================================================\nOmnibus:                      171.707   Durbin-Watson:                   0.765\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              611.061\nSkew:                           1.343   Prob(JB):                    2.04e-133\nKurtosis:                       7.236   Cond. No.                     3.36e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.36e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "linear_regression_py.html#evaluating-and-interpreting-regression-models",
    "href": "linear_regression_py.html#evaluating-and-interpreting-regression-models",
    "title": "7  Linear Regression",
    "section": "7.3 Evaluating and Interpreting Regression Models",
    "text": "7.3 Evaluating and Interpreting Regression Models\nWhen we fit a regression model, we have to consider both how well the model fits the data (evaluation) and the influence that each explanatory variable is having on the outcome (interpretation). The evaluation process is making sure that the model is doing a good enough job that we can reasonably draw some conclusions from it, while the interpretation process is understanding the conclusions we can draw. It’s important to understand the purpose that both of these processes serve, and to understand that the evaluation process is just kicking the tires, while what we are ultimately concerned with is the interpretation.\nThere are several elements to consider when interpreting the results of a linear regression model. Broadly speaking the focus should be on the magnitude and the precision of the model coeffficients. The magnitude of the coefficients indicates the strength of the association between the explanatory variables and the outcome variable, while the precision of the coefficients indicates the uncertainty in the estimated coefficients. We use the regression model coefficients to understand the magnitude of the effect and standard errors (and confidence intervals) to understand the precision of the effect.\n\n7.3.1 Model Evaluation\nThere are a number of ways we might evaluate model performance, but the starting point should be the model’s \\(R^2\\) and the p-values of the model coefficients. These can help us consider whether the model fits the data and whether we have enough data to trust the conclusions we can draw from each explanatory variable.\n\n7.3.1.1 \\(R^2\\)\nThe \\(R^2\\) value is the proportion of the variance in the outcome that is explained by the model. It is often used as a measure of the goodness of fit of the model, and by extension the extent to which the specified model explains the outcome, but caution should be applied when using \\(R^2\\) this way, because \\(R^2\\) can be misleading and doesn’t always tell the whole story. One of the main problems with \\(R^2\\) when used in an inferential context is that adding more variables to the model will always increase the \\(R^2\\) value, even if the new variables are not actually related to the outcome. Adjusted \\(R^2\\) values are used to account for this problem, but they don’t resolve all issues with \\(R^2\\), so it is important to consider \\(R^2\\) as just one of a number of indicators of the model’s fit. If the \\(R^2\\) is high, this doesn’t necessarily mean that the model is a good fit for the data, but if the \\(R^2\\) is particularly low, this might be a good indication that the model is not a good fit for the data, and at the very least it is good reason to be cautious about the results of the analysis, and to take a closer look at the model and the data to understand why the \\(R^2\\) is so low.\nDefining high/low \\(R^2\\) values is dependent on the context of the analysis. The theory behind the model should inform our expectations about the \\(R^2\\) value, and we should also consider the number of explanatory variables in the model (adding variables to a regression will increase the \\(R^2\\)). If the theory suggests that the model should explain a small amount of the variance in the outcome, and the model has only a few explanatory variables, then a low \\(R^2\\) value might not be a cause for concern.\nThe Adjusted \\(R^2\\) value for the above regression model is 0.843, which means that the model explains 84.3% of the observed variance in life expectancy in our dataset. This is a relatively high \\(R^2\\) value.\n\n\n7.3.1.2 P-Values\nP-values are the probability of observing a coefficient as large as the estimated coefficient for a particular explanatory variable, given that the null hypothesis is true (i.e. that the coefficient is equal to zero), indicating that no effect is present. This can be a little tricky to understand at first, but a slightly crude way of thinking about this is that p-values are an estimate of the probability that the model coefficient you observe is actually distinguishable from zero.\nP-values are used to test the statistical significance of the coefficients. If the p-value is less than the significance level then the coefficient is statistically significant, and we can reject the null hypothesis that the coefficient is equal to zero. If the p-value is greater than the significance level, then the coefficient is not statistically significant, and we cannot reject the null hypothesis that the coefficient is equal to zero. The significance level is often set at 0.05, which means that we are willing to accept a 5% chance of incorrectly rejecting the null hypothesis. This means that if the p-value is less than 0.05, then there is a greater than 95% probability that the coefficient is not equal to zero. However, the significance level is a subjective decision, and can be set at any value. For example, if the significance level is set at 0.01, then we are willing to accept a 1% chance of incorrectly rejecting the null hypothesis.\nIn recent years p-values (and the wider concept of statistical significance) have been the subject of a great deal of discussion in the scientific community, because there is a belief among many practitioners that a lot of scientific research places too much weight in the p-value, treating it as the most important factor when interpreting a regression model. I think this perspective is correct, but I think dismissing p-values entirely is a mistake too. The real problem is not with p-values themselves, but with the goal of an analysis being the identification of a statistically significant effect.\nI think the best way to think about p-values is the position prescribed by Gelman, Hill, and Vehtari (2020). Asking whether our coefficient is distinguishable from zero is the wrong question. As analysts, we should typically know enough about the context we are studying to build regression models with explanatory variables that will some effect. The question is how much. A statistically insignificant model coefficient is less a sign that a variable has no effect, and more a sign that there is insufficient data to detect a meaningful effect. If our model coefficients are not statistically significant, it is simply a good indication that we need more data. If the model coefficients are statistically significant, then our focus should be on interpreting the coefficients, not on the p-value itself.\n\n\n\n7.3.2 Model Interpretation\nModel interpretation is the process of understanding the influence that each explanatory variable has on the outcome. This involves understanding the direction, the magnitude, and the precision of the model coefficients. The coefficients themselves will tell us about direction and magnitude, while the standard errors and confidence intervals will tell us about precision.\n\n7.3.2.1 Model Coefficients\nThe model coefficients are the estimated values of the regression coefficients, or the estimated change in the outcome variable for a one unit change in the explanatory variable, while holding all other explanatory variables constant. The intercept is the estimated value of the outcome variable when all of the explanatory variables are equal to zero (or the baseline values if the variables are transformed).\nA regression’s coefficients are the most important part of interpreting the model. They specify the association between the explanatory variables and the outcome, telling us both the direction and the magnitude of the association. The direction of the association is indicated by the sign of the coefficient (positive = outcome increases with the explanatory variable, negative = outcome decreases with the explanatory variable). The magnitude of the association is indicated by the size of the coefficient (the larger the coefficient, the stronger the association). If the direction of the association is going in the wrong direction to that which we expect, this obviously indicates issues either in the theory or in the model, and if the magnitude of the effect is too small to be practically significant, then either the explanatory variable is not a good predictor of the outcome, or the model is not doing a good job of fitting the data (either the explanatory variable doesn’t really matter or the model is not capturing the true relationship between the explanatory variables and the outcome). However, while the direction is relatively easy to interpret, magnitude is not, because what counts as a practically significant effect is dependent on the context of the analysis.\n\n\n7.3.2.2 Standard Errors\nThe standard errors are the standard deviation of the estimated coefficients. This is a measure of the precision of the estimated association between the explanatory variables and the outcome. The larger the standard error, the less precise the coefficient estimate. Precisely measured coefficients suggest that the model is fitting the data well, and we can be confident that the association between the explanatory variables and the outcome is real, and not just a result of random variation in the data. Defining what counts as ‘precise’ in any analysis, much like the coefficient magnitude, is dependent on the context of the analysis. However, in general, a standard error of less than 0.5 is considered to be a good estimate, and a standard error of less than 0.25 is considered to be a very precise estimate.\nIt is important to recognise that a precise coefficient alone cannot be treated as evidence of a causal effect between the explanatory variable and the outcome but it is, at least, a good indication that the explanatory variable is a good predictor of the outcome. In order to establish a causal relationship, we may consider methods that are more robust to the presence of confounding or ommitted variables, the gold standard being randomised controlled trials, but with a strong theoretical basis and a good understanding of the data, we can make a good case for a causal relationship based on observational data."
  },
  {
    "objectID": "linear_regression_py.html#next-steps",
    "href": "linear_regression_py.html#next-steps",
    "title": "7  Linear Regression",
    "section": "7.4 Next Steps",
    "text": "7.4 Next Steps"
  },
  {
    "objectID": "linear_regression_py.html#resources",
    "href": "linear_regression_py.html#resources",
    "title": "7  Linear Regression",
    "section": "7.5 Resources",
    "text": "7.5 Resources\n\nMLU Explain: Linear Regression\nStatQuest: Linear Regression\nRegression & Other Stories\nModernDive: Statistical Inference via Data Science\nA User’s Guide to Statistical Inference and Regression\n\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari Vehtari. 2020. Regression and Other Stories. Cambridge University Press."
  },
  {
    "objectID": "ml_workflow_py.html#data",
    "href": "ml_workflow_py.html#data",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "8.1 Data",
    "text": "8.1 Data\n\n# data summary\ndf.info()\ndf.describe()\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 918 entries, 0 to 917\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   age                 918 non-null    int64  \n 1   sex                 918 non-null    object \n 2   resting_bp          918 non-null    int64  \n 3   cholesterol         918 non-null    int64  \n 4   fasting_bs          918 non-null    int64  \n 5   resting_ecg         918 non-null    object \n 6   max_hr              918 non-null    int64  \n 7   angina              918 non-null    object \n 8   heart_peak_reading  918 non-null    float64\n 9   heart_disease       918 non-null    int64  \ndtypes: float64(1), int64(6), object(3)\nmemory usage: 71.8+ KB\n\n\n\n\n\n\n\n\n\nage\nsex\nresting_bp\ncholesterol\nfasting_bs\nresting_ecg\nmax_hr\nangina\nheart_peak_reading\nheart_disease\n\n\n\n\n0\n40\nM\n140\n289\n0\nNormal\n172\nN\n0.0\n0\n\n\n1\n49\nF\n160\n180\n0\nNormal\n156\nN\n1.0\n1\n\n\n2\n37\nM\n130\n283\n0\nST\n98\nN\n0.0\n0\n\n\n3\n48\nF\n138\n214\n0\nNormal\n108\nY\n1.5\n1\n\n\n4\n54\nM\n150\n195\n0\nNormal\n122\nN\n0.0\n0\n\n\n\n\n\n\n\n\n8.1.1 Cleaning\nThough in this instance, the data is relatively clean, the following steps are included to demonstrate how you might clean data in a real-world scenario. Dropping NAs and duplicates should, however, be done with care (see discussion below).\n\n# check for missing values\ndf.isna().sum()\n\n# drop missing values\ndf.dropna(inplace=True)\n\n# check for duplicates\ndf.duplicated().sum()\n\n# drop duplicates\ndf.drop_duplicates(inplace=True)\n\nThere are several variables for which there appear to be missing values in the form of zeroes. This was identified in the exploratory data analysis notebook in this project. Variables like cholesterol and resting_bp should not be zero, and because the target class is disproportionately distributed within these zero values, it is important to address the problem.\nThere are a number of choices that could be made when it comes to dealing with null or missing values, and there are robust approaches to imputation (though it is necessary to take great care when doing so), however in the interest of simplicity, we will simply remove the rows with zero values in this instance.1\n\n# remove zero values\ndf = df[(df['cholesterol'] != 0) & (df['resting_bp'] != 0)]\n\n\n\n8.1.2 Train/Test Split\nOne of the central tenets of machine learning is that the model should not be trained on the same data that it is evaluated on. This is because the model could learn spurious/random patterns and correlations in the training data, and this will harm the model’s ability to make good predictions on new, unseen data. There are many way of trying to resolve this, but the most simple approach is to split the data into a training and test set. The training set will be used to train the model, and the test set will be used to evaluate the model.\n\n# split data into train and test sets\nX = df.drop('heart_disease', axis=1)\ny = df['heart_disease']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n\n\n8.1.3 Preprocessing\nThe purpose of preprocessing is to prepare the data for model fitting. This can take a number of different forms, but some of the most common preprocessing steps include:\n\nNormalising/Standardising data\nOne-hot encoding categorical variables\nRemoving outliers\nImputing missing values\n\nWe will build a preprocessing recipe that one-hot encodes all categorical features, and normalises the numeric features so that they have a mean of zero and a standard deviation of one.\n\n# specify all features for the model\nfeats = df.drop('heart_disease', axis=1)\n\n# specify target\ntarget = ['heart_disease']\n\n# get categorical columns\ncat_cols = df.select_dtypes(include='object').columns\n# get numerical columns (excluding target)\nnum_cols = df.drop('heart_disease', axis=1).select_dtypes(exclude='object').columns\n\nscaler = Pipeline(steps=[('scaler', StandardScaler())])\none_hot = Pipeline(steps=[('one_hot', OneHotEncoder(drop='first'))])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('nums', scaler, num_cols),\n        ('cats', one_hot, cat_cols)\n        ])"
  },
  {
    "objectID": "ml_workflow_py.html#model-training",
    "href": "ml_workflow_py.html#model-training",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "8.2 Model Training",
    "text": "8.2 Model Training\nThere are many different types of models that can be used for classification problems, and selecting the right model for the problem at hand can be difficult when you are first starting out with machine learning.\nSimple models like linear and logistic regressions are often a good place to start and can be used to get a better understanding of the data and the problem at hand, and can give you a good idea of baseline performance before building more complex models to improve performance. Another example of a simple model is K-Nearest Neighbours (KNN), which is a non-parametric model that can be used for both classification and regression problems.\nWe will fit a logistic regression and a KNN, as well as fitting a Random Forest model, which is a good example of a slightly more complex model that will often perform well on structured data.\n\n8.2.1 Cross-Validation\nOn top of the train/test split, we will also use cross-validation to train our models. Cross-validation is a method of training a model on a subset of the data, and then evaluating the model on the remaining data. This process is repeated multiple times, and the average performance is used to evaluate the model. This helps make the training process more generalisable.\nFor more information on cross-validation and how it can be used to train models:\n\nThe Cross-Validation section in the Scikit-learn documentation provides a good overview of cross-validation and how it can be used to train models. It includes a discussion of a typical cross-validation workflow; the different types of cross-validation, their implementation in Scikit-learn, and their pros and cons; and the documentation is accompanied by a lot of really helpful visuals.\nNeptune AI’s blog post on cross-validation goes into detail discussing cross-validation strategies, and gives clear visual demonstrations of how each strategy works.\n\nWe will use 5-fold stratified cross-validation to train our models. This means that the training data will be split into 5 folds, ensuring that each fold has the same proportion of the target classes. Each fold will be used as a test set once, and the remaining folds will be used as training sets. This will be repeated 5 times, and the average performance will be used to evaluate the model.\n\n# select \nmodels = []\nmodels.append(('log', LogisticRegression(random_state=123)))\nmodels.append(('knn', KNeighborsClassifier()))\nmodels.append(('rf', RandomForestClassifier(random_state=123)))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nn_splits = 5\n\nfor name, model in models:\n  \n  kf = StratifiedKFold(n_splits, random_state=123, shuffle=True)\n  \n  pipeline = Pipeline([\n    ('preprocess', preprocess),\n    ('model', model)\n    ])\n    \n  cv_scores = cross_val_score(\n    pipeline, \n    X_train, \n    y_train, \n    cv=kf, \n    scoring='f1'\n    )\n  \n  results.append(cv_scores)\n  names.append(name)\n  \n  print('%s: %.2f (%.3f)' % (name, cv_scores.mean(), cv_scores.std()))\n\nlog: 0.79 (0.025)\nknn: 0.74 (0.018)\n\n\nrf: 0.77 (0.032)\n\n\nAlthough the logistic regression performs slightly better than the random forest, the random forest has higher standard deviation and has more hyperparameters that can be tuned, so it is likely this will model has the highest potential.\n\n\n8.2.2 Hyperparameter Tuning\nHaving defined the preprocessing pipeline, we can now train and tune the models. We will use the GridSearchCV() function to carry out hyperparameter tuning[^tuning], which will search over a grid of hyperparameter values (specified by params) to find the best performing model. We will use 5-fold cross-validation to train the models, and will evaluate the models using F1 score and accuracy.\nFor a more detailed discussion of hyperparameter tuning, and the different methods for tuning in Scikit-learn, the Tuning the Hyperparameters of an Estimator section of the Scikit-learn documentation is a good place to start, and the Hyperparameter Tuning by Grid Search section of the Scikit-learn course is an excellent accompaniment.\nIn addition, the AWS overview of hyperparameter tuning is a good resource if you are only looking for a brief overview. For an in-depth discussion about how hyperparameter tuning works, including the mathematics behind it, Jeremy Jordan’s Hyperparameter Tuning for Machine Learning Models blogpost is thorough but easy to follow.\nThere are a lot of different Python libraries for hyperparameter tuning, however the most popular libraries that are specifically designed for tuning are Hyperopt and Optuna. Both can be used to tune models built with Scikit-learn (and many other frameworks) and can often produce better results than Scikit-learn’s built-in hyperparameter tuning methods. I have had limited experience using Hyperopt, however I have used Optuna often and I would highly recommend it if/when you are ready to move beyond Scikit-learn’s options for tuning.\n\nparams = { \n  'rf__bootstrap': [True],\n  'rf__max_depth': [10, 20, 30],\n  \"rf__min_samples_leaf\" : [1, 2, 4],\n  \"rf__min_samples_split\" : [2, 5, 10],\n  \"rf__n_estimators\": [100, 250, 500]\n  }\n\npipeline = Pipeline([\n    ('preprocess', preprocess),\n    ('rf', RandomForestClassifier(random_state=123))\n])\n\nclf = GridSearchCV(\n  estimator=pipeline,\n  param_grid=params,\n  scoring='f1',\n  cv=kf,\n  verbose=1,\n  refit=True\n  )\n\n# tune random forest\ntuning_results = clf.fit(X_train, y_train)\n\n# get the f1 score for training set\nprint('Training F1 Score: %.2f' % (tuning_results.best_score_))\n\n# get the best performing model on training set\nbest_model = tuning_results.best_estimator_\n\nFitting 5 folds for each of 81 candidates, totalling 405 fits\n\n\nTraining F1 Score: 0.79"
  },
  {
    "objectID": "ml_workflow_py.html#model-evaluation",
    "href": "ml_workflow_py.html#model-evaluation",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "8.3 Model Evaluation",
    "text": "8.3 Model Evaluation\n\n# get predictions on holdout set\npreds = best_model.predict(X_test)\n\n# get the f1 score for holdout sets\nprint('Holdout F1 Score: %.2f' % (f1_score(y_test, preds)))\n\nHoldout F1 Score: 0.82\n\n\n\n# get confusion matrix\nconf_mat = pd.crosstab(y_test, preds, rownames=['Actual'], colnames=['Predicted'])\n\n# plot confusion matrix\nfig, ax = plt.subplots()\n\nsns.heatmap(\n    data = conf_mat,\n    annot=True,\n    cmap='Blues',\n    fmt='g'\n    )\n\nsns.despine()\nplt.show()"
  },
  {
    "objectID": "ml_workflow_py.html#model-interpretation",
    "href": "ml_workflow_py.html#model-interpretation",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "8.4 Model Interpretation",
    "text": "8.4 Model Interpretation\nWe can compute the features that are most important in predicting heart disease.\n\n# get feature importance\nfeature_importance = best_model.named_steps['rf'].feature_importances_\n\n# get feature names\nfeature_names = (\n    best_model\n    .named_steps['preprocess']\n    .transformers_[1][1]\n    .named_steps['one_hot']\n    .get_feature_names_out(cat_cols)\n    .tolist()\n)\n\n# get feature names\nfeature_names = num_cols.tolist() + feature_names\n\n# create dataframe\nfeat_importance = pd.DataFrame({\n  'feature': feature_names, \n  'importance': feature_importance\n  })\n\n# plot feature importance\nfig, ax = plt.subplots()\n\nsns.barplot(\n    x='importance',\n    y='feature',\n    data=feat_importance.sort_values(by='importance', ascending=False),\n    color='#1C355E'\n    )\n\nsns.despine()\nplt.show()"
  },
  {
    "objectID": "ml_workflow_py.html#next-steps",
    "href": "ml_workflow_py.html#next-steps",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "8.5 Next Steps",
    "text": "8.5 Next Steps"
  },
  {
    "objectID": "ml_workflow_py.html#resources",
    "href": "ml_workflow_py.html#resources",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "8.6 Resources",
    "text": "8.6 Resources\nThere are lots of great (and free) introductory resources for machine learning:\n\nMachine Learning University\nMLU Explain\nMachine Learning for Beginners\n\nFor a guided video course, Data Talks Club’s Machine Learning Zoomcamp (available on Github and Youtube) is well-paced and well-presented, covering a variety of machine learning methods and even covering some of the aspects that introductory course often skip over, like deployment and data engineering principles. However, while the ML Zoomcamp course is intended as an introduction to machine learning, it does assume a certain level of familiarity with programming (the course uses Python) and software engineering. The appendix section of the course is definitely helpful for bridging some of the gaps in the course, but it is still worth being aware of the way the course is structured.\nIf you are looking for something that goes into greater detail about a wide range of machine learning methods, then there is no better resource than An Introduction to Statistical Learning (or its accompanying online course), by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\nFinally, if you are particularly interested in learning the mathematics that underpins machine learning, I would highly recommend Mathematics of Machine Learning, which is admittedly not free but is very, very good. If you want to learn about the mathematics of machine learning but are not comfortable enough tackling the Mathematics of ML book, the StatQuest and 3Blue1Brown Youtube channels are both really accessible and well-presented."
  },
  {
    "objectID": "ml_workflow_py.html#footnotes",
    "href": "ml_workflow_py.html#footnotes",
    "title": "8  End-to-End Machine Learning Workflow",
    "section": "",
    "text": "In a real-world analysis, it would be important to understand why the data is missing, and to consider the implications of removing the data. For example, if the data is missing at random, then removing the data will not have a significant impact on the analysis. However, if the data is missing because it was not collected, or if the data is missing because it is not available, then removing the data could have a significant impact on the analysis.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aschwanden, Christie. 2015. “Science Isn’t Broken: It’s Just a\nHell of a Lot Harder Than We Give It Credit For.”\nFiveThirtyEight. https://fivethirtyeight.com/features/science-isnt-broken/.\n\n\n———. 2016. “Statisticians Found One Thing They Can Agree on: It’s\nTime to Stop Misusing p-Values.” FiveThirtyEight. https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/.\n\n\nBlackwell, Matthew. 2023. “A User’s Guide to Statistical Inference\nand Regression.” https://mattblackwell.github.io/gov2002-book/.\n\n\nFisher, Ronald A. 1956. “Statistical Methods and Scientific\nInference.”\n\n\nGelman, Andrew. 2016. “The Problems with p-Values Are Not Just\nwith p-Values.” The American Statistician 70. http://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari Vehtari. 2020.\nRegression and Other Stories. Cambridge University Press.\n\n\nGelman, Andrew, and Eric Loken. 2021. “The Statistical Crisis in\nScience.” American Scientist 102 (6): 460. https://doi.org/10.1511/2014.111.460.\n\n\nGelman, Andrew, and Hal Stern. 2006. “The Difference Between\n\"Significant\" and \"Not Significant\" Is Not Itself Statistically\nSignificant.” The American Statistician 60 (4): 328–31.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin,\nCharles Poole, Steven N. Goodman, and Douglas G. Altman. 2016.\n“Statistical Tests, p-Values, Confidence Intervals, and Power: A\nGuide to Misinterpretations.” European Journal of\nEpidemiology 31: 337–50.\n\n\nLakens, Daniel. 2022. Improving Your Statistical Inferences. https://doi.org/10.5281/zenodo.6409077.\n\n\nLindeløv, Jonas Kristoffer. 2019. “Common Statistical Tests Are\nLinear Models.” https://lindeloev.github.io/tests-as-linear/.\n\n\nMcElreath, Richard. 2023. “None of the Above.” https://elevanth.org/blog/2023/07/17/none-of-the-above/.\n\n\nPoole, Charles. 2022. “The Statistical Arc of\nEpidemiology.” Presented at the \"What is the Value of the\nP-Value?\" Panel Discussion, Cosponsored by UNC TraCS, Duke University,\nand Wake Forest University CTSA Biostatistics, Epidemiology and Research\nDesign (BERD) Cores.\n\n\nPopper, Karl. 1935. The Logic of Scientific Discovery.\nRoutledge.\n\n\nPrytherch, Ben. 2022. DSCI 335: Inferential Reasoning in Data\nAnalysis. https://bookdown.org/csu_statistics/dsci_335_spring_2022/.\n\n\nSinikumpu, Suvi-Päivikki, Jari Jokelainen, Juha Auvinena, Markku\nTimonen, and Laura Huilaja. 2021. “Association Between\nPsychosocial Distress, Sexual Disorders, Self-Esteem and Quality of Life\nwith Male Androgenetic Alopecia: A Population-Based Study with Men at\nAge 46.” BMJ Open 11 (12).\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA\nStatement on p-Values: Context, Process, and Purpose.” The\nAmerican Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019.\n“Moving to a World Beyond \"p &lt; 0.05\".” The American\nStatistician 73 (sup1): 1–19. https://doi.org/10.1080/00031305.2019.1583913.\n\n\nZablotski, Yury. 2019. “Statistical Tests Vs. Linear\nRegression.” https://yury-zablotski.netlify.app/post/statistical-tests-vs-linear-regression/."
  },
  {
    "objectID": "version_control.html",
    "href": "version_control.html",
    "title": "Version Control",
    "section": "",
    "text": "Version control systems like Git are vital for the management of any codebase. Version control means that changes to the code (and anything else in the repository) can be tracked over time. This is a software engineering best-practice that has also been adopted in data science. It is an excellent ‘habit’ to learn when working with code, because it allows you to track changes, and therefore, revert to previous versions of the code if necessary. It also makes collaboration significantly easier, as it allows multiple people to work on the same codebase without creating conflicts.\nIf you are new to Git, a good place to start is the NHS-R Git Training. Their Introduction to Git will help you understand why version control is necessary, what Git & GitHub can offer, and how to navigate these tools.\nThese data science guides are designed to be used with Git because this is a very important skill for data science, and while it can be a bit challenging at first, it is incredibly valuable to learn, and everyone who wants to do some of the tasks in these guides should also be using Git to manage their work."
  }
]