<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Python Data Science Guides - 5&nbsp; Hypothesis Testing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./linear_regression_py.html" rel="next">
<link href="./eda_py.html" rel="prev">
<link href="./figures/favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./hypothesis_testing_py.html">Statistical Inference</a></li><li class="breadcrumb-item"><a href="./hypothesis_testing_py.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./figures/scw_logo.jpg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Python Data Science Guides</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/NHS-South-Central-and-West/data-science-guides/tree/main/book/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./good_science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Doing Good Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Dealing with Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sql.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Importing Data from SQL</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eda_py.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis_testing_py.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear_regression_py.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ml_workflow_py.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">End-to-End Machine Learning Workflow</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Deep Learning</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Delivering Data Science</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./version_control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Version Control</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-motivation-for-hypothesis-testing" id="toc-the-motivation-for-hypothesis-testing" class="nav-link active" data-scroll-target="#the-motivation-for-hypothesis-testing"><span class="header-section-number">5.1</span> The Motivation for Hypothesis Testing</a></li>
  <li><a href="#the-problems-with-hypothesis-testing" id="toc-the-problems-with-hypothesis-testing" class="nav-link" data-scroll-target="#the-problems-with-hypothesis-testing"><span class="header-section-number">5.2</span> The Problems With Hypothesis Testing</a></li>
  <li><a href="#avoiding-the-pitfalls" id="toc-avoiding-the-pitfalls" class="nav-link" data-scroll-target="#avoiding-the-pitfalls"><span class="header-section-number">5.3</span> Avoiding the Pitfalls</a></li>
  <li><a href="#building-statistical-tests" id="toc-building-statistical-tests" class="nav-link" data-scroll-target="#building-statistical-tests"><span class="header-section-number">5.4</span> Building Statistical Tests</a>
  <ul class="collapse">
  <li><a href="#t-tests" id="toc-t-tests" class="nav-link" data-scroll-target="#t-tests"><span class="header-section-number">5.4.1</span> T-Tests</a></li>
  <li><a href="#chi-squared-tests" id="toc-chi-squared-tests" class="nav-link" data-scroll-target="#chi-squared-tests"><span class="header-section-number">5.4.2</span> Chi-Squared Tests</a></li>
  </ul></li>
  <li><a href="#moving-beyond-testing" id="toc-moving-beyond-testing" class="nav-link" data-scroll-target="#moving-beyond-testing"><span class="header-section-number">5.5</span> Moving Beyond Testing</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps"><span class="header-section-number">5.6</span> Next Steps</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources"><span class="header-section-number">5.7</span> Resources</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/NHS-South-Central-and-West/data-science-guides/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/NHS-South-Central-and-West/data-science-guides/blob/main/book/hypothesis_testing_py.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-hypothesis-testing" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- links used throughout the book -->
<!-- intro -->
<!-- good science -->
<!-- wrangling -->
<!-- eda -->
<!-- hypothesis testing -->
<!-- machine learning -->
<!-- version control -->
<div id="setup" class="cell" data-execution_count="2">
<details>
<summary>Setup Code (Click to Expand)</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># packages needed to run the code in this section</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install pandas numpy matplotlib seaborn statsmodel</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># import packages</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pingouin <span class="im">as</span> pg</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> rc</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot style</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot font</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>rc(<span class="st">'font'</span>,<span class="op">**</span>{<span class="st">'family'</span>:<span class="st">'sans-serif'</span>,<span class="st">'sans-serif'</span>:[<span class="st">'Arial'</span>]})</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot colour palette</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>colours <span class="op">=</span> [<span class="st">'#1C355E'</span>, <span class="st">'#00A499'</span>, <span class="st">'#005EB8'</span>]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>sns.set_palette(sns.color_palette(colours))</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># set random seed</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This chapter will cover a topic that is well known to many practitioners - hypothesis testing. Hypothesis testing is a popular approach for attempting to make inferences about data without being led astray by noise. Although hypothesis testing remains very popular, it has been much criticised because the most common framework for testing, Null Hypothesis Significance Testing (NHST), has faced increasing criticisms regarding it’s external validity. If you have ever come across discussions about the “replication crisis” in science, you will have read about the many issues concerning the use of statistical significance as a measure of evidence in science and the misunderstanding and misuse of p-values. The null hypothesis significance testing framework is another of the central culprits in this crisis.</p>
<p>Hypothesis testing may be one of the most prominent tools for statistical inference, but there is plenty of debate about how we should be doing it, and whether the method should be consigned to the history books of statistics, only to be remembered as that silly time in our lives when we used to do science really badly. Here I hope to be able to give readers a clear understanding of how hypothesis testing works, it’s biggest drawbacks, and some advice about how to carry out robust tests of hypotheses.</p>
<p>Beyond this, I will make a case for moving away from hypothesis testing and towards the estimation of effect sizes. In doing so, I will demonstrate how to carry out some common statistical tests, using the guidelines in this chapter to inform how tests are carried out and how the results are presented. I will attempt to thread the needle between caution and clarity<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, but I recognise that this is not easy, and I’m not sure how successfully I will manage it!</p>
<p>The compromise that I make here is that this chapter will be like a really terrible Choose Your Own Adventure book, where you choose to take the short route, which lays out a process for carrying out hypothesis testing in a robust manner, or you can tread the long, arduous path through this subject. The intention is that the shorter path should give you everything you need, particularly for anyone that is unfamiliar with hypothesis testing. However, if you have are looking to improve your understanding of hypothesis testing, you can choose to expand the sections with further details.</p>
<section id="the-motivation-for-hypothesis-testing" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="the-motivation-for-hypothesis-testing"><span class="header-section-number">5.1</span> The Motivation for Hypothesis Testing</h2>
<p>A hypothesis is a statement about an effect that we expect to observe in our sample data, that is intended to generalise to the population. We do not know what the true effect in the population is, but we have a theory about what it might be, and we can test that theory using a hypothesis about what we will observe in the sample.</p>
<p>Our hypothesis might relate to how the mean value of a certain quantity of interest (like height) differs from one group to another, or how we expect the value of one quantity in our sample to change as the value of another quantity changes (like the effect of weight on height). A hypothesis test is a statistical method for empirically evaluating a hypothesis, to determine whether what we observe in our sample data is likely to exist in the population.</p>
<p>Hypothesis testing is effectively a statistical method for testing the compatibility of the observed data against a relevant hypothesised effect, often the hypothesis of zero effect. The measure of that incompatibility is a test statistic, which quantifies the distance between the observed data and the expectations of the statistical test, given all the assumptions of the test (including the hypothesised effect size). For example, if testing the hypothesis that there is no effect, a distribution under the assumptions of the statistical test, including the null hypothesis, is generated, and the test statistic measures the distance between the observed data and the null distribution. A p-value is then calculated from this test statistic, representing the probability of a test statistic at least as extreme as the computed test statistic if all the assumptions of the statistical test are true, including the hypothesis that is being tested.</p>
<p>The most common approach to testing hypotheses is the Null Hypothesis Significance Testing (NHST) framework, which involves testing the hypothesis that there is no effect (the null hypothesis) and, where it is possible to reject the null, this can be used as evidence that <strong>something</strong> is going on, to which this may help present evidence for the theory developed by the researcher (the alternative hypothesis).</p>
<p>Hypothesis testing can be valuable for a variety of different use-cases, but they are not without their issues. Despite those issues, they remain a useful tool when you want to know whether the difference between two groups is real, or whether it’s a product of noise, or when you are interested in knowing whether an effect size is different from zero. When you have a need to answer questions with greater precision, I would argue that there is a need to go further than hypothesis testing, but in industry, where speed and simplicity are often of equal importance to precision<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, hypothesis testing can serve real value.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Further Details
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The goal of NHST is to “adjudicate between two complementary hypotheses” <span class="citation" data-cites="blackwell2023">(<a href="references.html#ref-blackwell2023" role="doc-biblioref">Blackwell 2023</a>)</span>, the null hypothesis (<span class="math inline">\(H_0\)</span>) and the alternative/research hypothesis (<span class="math inline">\(H_1\)</span>). The null hypothesis, which is the hypothesis that no effect exists in the population, is our starting assumption, and we require strong evidence to reject it. If we are able to find strong enough evidence to reject the null, this moves us one step closer to making a case for our alternative hypothesis.</p>
<p>The reasoning for using two complementary hypotheses is based on <span class="citation" data-cites="popper1935">Popper (<a href="references.html#ref-popper1935" role="doc-biblioref">1935</a>)</span>’s <em>The Logic of Scientific Discovery</em>. Popper argues that it is impossible to confirm a hypothesis, because hypotheses are a form of inductive reasoning, which involves drawing conclusions about a general principle based on specific observations. We cannot confirm a hypothesis because we cannot prove that it is true by generalising from specific instances. However, we <em>can</em> falsify a hypothesis. If what we observe in our sample provides strong evidence that our hypothesis cannot be true, then we can conclude (with a degree of certainty) that the hypothesis is false For a better understanding about the null and alternative hypothesis, I would recommend Josh Starmer’s StatQuest videos, embedded below:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0oc49DyA3hU?si=wDuNFfgpP4hQml8e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/5koKb5B_YWo?si=ydyTZguZATS9wehl" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>When we carry out a hypothesis test, we are quantifying our confidence that what we observe in the sample did not occur by chance. If we are able to show that a effect is extremely unlikely to have occurred by chance, we can reject the null hypothesis. Rejecting the null doesn’t demonstrate that the observed effect can be explained by our theory, however, demonstrating that the observed data does not conform to our expectations given that all of our test’s assumptions, including the null hypothesis, are true, moves us one step closer to supporting our theory.</p>
</div>
</div>
</div>
</section>
<section id="the-problems-with-hypothesis-testing" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="the-problems-with-hypothesis-testing"><span class="header-section-number">5.2</span> The Problems With Hypothesis Testing</h2>
<p>Hypothesis testing serves an important purpose in science, both in academia and in industry. We use hypothesis testing to sift through noise in data to dry and draw conclusions, and this is a perfectly reasonable goal.</p>
<p>However, the dominant framework for testing hypotheses, Null Hypothesis Significance Testing, is rife with issues. The primary problems with NHST are that it defines tests in terms of them being statistically significant or not, based on an arbitrary threshold value for p-values, and it frames p-values as the primary concern when interpreting a statistical model. While p-values are still relevant, they are not the whole story (nor the main part of the story).</p>
<p>Further, the importance afforded to p-values in the NHST framework encourages certain common misconceptions about them, and the framing that prioritises them also makes those misconceptions even more harmful. P-values are not the measure of the substantive importance of the effect; they are not the probability that the null hypothesis is true; and finally, a statistically significant p-value does not confirm the alternative hypothesis, just as a non-significant p-values does not confirm the null. They are a measure of how surprising the observed data would be if the hypothesis being tested (like the null hypothesis), and all other assumptions of the statistical test, are true.</p>
<p>Although these problems are of real significance in academia, where precision is of the greatest importance,</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Further Details
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There are many complaints and criticisms that can be levelled at hypothesis testing. Here are some of the main ones.</p>
<section id="p-values-statistical-significance" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="p-values-statistical-significance">P-Values &amp; Statistical Significance</h4>
<p>The biggest problems with NHST relate to the misuse and misunderstanding of p-values, and their dichotomisation in order to make arbitrary judgements about statistical significance. The extent of the issues surrounding p-values and statistical significance are so extensive that it led the American Statistical Association to make an official statement addressing the misconceptions and clarifying the correct usage of p-values <span class="citation" data-cites="wasserstein2016">(<a href="references.html#ref-wasserstein2016" role="doc-biblioref">Wasserstein and Lazar 2016</a>)</span>. The statement makes the point that p-values do not measure the probability that a hypothesis is true, or that the data was produced by random chance alone; nor should they be interpreted as measuring the size of an effect or the importance of a result; and that by themselves p-values are not a good measure of the evidence for a model or a hypothesis <span class="citation" data-cites="wasserstein2016">(<a href="references.html#ref-wasserstein2016" role="doc-biblioref">Wasserstein and Lazar 2016</a>)</span>. Unfortunately, these are all very common misconceptions that find their way into scientific research. The misconceptions go much further than this, however, as illustrated by <span class="citation" data-cites="greenland2016">Greenland et al. (<a href="references.html#ref-greenland2016" role="doc-biblioref">2016</a>)</span>. <span class="citation" data-cites="greenland2016">Greenland et al. (<a href="references.html#ref-greenland2016" role="doc-biblioref">2016</a>)</span> build on the wider discussion to include statistical tests, laying out a total of 25 different misconceptions about statistical testing, p-values (and confidence intervals) and statistical power<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. They make the case that the biggest culprit is the arbitrary use of thresholds for defining statistical significance, which frame p-values in problematic terms, reduce a need for critical thinking about findings, and are far too easily abused.</p>
<p>This discussion has even permeated some of the media <span class="citation" data-cites="aschwanden2015 aschwanden2016">(<a href="references.html#ref-aschwanden2015" role="doc-biblioref">Aschwanden 2015</a>, <a href="references.html#ref-aschwanden2016" role="doc-biblioref">2016</a>)</span>. FiveThirtyEight have written about this issue in some detail, and their discussion of the topic is perhaps more accessible than the more detailed discussions going on. <span class="citation" data-cites="aschwanden2015">Aschwanden (<a href="references.html#ref-aschwanden2015" role="doc-biblioref">2015</a>)</span> is a particularly good summary of the discussion around p-values, and does a good job highlighting that the issues around p-values and the “replication crisis” are not evidence that science can’t be trusted, but rather that this stuff is extremely difficult, and that it takes a community of scientists working towards the truth, to try and achieve results.</p>
</section>
<section id="garden-of-forking-paths" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="garden-of-forking-paths">Garden of Forking Paths</h4>
<p><span class="citation" data-cites="gelman2021">Gelman and Loken (<a href="references.html#ref-gelman2021" role="doc-biblioref">2021</a>)</span> add to the discussion by highlighting the role that the “garden of forking paths” approach to analysis, where there are many possible explanations for a statistically significant result, plays in the statistical crisis in science. It’s important to recognise how a scientific hypothesis can correspond to multiple statistical hypotheses <span class="citation" data-cites="gelman2021">(<a href="references.html#ref-gelman2021" role="doc-biblioref">Gelman and Loken 2021</a>)</span>, and to guard against this possibility. This effectively creates a “multiple comparisons” situation, where an analysis can uncover statistically significant results that can be used to support a theory by multiple “forking paths”. A “one-to-many mapping from scientific to statistical hypotheses” can create a situation that is equivalent to p-hacking but without the necessary intent <span class="citation" data-cites="gelman2021">(<a href="references.html#ref-gelman2021" role="doc-biblioref">Gelman and Loken 2021</a>)</span>.</p>
<p>One of the central questions that <span class="citation" data-cites="gelman2021">Gelman and Loken (<a href="references.html#ref-gelman2021" role="doc-biblioref">2021</a>)</span> ask, is whether the same data-analysis decisions would have been made with a different data set. The garden of forking paths is driven by the fact that we devise scientific hypotheses using our common sense and domain knowledge, given the data we have, and we do not consider the myriad implicit choices that this involves. By not considering these implicit choices, we fail to account for the garden of forking paths.</p>
</section>
<section id="comparisons-are-not-enough" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="comparisons-are-not-enough">Comparisons Are Not Enough</h4>
<p>I think that the biggest issue with p-values themselves is that we have placed far too much value in them. We use methods that are designed to compute p-values, we define “success” as the discovery of a statistically significant finding, and we give little thought to the design choices made in the process of carrying out these tests. This is partly driven by the frequency of misconceptions and misunderstandings of p-values, but it’s also a result of the methods we use.</p>
<p>If we are more precise about how we think about p-values, this should help us avoid making the mistake of thinking they are proving a hypothesis or that they are sufficient evidence for a theory, but if we are more precise about what a p-value really represents, this raises significant questions about the methods we use. What purpose does a test for “statistical significance” serve if we acknowledge that p-values are not enough? <span class="citation" data-cites="gelman2016">Gelman (<a href="references.html#ref-gelman2016" role="doc-biblioref">2016</a>)</span> argue that p-values are not the real problem, because the real problem is null hypothesis significance testing, which they refer to as a “parody of falsificationism in which straw-man null hypothesis A is rejected and this is taken as evidence in favour of preferred alternative B.” The null hypothesis significance testing framework places far too much weight on the shoulders of p-values, and even if we are doing a better job of hypothesis testing (being particularly careful not to make mistakes about how we use p-values), what is it really worth? When we are treating p-values as a part of the wider toolkit, hypothesis tests become less useful.</p>
</section>
</div>
</div>
</div>
</section>
<section id="avoiding-the-pitfalls" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="avoiding-the-pitfalls"><span class="header-section-number">5.3</span> Avoiding the Pitfalls</h2>
<p>There has been plenty of debate among statisticians about the issues around hypothesis testing and p-values, and while there is no detail too minor for two academics to throw hands over, there is a general consensus that our approach to p-values is a problem. What has not been settled, however, is what the right approach is. That makes this section of this chapter a little trickier. However, I will attempt to give some sensible advice about how to avoid the greatest dangers pose by hypothesis testing and the use of p-values more broadly.</p>
<p>A more general approach to hypothesis testing would frame things in terms of a test hypothesis, which is a hypothesised effect (including but not limited to zero effect – the null hypothesis) that is assumed to be true, and a statistical test is carried out that considers how well the observed date conforms to our expectations given that the test’s assumptions, including the test hypothesis, are true.</p>
<p>Perhaps unsurprisingly, the primary way to improve how we do hypothesis testing is to approach p-values differently. The simplest solution is not to use statistical significance at all. This is an arbitrary threshold that gives us very little good information, and allows researchers to avoid thinking critically about their findings. Instead, we should report the p-value itself, and be clear about what it is that p-values actually tell us!</p>
<p>Beyond framing hypothesis tests in more appropriate terms, a big part of the improvements that can be made is about transparency. Being transparent about results, about what those results mean, and about what they don’t mean, is important. Further, it is important to report all findings, and not pick and choose what we found based on which results support our theory (this becomes easier if we’re not in search of statistical significance).</p>
<p>Ultimately, hypothesis testing requires a careful, critical approach, and this applies to the development of the hypothesis, the design of the test, and the analysis of the findings.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Further Details
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="rethink-p-values" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="rethink-p-values">Rethink P-Values</h4>
<p>The p-value is not a magic number. It does not give the probability that your research hypothesis is true, nor does it give the probability that the observed association is a product of random chance <span class="citation" data-cites="wasserstein2019">(<a href="references.html#ref-wasserstein2019" role="doc-biblioref">Wasserstein, Schirm, and Lazar 2019</a>)</span>. One of the biggest issues I have with p-values is that they feel, given the way we use them in NHST, like they <strong>should</strong> be the probability that our hypothesis is true. However, it is important to never let those urges win - the p-value is not a hypothesis probability.</p>
<p>Instead, p-values are the degree to which the observed data are consistent with the pattern predicted by the research hypothesis, given the statistical test’s assumptions are not violated <span class="citation" data-cites="greenland2016">(<a href="references.html#ref-greenland2016" role="doc-biblioref">Greenland et al. 2016</a>)</span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p><span class="citation" data-cites="lakens2022">Lakens (<a href="references.html#ref-lakens2022" role="doc-biblioref">2022</a>)</span> recommends thinking about p-values as a statement about the probability of data, rather than a statement about the probability of a given hypothesis or theory. I think remembering this framing is a good way to avoid misconstruing exactly what p-values represent. Similarly, we can think about p-values as our confidence in what it is we observe in our data. How confident can we be that the data we observed is the product of random chance (and the assumptions that underpin our statistical model)? A p-value is an approximation of this.</p>
<p>The point is that none of these definitions (or more precisely, intuitive explanations) of p-values are claiming that p-values are the probability that the research hypothesis is true. They say nothing about a hypothesis, because they are not a hypothesis probability.</p>
<p>Framing p-values appropriately is one part of the necessary rethink about p-values. The other is the need to place less weight in what p-values tell us. Having acknowledged all the things that p-values are not, this part should be easy to understand. Given that p-values are not a hypothesis probability, and they are really a statement about our data, they are far from sufficient (though I would argue they are at least necessary) for our understanding of our data. Instead, they are part of a wider toolkit. A toolkit that contains even more important tools, like effect sizes!</p>
</section>
<section id="stop-using-statistical-significance" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="stop-using-statistical-significance">Stop Using Statistical Significance</h4>
<p>In addition to the common misuse of p-values, the null hypothesis significance testing framework also defines an arbitrary threshold for what is considered a sufficiently “meaningful” result - statistical significance.</p>
<p>The commonly defined significance threshold (or the alpha (<span class="math inline">\(\alpha\)</span>) level) is a p-value of 0.05. A p-value of less than 0.05 is deemed statistically significant, while a p-value greater than 0.05 is insignificant. So two p-values, one that equals 0.499 and another that equals 0.501 are both deemed to offer something very different to each other. One is treated as a sign of success, and the other is filed away in the desk drawer, never to be seen again! The arbitrariness of this threshold has been best highlighted by <span class="citation" data-cites="gelman2006">Gelman and Stern (<a href="references.html#ref-gelman2006" role="doc-biblioref">2006</a>)</span> when they demonstrated the fact that the difference between statistically significant and insignificant is not itself statistically significant.</p>
<p>Defining whether a research finding is of real value based on such a meaningless threshold is clearly one of the contributing factors in the problems with hypothesis testing, and the simple solution is to just stop treating p-values as either statistically significant or not!</p>
<p>If you need further evidence of the problems with how we have been using statistical significance, look no further than the man credited with first developing the method, Ronald A Fisher. <span class="citation" data-cites="fisher1956">Fisher (<a href="references.html#ref-fisher1956" role="doc-biblioref">1956</a>)</span> claimed that no researcher has a fixed level of significance that they are using to reject all hypotheses, regardless of their context. Ronald, I have some terrible, terrible news.</p>
<p>Where possible, do not use terms like statistical significance, and do not use a threshold (the <span class="math inline">\(\alpha\)</span> level) for significance. Report p-values instead. P-values should be “viewed as a continuous measure of the compatibility between the data and the entire model used to compute it, ranging from 0 for complete incompatibility to 1 for perfect compatibility, and in this sense may be viewed as measuring the fit of the model to the data.” <span class="citation" data-cites="greenland2016">(<a href="references.html#ref-greenland2016" role="doc-biblioref">Greenland et al. 2016, 339</a>)</span></p>
</section>
<section id="report-all-findings" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="report-all-findings">Report All Findings</h4>
<p>This suggestion ties in closely with the previous two. If p-values are treated like a continuous measure of compatibility, and we are no longer using thresholds to define whether a result is good or bad, this should reduce the urge to report findings selectively, based only on those that say the right thing.</p>
<p>Do not carry out multiple tests and only show the significant results. Be open and transparent about your process and what was tested. Only reporting significant results is problematic for many reasons. There is no reason to favour statistical significant results. Instead, we should either report all our findings, as this is the most transparent approach, or we should select the substantively meaningful results, taking great care to avoid only reporting those that confirm our theory<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>I think this principle is easier to follow in academia, where extensive reporting of findings is encouraged. I don’t think there is a downside to open and transparent reporting in academia (ignoring the potential risks posed to publication). However, in industry, there is often a need to be as clear and concise as possible. Reporting all results could potentially cause confusion, and make statistical findings much harder for non-technical audiences to understand. Therefore I think the advice for industry should be that reporting results should not be defined by the statistical significance of the results. We should report the results that matter, that contribute the most to our understanding of a quantity of interest, and that help others understand the findings. Communication of findings in industry should focus on communicating the key results from the analysis, and that should not be defined by significance.</p>
</section>
<section id="build-scientific-models" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="build-scientific-models">Build Scientific Models</h4>
<p>Finally, it is important to think carefully about the implications of any hypothesis, and to understand that there is a difference between a scientific hypothesis and a statistical hypothesis<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Statistical models do not, on their own, allow scientific hypotheses</p>
<p>There are often many different statistical hypotheses that would be consistent with a particular scientific hypothesis. This is well explained and multiple examples are given by <span class="citation" data-cites="gelman2021">Gelman and Loken (<a href="references.html#ref-gelman2021" role="doc-biblioref">2021</a>)</span>.</p>
<p>The garden of forking paths may be a particularly big issue in hypothesis testing, but it doesn’t go away if we scrap NHST. It is important to think about the ways that any scientific hypothesis can be explained by multiple statistical hypotheses, and it’s also very important to think about the many choices made when building any statistical model, and what this might mean for our work. There are so many different assumptions that go into the process of building a statistical model, and when we fail to consider the breadth of assumptions that underpin our work, we can easily invalidate our findings. As discussed earlier in this chapter, we have to remember that p-values are assuming that <strong>all</strong> of our assumptions are true, and if any of them are not true, we are in hot water.</p>
<p>What is needed is to build coherent, logically-specified scientific models from which we design our statistical analysis. <span class="citation" data-cites="mcelreath2023">McElreath (<a href="references.html#ref-mcelreath2023" role="doc-biblioref">2023</a>)</span> makes this case, arguing that we need to build theory-driven scientific models that describe the causal relationship we are modelling before we build statistical models that attempt to test our scientific models.</p>
<p>Building scientific models doesn’t necessarily address all concerns around the “garden of forking paths” issue, but by doing so we are forced to be more thoughtful about our research, and we are more able to identify problems with our analysis.</p>
</section>
</div>
</div>
</div>
</section>
<section id="building-statistical-tests" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="building-statistical-tests"><span class="header-section-number">5.4</span> Building Statistical Tests</h2>
<p>A generalised framework for hypothesis testing, that discards of NHST’s problematic baggage (instead following a Fisherian approach <span class="citation" data-cites="fisher1956">(<a href="references.html#ref-fisher1956" role="doc-biblioref">Fisher 1956</a>)</span>), would take the following steps:</p>
<ol type="1">
<li>Specify the test hypothesis (<span class="math inline">\(H_0\)</span>) that assumes some effect size against which the observed effect size will be compared - this will often be the assumption of zero effect (the null hypothesis), but it can be anything<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</li>
<li>Generate the test distribution, defined by the data being analysed and the test being carried out, which represents our expectation of what we would observe if the statistical model’s assumptions, which include the test hypothesis, are all true.</li>
<li>Compute the test statistic, which quantifies how extreme the observed data is, given the test distribution.</li>
<li>Compute the p-value, representing the probability of observing a test statistic as large or larger than the observed test statistic if all of the assumptions of the statistical model, including the test hypothesis, are true.</li>
</ol>
<p>The important takeaway is that the test statistic is a measure of the sample data, the test distribution is a measure of what we would expect to observe if the test hypothesis, and all other model assumptions, are true, and the statistical test is simply quantifying how compatible the observed data would be with the generated distribution.</p>
<p>While there are many different statistical tests that are designed to test different data distributions, using different assumptions, the general framework is the same. Below are some examples of statistical tests that are commonly used.</p>
<section id="t-tests" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="t-tests"><span class="header-section-number">5.4.1</span> T-Tests</h3>
<p>The general idea of a t-test is to calculate the statistical significance of the difference between two groups. What the groups represent depends on the type of t-test being carried out, whether it be comparing a single group of observed data against a null distribution that assumes the data was generated by chance, or comparing the difference between two observed groups against a null distribution that assumes the difference is zero.</p>
<p>There are several broad groups of t-tests:</p>
<ul>
<li>One-sample t-test - comparing the sample mean of a single group against a “known mean”, generally testing whether a sample is likely to be generated by chance.</li>
<li>Paired t-test - comparing means from the same group measured multiple times, like how someone responds to a survey at different points in time.</li>
<li>Two samples t-test - comparing the means of two groups, such as measuring the difference in blood glucose levels of two different groups.</li>
</ul>
<p>There are several variations within these groups, for example a paired t-test generally assumes equal variance between the two groups, but this is often not true, so an unequal variance t-test can be used.</p>
<p>Finally, when deciding on the nature of a t-test, you must also decide if the test should be one-tailed or two-tailed. A one-tailed t-test is used to test a hypothesis that includes a directional component (such as <span class="math inline">\(x &gt; y\)</span>), while a two-tailed t-test is just testing whether there is a difference between two groups. A one-tailed t-test is appropriate when you believe the difference between two groups should be positive or negative, but if the only belief is that the groups are different, a two-tailed t-test is appropriate.</p>
<p>A two-tailed t-test compares whether the sample mean(s) is different, whether larger or smaller, while a one-tailed t-test assumes the direction of the difference is known. For example, if it is known that Group A is either the same or bigger than Group B (or that Group A being smaller is not of interest), then a one-tailed t-test would be appropriate.</p>
<p>I will simulate data from real-world examples to demonstrate one-sample, paired, and two-sample t-tests, however, these examples are not exhaustive, and there are many variations on t-test implementations.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<section id="one-sample-t-test" class="level4" data-number="5.4.1.1">
<h4 data-number="5.4.1.1" class="anchored" data-anchor-id="one-sample-t-test"><span class="header-section-number">5.4.1.1</span> One-Sample T-Test</h4>
<p>We can carry out a one-sample t-test using IQ scores as our example, because there is plenty of data out there about IQ scores that we can base our simulations on. The average IQ score is said to be around 100, and scores are normally distributed with a standard deviation of 15, meaning that the majority of people have an IQ in the range of 85-115.</p>
<p>In a one-sample t-test we are interested in testing a sample mean against an “expected” mean. For example, perhaps we have a group of individuals that are viewed as “high achievers”, and we want to use IQ scores to test whether this group is meaningfully different to the rest of the population (despite being aware that IQ is not a particularly good test of either an individual’s intelligence or their potential).</p>
<p>A one-sample t-test would allow us to do this because we are able to test the mean IQ score for our high achievers against the expected mean value if there was no real difference between them and the population, which would just be the mean IQ score in the population (100).</p>
<p>Lets first start by simulating our high achievers’ IQ scores, assuming that their mean score is 105 (perhaps some of the group studied IQ tests extensively and are now able to ace the test), with a standard deviation of 15.</p>
<div id="simulate-high-achievers" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># mean and standard deviation</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>mean, sd <span class="op">=</span> <span class="dv">105</span>, <span class="dv">15</span> </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># draw sample from normal distribution</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>high_achievers <span class="op">=</span> np.random.normal(mean, sd, <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can take a look at the sample data we have simulated, to see how close it is to the true mean and standard deviation (because we are simulating the data it won’t be a perfect match).</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compute summary statistics for our sample</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>np.mean(high_achievers)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>np.std(high_achievers)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="summarise-sample" class="cell-output cell-output-display" data-execution_count="3">
<pre><code>11.27963405536393</code></pre>
</div>
</div>
<p>And we can visualise the data to get a better sense of what the group looks like.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>sns.histplot(x<span class="op">=</span>high_achievers, binwidth<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot labels and title</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'IQ Score'</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">''</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="hypothesis_testing_py_files/figure-html/visualise-sample-output-1.png" id="visualise-sample" width="791" height="501"></p>
</div>
</div>
<p>It is not clear what the underlying distribution of our sample data is here. We already know that the data was drawn from a normal distribution, but if we did not, we would need more observations to confirm this.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>pg.ttest(high_achievers, y <span class="op">=</span> <span class="dv">100</span>, alternative <span class="op">=</span> <span class="st">"greater"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="one-sample-test" class="cell-output cell-output-display" data-execution_count="5">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">T</th>
<th data-quarto-table-cell-role="th">dof</th>
<th data-quarto-table-cell-role="th">alternative</th>
<th data-quarto-table-cell-role="th">p-val</th>
<th data-quarto-table-cell-role="th">CI95%</th>
<th data-quarto-table-cell-role="th">cohen-d</th>
<th data-quarto-table-cell-role="th">BF10</th>
<th data-quarto-table-cell-role="th">power</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">T-test</td>
<td>1.568435</td>
<td>9</td>
<td>greater</td>
<td>0.075612</td>
<td>[99.0, inf]</td>
<td>0.495983</td>
<td>1.576</td>
<td>0.422705</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The p-value of our one-sample t-test is 0.105. This means that we would expect to observe a sample mean at least as extreme as our observed mean (106), if the null that there is no difference between the high achiever group’s mean IQ score and the population mean IQ score is true, a little more than 10% of the time, or 1 in 10 times. That’s quite high, so I wouldn’t be confident that we are observing a meaningful difference.</p>
<p>However, we know that there is a meaningful difference between our high achievers and the population, because we specified a difference when we simulated our sample data. If we were to conclude that the results we observe in our t-test do not lend any support to our hypothesis that the high achievers are better at IQ tests than the average person, this would be a false negative (or sometimes unhelpfully referred to as a Type 1 Error).</p>
<p>If we increase our sample size, maybe we will have more luck!</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>larger_sample <span class="op">=</span> np.random.normal(mean, sd, <span class="dv">30</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>pg.ttest(larger_sample, y <span class="op">=</span> <span class="dv">100</span>, alternative <span class="op">=</span> <span class="st">"greater"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="increase-sample-size" class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">T</th>
<th data-quarto-table-cell-role="th">dof</th>
<th data-quarto-table-cell-role="th">alternative</th>
<th data-quarto-table-cell-role="th">p-val</th>
<th data-quarto-table-cell-role="th">CI95%</th>
<th data-quarto-table-cell-role="th">cohen-d</th>
<th data-quarto-table-cell-role="th">BF10</th>
<th data-quarto-table-cell-role="th">power</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">T-test</td>
<td>2.865017</td>
<td>29</td>
<td>greater</td>
<td>0.003839</td>
<td>[103.31, inf]</td>
<td>0.523078</td>
<td>11.272</td>
<td>0.875403</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Increasing the sample size from 10 to 30 has decreased the probability of observing a test statistic at least as extreme as our sample mean if our data was generated by chance (and the assumptions of our statistical model are all met) from around 10% to just over 1.5%. That seems a lot more reasonable, and at this point I’d be more comfortable concluding that the high achiever’s IQ scores are meaningfully different to the average score in the population.</p>
<p>Of course, our p-value doesn’t tell us anything about the substantive importance of the difference in IQ scores. We might be willing to conclude that there <strong>is</strong> a difference, but does it matter? Well, the answer is no because it’s IQ scores and IQ scores don’t matter.</p>
<p>But what if, for some truly baffling reason, we had reason to believe IQ scores are not nonsense? In that case, how do we know if the difference between the mean value in the high achievers group and the population is large enough to be of scientific and substantive relevance? This is not a question that significance tests can answer, and is instead something that our domain expertise should answer.</p>
</section>
<section id="paired-t-test" class="level4" data-number="5.4.1.2">
<h4 data-number="5.4.1.2" class="anchored" data-anchor-id="paired-t-test"><span class="header-section-number">5.4.1.2</span> Paired T-Test</h4>
<p>When respondents in surveys are asked to self-report certain details about themselves, this can invite bias. Take, for example, the difference in <a href="https://digital.nhs.uk/data-and-information/areas-of-interest/public-health/health-survey-for-england-predicting-height-weight-and-body-mass-index-from-self-reported-data/differences-between-self-reported-and-interviewer-measured-height-weight-and-bmi-using-hse-2011-2016-data">self-reported and interviewer-measured height among men in the UK</a>. Although there are many proud short kings out there that will confidently declare their height, safe in the knowledge that good things come in small packages, there are plenty of men that wish they were a little bit taller (someone should write a song about that), and for whom a self-reported survey is an opportunity to dream big (literally). Any time that respondents are given the opportunity to report details about themselves that have any social baggage attached, there is a possibility that the responses will be subject to social desirability bias.</p>
<p>In the case of self-reported and measured male heights, the differences are not particularly large in absolute terms. It’s very possible that the difference is actually a product of some random variation, or measurement error. We can test this!</p>
<p>Using the mean values for reported and measured heights in 2016, calculating the standard deviation from the reported standard error<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, and calculating the correlation between the two variables using the value for each variable from 2011-2016<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, we can simulate our sample data.</p>
<div id="simulate-male-heights" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>mean, cov, n <span class="op">=</span> [<span class="fl">177.23</span>, <span class="fl">175.71</span>], [[<span class="dv">1</span>, <span class="fl">.83</span>], [<span class="fl">.83</span>,  <span class="dv">1</span>]], <span class="dv">100</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>self_reported, measured <span class="op">=</span> np.random.multivariate_normal(mean, cov, n).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We have simulated the self-reported and measured height for 100 men, given the mean, standard deviation, and correlation as detailed in Table 1 of the above analysis, and carried out a paired t-test that compares the difference between the two simulated groups.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>pg.ttest(self_reported, measured, alternative<span class="op">=</span><span class="st">"greater"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">T</th>
<th data-quarto-table-cell-role="th">dof</th>
<th data-quarto-table-cell-role="th">alternative</th>
<th data-quarto-table-cell-role="th">p-val</th>
<th data-quarto-table-cell-role="th">CI95%</th>
<th data-quarto-table-cell-role="th">cohen-d</th>
<th data-quarto-table-cell-role="th">BF10</th>
<th data-quarto-table-cell-role="th">power</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">T-test</td>
<td>11.5537</td>
<td>198</td>
<td>greater</td>
<td>3.089721e-24</td>
<td>[1.32, inf]</td>
<td>1.63394</td>
<td>9.509e+20</td>
<td>1.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The difference between the self-reported and measured heights has a p-value of 0.00719, suggesting there is a 0.7% probability of observing a t-statistic at least as extreme as we observe here, if the data was generated by chance and if the assumptions of our statistical model are valid.</p>
<p>The estimated difference between self-reported height and measured height is 1.4cm, with about 1cm difference from our estimate and our lower and upper confidence intervals, which suggests that the difference between self-reported height for men is larger than their measured heights. However, is 1.4cm significant enough to really care? I mean, none of this is important enough (or valid enough) to really care, but if we suspend disbelief for a moment, is a 1.4cm difference substantively meaningful? I’ll leave that up to you to decide.</p>
</section>
<section id="two-samples-t-test" class="level4" data-number="5.4.1.3">
<h4 data-number="5.4.1.3" class="anchored" data-anchor-id="two-samples-t-test"><span class="header-section-number">5.4.1.3</span> Two Samples T-Test</h4>
<p>So we’ve established a very real, totally scientific over-reporting of male height across the UK, and some clever, entrepreneurial, and not at all exploitative pharmaceutical company has had the good sense to develop a medication that gives you an extra couple of inches in height.</p>
<p>In order to prove that it works it needs to go through testing (because we are particularly reckless, we don’t really care if it is safe in this example). An experiment is designed, with a treatment and control group that is representative of the wider male population, and the wonder drug is given to the treatment group.</p>
<p>We can compare the change in height for the two groups in the 6 months after the treatment group have taken the medication, hypothesising that the drug has had a positive effect on the treatment group, helping them grow by a whopping 1.5cm! This is compared against the average increase in height for the control group that is approximately zero, with a very small standard deviation. A two-sample t-test can help us shed some light on whether we can be confident that this difference did not occur by chance.</p>
<p>We simulate our treatment and control groups, with 20 observations in each, with the treatment group having a mean growth of 3cm and standard deviation of 3cm, and the control group having a mean growth of 0cm and a standard deviation of 0.5cm.</p>
<div id="simulate-height-experiment" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create an empty list for the simulated data</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for loop to simulate 300 not bald observations</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">300</span>):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Not Bald"</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.039</span>, <span class="fl">0.594</span>, <span class="fl">0.367</span>]</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># collect all simulated variables into a dictionary</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>  not_bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># append dictionary to data</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>  data.append(not_bald)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co"># create pandas dataframe from simulated data</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>baldness_survey <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># draw samples from normal distribution</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>treatment, control <span class="op">=</span> np.random.normal(<span class="dv">3</span>, <span class="fl">.5</span>, <span class="dv">20</span>), np.random.normal(<span class="dv">0</span>, <span class="fl">.5</span>, <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Having simulated our treatment and control groups, we can compute a t-test to see if there is a difference in how much each group has grown, on average.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>pg.ttest(x<span class="op">=</span>treatment, y<span class="op">=</span>control, paired<span class="op">=</span><span class="va">False</span>, alternative<span class="op">=</span><span class="st">'greater'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="height-experiment" class="cell-output cell-output-display" data-execution_count="10">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">T</th>
<th data-quarto-table-cell-role="th">dof</th>
<th data-quarto-table-cell-role="th">alternative</th>
<th data-quarto-table-cell-role="th">p-val</th>
<th data-quarto-table-cell-role="th">CI95%</th>
<th data-quarto-table-cell-role="th">cohen-d</th>
<th data-quarto-table-cell-role="th">BF10</th>
<th data-quarto-table-cell-role="th">power</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">T-test</td>
<td>19.437624</td>
<td>38</td>
<td>greater</td>
<td>1.214921e-21</td>
<td>[2.66, inf]</td>
<td>6.146716</td>
<td>1.766e+18</td>
<td>1.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The probability that we would observe a test statistic as large or larger than observed here is very, very small, and the results of the t-test estimate that the difference between the two groups is a little bit less than 4cm, with a lower confidence interval of just under 3cm, suggesting that this medication is having a positive difference on height in the treatment group!</p>
</section>
</section>
<section id="chi-squared-tests" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="chi-squared-tests"><span class="header-section-number">5.4.2</span> Chi-Squared Tests</h3>
<p>Although there are many ways that you may encounter chi-squared (<span class="math inline">\(\chi^2\)</span>) tests, the two most common are tests of independence and a goodness of fit tests. The chi-squared test of independence is a test of the association between two categorical variables, meaning whether the variables are related or not. It is testing whether they vary independently of each other. A chi-squared goodness of fit test is used to examine how well a theoretical distribution fits the distribution of a categorical variable. We will focus on the test of independence here, because I think this is the most common use case.</p>
<p>A common use-case for chi-squared tests of independence is survey data, where individuals from different groups respond to a survey question that offers a finite number of discrete answers, like a scale from good to bad. A good example of categorical variables used in survey data is the impact of male baldness on 46 year old men, carried out by <span class="citation" data-cites="sinikumpu2021">Sinikumpu et al. (<a href="references.html#ref-sinikumpu2021" role="doc-biblioref">2021</a>)</span>. In this survey they grouped men by levels of baldness, and asked them various questions about their lives, from education to sex-life, to consider the impact that being bald has on men’s lives and well-being.</p>
<p>We will use the education question as the basis for simulating the data for our chi-squared test. The survey splits education-level in to three groups - basic, secondary, and tertiary - while I have simplified the question by turning the levels of baldness into a binary categorical variable.</p>
<div id="simulate-survey" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create an empty list for the simulated data</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for loop to simulate 300 not bald observations</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">300</span>):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Not Bald"</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.039</span>, <span class="fl">0.594</span>, <span class="fl">0.367</span>]</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># collect all simulated variables into a dictionary</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>  not_bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># append dictionary to data</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>  data.append(not_bald)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co"># for loop to simulate 600 bald observations</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">600</span>):</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Bald"</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.03</span>, <span class="fl">0.603</span>, <span class="fl">0.367</span>]</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># collect all simulated variables into a dictionary</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>  bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># append dictionary to data</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>  data.append(bald)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="co"># create pandas dataframe from simulated data</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>baldness_survey <span class="op">=</span> pd.DataFrame(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We have simulated each of the different baldness categories individually, before combining them into a single dataset. I think there should be a more concise way of doing this, but I haven’t wrapped my head round how I should do that yet.</p>
<p>We can visualise the difference between the three education-levels in terms of proportion of baldness, to see if there is an obvious difference.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>sns.histplot(</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> baldness_survey, </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="st">'education'</span>, hue <span class="op">=</span> <span class="st">'hair_status'</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  multiple<span class="op">=</span><span class="st">'fill'</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot labels and title</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Education Level'</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Proportion'</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="hypothesis_testing_py_files/figure-html/visualise-survey-output-1.png" id="visualise-survey" width="808" height="506"></p>
</div>
</div>
<p>There’s a small difference between people with a basic education, but for secondary and tertiary education it seems to be pretty evenly split. Our chi-squared test can test this empirically.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>expected, observed, stats <span class="op">=</span> pg.chi2_independence(baldness_survey, x<span class="op">=</span><span class="st">'education'</span>, y<span class="op">=</span><span class="st">'hair_status'</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>stats.<span class="bu">round</span>(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="chi-sq-test" class="cell-output cell-output-display" data-execution_count="13">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">test</th>
<th data-quarto-table-cell-role="th">lambda</th>
<th data-quarto-table-cell-role="th">chi2</th>
<th data-quarto-table-cell-role="th">dof</th>
<th data-quarto-table-cell-role="th">pval</th>
<th data-quarto-table-cell-role="th">cramer</th>
<th data-quarto-table-cell-role="th">power</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>pearson</td>
<td>1.000</td>
<td>5.075</td>
<td>2.0</td>
<td>0.079</td>
<td>0.075</td>
<td>0.510</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>cressie-read</td>
<td>0.667</td>
<td>5.028</td>
<td>2.0</td>
<td>0.081</td>
<td>0.075</td>
<td>0.506</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>log-likelihood</td>
<td>0.000</td>
<td>4.950</td>
<td>2.0</td>
<td>0.084</td>
<td>0.074</td>
<td>0.499</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>freeman-tukey</td>
<td>-0.500</td>
<td>4.904</td>
<td>2.0</td>
<td>0.086</td>
<td>0.074</td>
<td>0.496</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>mod-log-likelihood</td>
<td>-1.000</td>
<td>4.867</td>
<td>2.0</td>
<td>0.088</td>
<td>0.074</td>
<td>0.492</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>neyman</td>
<td>-2.000</td>
<td>4.821</td>
<td>2.0</td>
<td>0.090</td>
<td>0.073</td>
<td>0.488</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>So our p-value tells us that we should expect to observe data at least extreme as our simulated survey data a little less than 20% of the time, if the null hypothesis that there is no difference in education levels across levels of hair loss is true. This is a smaller p-value than <span class="citation" data-cites="sinikumpu2021">Sinikumpu et al. (<a href="references.html#ref-sinikumpu2021" role="doc-biblioref">2021</a>)</span> observe (0.299), but that will be a consequence of the fact we are simulating the data using probabilistic samples, not replicating their results exactly.</p>
<p>If we were to fudge the numbers a little bit, so as to artificially increase the difference in education levels based on the amount of hair a 46 year old man has, what would our results look like?</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1, empty list `data`:</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: for-loop:</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">300</span>):</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 3: simulate all real-world factors:</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Not Bald"</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.15</span>, <span class="fl">0.55</span>, <span class="fl">0.3</span>]</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 4: accumulate all factors in dictionary `d`:</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  not_bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 5: append `d` to `data`</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>  data.append(not_bald)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: for-loop:</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">600</span>):</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 3: simulate all real-world factors:</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Bald"</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.05</span>, <span class="fl">0.6</span>, <span class="fl">0.35</span>]</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 4: accumulate all factors in dictionary `d`:</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>  bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 5: append `d` to `data`</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>  data.append(bald)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: create the DataFrame (outside of the for-loop)</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>baldness_survey_with_effect <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>expected, observed, stats <span class="op">=</span> pg.chi2_independence(baldness_survey_with_effect, x<span class="op">=</span><span class="st">'education'</span>, y<span class="op">=</span><span class="st">'hair_status'</span>)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>stats.<span class="bu">round</span>(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="add-effect" class="cell-output cell-output-display" data-execution_count="14">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">test</th>
<th data-quarto-table-cell-role="th">lambda</th>
<th data-quarto-table-cell-role="th">chi2</th>
<th data-quarto-table-cell-role="th">dof</th>
<th data-quarto-table-cell-role="th">pval</th>
<th data-quarto-table-cell-role="th">cramer</th>
<th data-quarto-table-cell-role="th">power</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>pearson</td>
<td>1.000</td>
<td>25.238</td>
<td>2.0</td>
<td>0.0</td>
<td>0.167</td>
<td>0.997</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>cressie-read</td>
<td>0.667</td>
<td>24.650</td>
<td>2.0</td>
<td>0.0</td>
<td>0.165</td>
<td>0.996</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>log-likelihood</td>
<td>0.000</td>
<td>23.791</td>
<td>2.0</td>
<td>0.0</td>
<td>0.163</td>
<td>0.995</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>freeman-tukey</td>
<td>-0.500</td>
<td>23.400</td>
<td>2.0</td>
<td>0.0</td>
<td>0.161</td>
<td>0.994</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>mod-log-likelihood</td>
<td>-1.000</td>
<td>23.212</td>
<td>2.0</td>
<td>0.0</td>
<td>0.161</td>
<td>0.994</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>neyman</td>
<td>-2.000</td>
<td>23.406</td>
<td>2.0</td>
<td>0.0</td>
<td>0.161</td>
<td>0.994</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Having increased the number of respondents with a basic education (and in turn decreasing those with secondary and tertiary educations) for the not bald group, our chi-squared test results suggest the data we observe is much more extreme. The p-value for this test is extremely small, but that’s not totally surprising. We did fake our data, after all.</p>
<!-- ### Simulation-Based Statistical Tests -->
<!-- One of the reasons I always struggled with hypothesis testing when first learning about them is the seemingly endless variations of tests, each suited to a slightly different data distribution or context. While you could take on the arduous task of learning as many different tests as possible, it is probably (definitely) better to think in terms of the framework for carrying out a hypothesis test that was detailed earlier, and understanding all tests as generalisations of this framework. -->
<!-- This was an idea pitched by the computer scientist Allen Downey, when he wrote that "there is only one test". He's right. This framework for understanding all hypothesis tests as variations on the same basic method uses simulated data from permutation or bootstrapping methods to generate the null distribution against which observed data can be evaluated.  -->
<!-- When classical significance testing methods were developed, simulation wasn't possible, due to the absence of computational power. Now that we have high-powered machines that will do everything for us, we don't have to worry about this so much, which makes simulation much more viable. -->
<!-- Downey visualises how this simulation-based workflow for hypothesis testing works using the following diagram: -->
<!-- ![](figures/hypothesis_testing.png) -->
<!-- The `{infer}` workflow for carrying out simulation-based hypothesis tests is designed based on the same principles (`specify()`, `hypothesize()`, `generate()`, `calculate()`, & `visualize()`). This all-in-one process for hypothesis testing is a little more involved than the humble t-test, and the code for computing hypothesis tests using this approach is definitely more verbose. However, the power that this process gives you is that you can easily carry out any hypothesis test once you've learned Downey's framework, and the consistency of the `{infer}` workflow makes it easier to learn how to do this. The `{infer}` documentation also provides a [pretty exhaustive list][{infer} Examples] of how to compute hypothesis tests under a variety of conditions. -->
<!-- We can see `{infer}` in action by recalculating our IQ scores t-test that we carried out earlier on. Here is what that looks like using simulation-based methods. -->
<!-- https://allendowney.github.io/ElementsOfDataScience/13_hypothesis.html  -->
<!-- ```{python} -->
<!-- # compute test statistic -->
<!-- np.mean(larger_sample) -->
<!-- # generate null distribution -->
<!-- null = np.random.normal(100, sd, 1000) -->
<!-- ``` -->
<!-- ```{python} -->
<!-- np.sum(null < np.mean(larger_sample)) / len(null) -->
<!-- ``` -->
<!-- ```{python} -->
<!-- #| label: visualise-sample -->
<!-- fig, ax = plt.subplots() -->
<!-- sns.histplot(x=null, binwidth=5) -->
<!-- plt.axvline(x=np.mean(larger_sample), color = "red") -->
<!-- # set plot labels and title -->
<!-- plt.xlabel('IQ Score') -->
<!-- plt.ylabel('') -->
<!-- sns.despine() -->
<!-- plt.show() -->
<!-- ``` -->
<!-- ```{python} -->
<!-- def get_bootstrap(data_column_1, data_column_2, iterations=15000, statistic=np.mean, sign_level=0.05): -->
<!--   boot_len = max(len(data_column_1), len(data_column_2)) -->
<!--   boot_data, samples_1_a, samples_2_a = [], [], [] -->
<!--   # Bootstrap sampling -->
<!--   for _ in tqdm(range(iterations)): -->
<!--     sample_1, sample_2 = data_column_1.sample(boot_len, replace=True).values, data_column_2.sample(boot_len, replace=True).values -->
<!--     boot_data.append(statistic(sample_2 - sample_1)) -->
<!--     samples_1_a.append(np.mean(sample_1)) -->
<!--     samples_2_a.append(np.mean(sample_2)) -->
<!--     # Calculate quantiles -->
<!--     quants = pd.DataFrame(boot_data).quantile([sign_level / 2, 1 - sign_level / 2]) -->
<!--     # Calculate p-values -->
<!--     boot_mean, boot_std = np.mean(boot_data), np.std(boot_data) -->
<!--     p_1 = norm.cdf(0, boot_mean, boot_std) -->
<!--     p_2 = norm.cdf(0, -boot_mean, boot_std) -->
<!--     p_value = min(p_1, p_2) * 2 -->
<!--     # Plotting -->
<!--     fig, axes = plt.subplots(2, 1, figsize=(10, 10), dpi=100) -->
<!--     # Plot 1 -->
<!--     axes[0].grid(True, linestyle='-', linewidth=0.7, color='lightgrey') -->
<!--     axes[0].set_facecolor('white') -->
<!--     n, _, bars = axes[0].hist(boot_data, bins=50) -->
<!--     for bar in bars: -->
<!--       if bar.get_x() <= quants.iloc[0][0] or bar.get_x() >= quants.iloc[1][0]: -->
<!--         bar.set_facecolor('palevioletred') -->
<!--       else: -->
<!--         bar.set_facecolor('deepskyblue') -->
<!--         bar.set_edgecolor('purple') -->
<!--     axes[0].vlines(boot_mean, ymin=0, ymax=max(n),linestyle='-', color='purple',linewidth=0.7)    -->
<!--     axes[0].vlines(quants,ymin=0,ymax=max(n),linestyle='--', color='purple',linewidth=0.7) -->
<!--     axes[0].annotate(f"{quants.iloc[0][0]:.3f}", xy=(quants.iloc[0][0], 0), xytext=(quants.iloc[0][0]+.001, max(n)/2)) -->
<!--     axes[0].annotate(f"{quants.iloc[1][0]:.3f}", xy=(quants.iloc[1][0], 0), xytext=(quants.iloc[1][0]+.001, max(n)/2)) -->
<!--     axes[0].set(xlabel='boot_data', ylabel='Frequency', title=f'Distribution of differences in {statistic.__name__}s') -->
<!--     # Plot 2 -->
<!--     axes[1].grid(True, linestyle='-', linewidth=0.7, color='lightgrey') -->
<!--     axes[1].set_facecolor('white') -->
<!--     axes[1].hist(samples_1_a, bins=50, color='deepskyblue', alpha=0.5, label=f"{statistic.__name__}s of boot dataset 1") -->
<!--     axes[1].hist(samples_2_a, bins=50, color='crimson', alpha=0.5, label=f"{statistic.__name__}s of boot dataset 2") -->
<!--     axes[1].set(ylabel='Frequency') -->
<!--     axes[1].legend() -->
<!--     plt.subplots_adjust(hspace=0.2) -->
<!--     # Result -->
<!--     conclusion = "reject" if p_value <= sign_level else "fail to reject" -->
<!--     print(f"The p-value of {np.round(p_value, 6)} suggests that there is evidence to {conclusion} the null hypothesis at the {sign_level} significance level.") -->
<!--     return {'quants': quants, 'boot mean': boot_mean, 'p_value': p_value} -->
<!-- ``` -->
<!-- ```{r} -->
<!-- #| label: high-achievers-sim-test -->
<!-- # compute test statistic -->
<!-- test_statistic <-  -->
<!--   larger_sample |>  -->
<!--   specify(response = score) |>  -->
<!--   calculate(stat = "mean") -->
<!-- # generate null distribution -->
<!-- null_distribution <-  -->
<!--   larger_sample |> -->
<!--   specify(response = score) |>  -->
<!--   hypothesize(null = "point", mu = 100) |>  -->
<!--   generate(reps = 1000, type = "bootstrap") |>  -->
<!--   calculate(stat = "mean") -->
<!-- # calculate the p-value of observing a test statistic at least as extreme as  -->
<!-- # our observed statistic if the data was generated by chance -->
<!-- p_value <-  -->
<!--   null_distribution |> -->
<!--   get_p_value(obs_stat = test_statistic, direction = "greater") -->
<!-- # visualise the test statistic against the null -->
<!-- null_distribution |> -->
<!--   visualize() +  -->
<!--   shade_p_value(test_statistic, direction = NULL, color = "#00A499") + -->
<!--   geom_hline(yintercept = 0, colour = "#333333", linewidth = 1) + -->
<!--   annotate( -->
<!--     "text", x = 95, y = 125, -->
<!--     label = paste0("t = ", round(test_statistic, 2), "\n p = ", p_value), -->
<!--     size = rel(8), color="grey30" -->
<!--     ) + -->
<!--   labs(x = "IQ Score", y = NULL, title = NULL) -->
<!-- ``` -->
<!-- I think that one of the main strengths of this approach is the intuition it gives us for how hypothesis testing really works. By going through the process step-by-step, we can see what we are doing more easily. We compute our test statistic, which is some summary measure of the data (in this case the mean value), before simulating a null distribution under the assumption that there is no difference between our high achievers and the population. We do this by specifying the data we are testing (the response variable), stating our null hypothesis (that we are carrying out a test using a point estimate, and that the mean of the null is 100), and generating a distribution by taking 1000 bootstrap samples (where sample size is equal to our input and samples are drawn from our data with replacement).  -->
<!-- Having built a null distribution, we can compare our test statistic against that null and see how extreme it appears to be if the assumption that the null hypothesis is true is correct. We can quantify this by computing a p-value, which is just a measure of the proportion of values in our null distribution that are at least as extreme as the test statistic. In the above example, because we are carrying out a one-tailed test (`direction = "greater"`), we would just need to filter the null distribution for values equal to or greater than our sample mean. If we were carrying out a two-tailed test, we would have to calculate the absolute distance of all values from the null mean, and then filter for all values equal to or greater than the difference between our sample mean and the null mean. -->
<!-- To further illustrate this workflow, below is the chi-squared test equivalent. -->
<!-- ```{r} -->
<!-- #| label: chi-squared-sim-test -->
<!-- # calculate test statistic -->
<!-- chisq_statistic <-  -->
<!--   baldness_survey |>  -->
<!--   specify(education ~ hair_status) |>  -->
<!--   hypothesize(null = "independence") |>  -->
<!--   calculate(stat = "Chisq") -->
<!-- # generate null distribution -->
<!-- chisq_null <-  -->
<!--   baldness_survey |>  -->
<!--   specify(education ~ hair_status) |>  -->
<!--   hypothesize(null = "independence") |>  -->
<!--   generate(reps = 1000, type = "permute") |>  -->
<!--   calculate(stat = "Chisq") -->
<!-- # calculate the p value from the test statistic and null distribution -->
<!-- chisq_p <- -->
<!--   chisq_null |>  -->
<!--   get_p_value(chisq_statistic, direction = "greater") -->
<!-- # visualize the null distribution and test statistic -->
<!-- chisq_null |>  -->
<!--   visualize() +  -->
<!--   shade_p_value(chisq_statistic, direction = NULL, color = "#00A499") + -->
<!--   geom_hline(yintercept = 0, colour = "#333333", linewidth = 1) + -->
<!--   annotate( -->
<!--     "text", x = 7.5, y = 250, -->
<!--     label =  -->
<!--       paste0("χ2 = ", round(chisq_statistic$stat, 2), -->
<!--              "\n p = ", chisq_p), -->
<!--     size = rel(8), color="grey30" -->
<!--     ) + -->
<!--   labs(x = "Chi-Squared Statistic", y = NULL, title = NULL) -->
<!-- ``` -->
<!-- As discussed above, the simulation-based approach to hypothesis testing involves carrying out the processes in the hypothesis test more explicitly, and this helps us gain intuition for what we are doing. This also forces us to think about how we specify and compute our test. It's pretty easy to build a t-test without really getting as far as actually checking if it is the right approach, and you don't need to think long and hard about exactly what is going on when you run a t-test. Downey's "There is only one test" framework, and the `{infer}` implementation, forces you to think about what you are doing and to be a little more deliberate about your choices. -->
</section>
</section>
<section id="moving-beyond-testing" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="moving-beyond-testing"><span class="header-section-number">5.5</span> Moving Beyond Testing</h2>
<p>Hypothesis testing is still a useful tool when we are looking to make conclusions about data without overinterpreting noise <span class="citation" data-cites="gelman2020">(<a href="references.html#ref-gelman2020" role="doc-biblioref">Gelman, Hill, and Vehtari 2020</a>)</span>. Simple statistical tests of difference between groups or differences from a hypothesised value have plenty of utility, especially in industry, where getting reasonably reliable results quickly can often be more valuable than taking more time to get more precision.</p>
<p>However, where more precision is needed, a different approach may be necessary. Testing a test hypothesis can only give us some sense of the degree to which our data is consistent with the hypothesis. This may move us closer to being able to make inferences, especially if the testing approach is robust, and sound scientific models have been developed for which the hypothesis test is a reasonable statistical representation, but answering meaningful questions usually involves trying to make some assertions not just about the existence of an effect or a difference, but also quantifying it too. Where this is the goal, estimation is a better choice. The estimation approach to statistical inference does not rely on a hypothesis, and instead focuses on a “quantity of interest”, for which we use the data to estimate <span class="citation" data-cites="prytherch2022">(<a href="references.html#ref-prytherch2022" role="doc-biblioref">Prytherch 2022</a>)</span>. While testing tries to give us a sense of whether or not what we observe in our sample is likely to exist in the population, the estimation approach tries to measure the effect size itself.</p>
<p>Although estimating an effect size is a good start, it is also important to quantify the precision of that estimate as well. Estimating the size of an effect, a point estimate, is inherently uncertain. This uncertainty can be a strength, rather than a weakness. By incorporating uncertainty in to our estimates, we can say more about what our sample data is telling us, and we are better able to make inferences about the population.</p>
<p>Further, by acknowledging the uncertainty in the statistical modelling process, not just about the model’s point estimates, the process should become a more honest representation of the quantity of interest. <span class="citation" data-cites="greenland2016">Greenland et al. (<a href="references.html#ref-greenland2016" role="doc-biblioref">2016</a>)</span> argue that the goal of statistical analysis is to evaluate the certainty of an effect size, and acknowledging and embracing that uncertainty should include every aspect of the process, including the conclusions we draw and the way we present our findings.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Further Details
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="estimate-meaningful-quantities" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="estimate-meaningful-quantities">Estimate Meaningful Quantities</h4>
<p>This is, I believe, the most important suggestion made in this chapter. I am borrowing a phrase used by the UNC Epidemiologist, Charles Poole, in a talk he gave about the value of p-values. He states that we should “stop testing null hypotheses, start estimating meaningful quantities” <span class="citation" data-cites="poole2022">(<a href="references.html#ref-poole2022" role="doc-biblioref">Poole 2022</a>)</span>. His (very firmly-held) position is that there is no reforming null hypothesis significance testing. Instead, our focus should be on estimation. <span class="citation" data-cites="poole2022">Poole (<a href="references.html#ref-poole2022" role="doc-biblioref">2022</a>)</span> argues that wherever significance testing is of some utility, we should instead frame the results in terms of substantive results and estimation of effect size. This would still do what hypothesis testing is attempting to do, but it should be more robust (if done properly), and the framing itself offers a lot more.</p>
<p><span class="citation" data-cites="gelman2020">Gelman, Hill, and Vehtari (<a href="references.html#ref-gelman2020" role="doc-biblioref">2020</a>)</span> argue that we are generally capable of being interested in issues where effects exist, and that the presence of an effect is therefore not entirely interesting, in and of itself. Therefore, we should concern ourselves with the things that do matter, like the size of the effect, not the existence of the effect itself.</p>
<p>Although I’m not going to take quite as strong a position as <span class="citation" data-cites="poole2022">Poole (<a href="references.html#ref-poole2022" role="doc-biblioref">2022</a>)</span> or <span class="citation" data-cites="gelman2020">Gelman, Hill, and Vehtari (<a href="references.html#ref-gelman2020" role="doc-biblioref">2020</a>)</span>, I do think his position is pretty much correct. I am of the view that hypothesis testing is generally a sub-optimal approach when trying to better understand phenomena using data. I think there are still plenty of situations where a hypothesis test is sufficient, and in industry where there is often value to providing functionally useful (if relatively imprecise) conclusions quickly, it is reasonable to turn to testing. However, there are lots of situations where an estimate of the effect size, and the precision of that estimate, are both very valuable!</p>
<p>It is rare that we should only care that the effect exists. Generally speaking, this should be a prerequisite to the more important question of what the magnitude and direction of that effect is. Even where the knowledge of the existence of an effect is sufficient, it is hard for me to imagine a situation where knowing the magnitude and direction of the effect wouldn’t be useful, and wouldn’t allow us to estimate our confidence in that effect more effectively.</p>
<p>Taking an estimation approach to statistical inference doesn’t necessarily alter the statistical methods used (though more on this at the end of this chapter), but it does require our adjusting how we approach these methods and what we look to get out of them.</p>
<p>Estimation is not specifically about method. If statistical tests are used, then it requires taking the estimate of the effect size, or calculating it if the context means that any output does not offer an intuitive interpretation of effect. Other methods, like regression, can give a more direct measure of effect size (though there are plenty of regression models that also require work to produce more interpretable effect estimates).</p>
<p>The important change in approach is to focus on the importance of the estimated effect size, not on the p-value, or statistical significance!</p>
</section>
<section id="embrace-uncertainty" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="embrace-uncertainty">Embrace Uncertainty</h4>
<p>When we move away from testing null hypotheses and focus instead on the estimation of effect sizes, we go from testing for the existence of an effect to estimating the magnitude (effect size) and the direction (whether the effect is positive or negative). However, a key part of this that is missing is the precision with which we are able to estimate the effect. It’s important to remember, in any statistical modelling, that we are estimating quantities, and therefore any point estimate is just our best effort at quantifying an unknown population parameter. The point estimate is probably wrong! It is important that we acknowledge the uncertainty in our estimations and account for them in the way we present model outputs<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>.</p>
<p>However, the embrace of uncertainty is not limited to our uncertainty in our estimates. It is a wider issue. We have to accept that every part of the process of estimating meaningful quantities is done with a degree of uncertainty, and as such the way we think about statistical modelling and the way we present our results should account for the existence of variance. Uncertainty exists everywhere in statistical analysis and inference <span class="citation" data-cites="wasserstein2019">(<a href="references.html#ref-wasserstein2019" role="doc-biblioref">Wasserstein, Schirm, and Lazar 2019</a>)</span>.</p>
<p><span class="citation" data-cites="wasserstein2019">Wasserstein, Schirm, and Lazar (<a href="references.html#ref-wasserstein2019" role="doc-biblioref">2019</a>)</span> describe the use of “significance tests and dichotomized p-values” as an attempt by some practitioners to avoid dealing with uncertainty by escaping to a more simplistic world where results are either statistically significant or not. Perhaps a little uncharitable (though in an academic context I am more inclined to agree), but we’ve already discussed the importance of moving away from significance testing, so we should complete the journey and confront uncertainty head-on! We have to accept that findings are more uncertain than is often acknowledged. Our embrace of uncertainty should help us to seek out better methods, apply the necessary amount of care and critical thinking to our work, and draw inferences with the appropriate amount of confidence.</p>
<p>One way of acknowledging and working with this uncertainty is the use of confidence intervals when estimating effect sizes. Confidence intervals estimate a range of values that would be compatible with the data, given all of the statistical model’s assumptions are met. While confidence intervals are ultimately estimated from the p-value, and the typical 95% confidence interval corresponds to the p &gt; .05 threshold that I have warned against, they are useful because they frame results in terms of effect sizes, and they incorporate uncertainty in our estimates, helping to quantify the precision with which we are able to estimate effect size.</p>
<p>Confidence intervals are a step in the right direction, because point estimates, while they are our best shot at the quantity of interest, are highly likely to be wrong. Estimating the interval within which we would expect the quantity of interest to fall, acknowledges that uncertainty, and there’s a better chance that the population parameter falls within the estimated range than bang on the point estimate. However, it’s important that we don’t treat confidence intervals as the fix for all uncertainty! <span class="citation" data-cites="gelman2016">Gelman (<a href="references.html#ref-gelman2016" role="doc-biblioref">2016</a>)</span> states that “the solution is not to reform p-values or to replace them with some other statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty and embracing of variation”.</p>
<p>Ultimately, confidence intervals are valuable when estimating meaningful quantities, but there are very far from sufficient when it comes to embracing uncertainty. We have to acknowledge the uncertainty in the entire statistical modelling process, both when we are making decisions about that modelling process and when we present our findings to others.</p>
</section>
</div>
</div>
</div>
</section>
<section id="next-steps" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="next-steps"><span class="header-section-number">5.6</span> Next Steps</h2>
<p>The goal for this chapter is that readers come away with an understanding of the significant limitations of null hypothesis significance testing and the more robust approaches to testing hypotheses, and the alternative approach of estimating meaningful effects. I hope that this chapter has helped set people up reasonably well when carrying out tests. Equally, I hope that they will consider the alternatives, recognising when it is defensible to carry out a statistical test and when estimating the effect size is necessary.</p>
<p>I have taken a slightly more forgiving position than many of the statisticians that I respect, and from which I have learned so much, but I have deliberately done so because I think industry has slightly different incentives, and therefore slightly different requirements. I think that there are situations where the use of hypothesis testing is sufficient and defensible, and those situations are much more common in industry. The criticisms remain perfectly valid, but I hope I have given readers the tools to carry out more robust testing, and a good understanding of when to choose estimation instead, where necessary.</p>
<p>While it is possible to use statistical tests for estimation, I also believe that the big wide world of estimation is better served by regression models. I think regression modelling is a better starting point than statistical tests because the structure of regression modelling places more emphasis on the effect size. Further, estimating effect sizes does not rule out the use of statistical tests, but it does require redirecting our focus and making sure to treat the magnitude and direction of effect size as the most important part of the analysis. The premise of statistical tests is that they test for the existence of an effect, and while packages like {infer} frame test outputs in a way that makes using statistical tests for these purposes, it does still require a little work to think about about the results of statistical tests in these terms. Further, I think that regression modelling gives a lot more flexibility when we are attempting to estimate an effect as precisely as possible, and the process and resulting outputs of a regression model lend themselves more naturally to a focus that steers away from hypothesis testing and towards estimating effects.</p>
<p>If it hasn’t been made clear before, I think it is important to stress the distinction between the null hypothesis testing framework and the statistical tests used to carry them out. The tests themselves are not really the problem. However, they are closely tied to null hypothesis testing. Ultimately, the statistical tests discussed in this chapter (and many more) are just special cases of linear models <span class="citation" data-cites="lindelov2019">(<a href="references.html#ref-lindelov2019" role="doc-biblioref">Lindeløv 2019</a>)</span>, and the results in both will be identical <span class="citation" data-cites="zablotski2019">(<a href="references.html#ref-zablotski2019" role="doc-biblioref">Zablotski 2019</a>)</span> (if not the way the outputs are presented), so the same things can be achieved with either approach, but regression modelling offers more flexibility, allows for the layering of complexity/precision in estimates, and the way regression outputs are framed are generally a little more balanced. Treating regressions as the first point of call for inferential statistics is not a necessary prerequisite for “doing things the right way”, but doing things the right way becomes easier, because it is easier to frame everything in terms of the magnitude and direction of effects.</p>
<p>The good news? The next chapter is about modelling linear effects using regression!</p>
</section>
<section id="resources" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="resources"><span class="header-section-number">5.7</span> Resources</h2>
<ul>
<li><p><a href="https://infer.tidymodels.org/">{infer}</a></p></li>
<li><p><a href="https://seeing-theory.brown.edu/">Seeing Theory</a></p></li>
<li><p><a href="https://www.jwilber.me/permutationtest/">The Permutation Test</a></p></li>
<li><p><a href="http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html">There is Only One Test</a></p></li>
<li><p><a href="http://allendowney.blogspot.com/2011/06/more-hypotheses-less-trivia.html">More Hypotheses, Less Trivia</a></p></li>
<li><p><a href="https://thomasleeper.com/Rcourse/Tutorials/permutationtests.html">Permutation Tests</a></p></li>
<li><p><a href="https://medium.com/analytics-vidhya/permutation-test-as-an-alternative-to-two-sample-t-test-using-r-9f5da921bc95">Permutation Test as an Alternative to Two-Sample T-Test Using R</a></p></li>
<li><p><a href="https://deliveroo.engineering/2018/12/07/monte-carlo-power-analysis.html">Power Analysis</a></p></li>
<li><p><a href="https://hookedondata.org/posts/2023-04-19-recommended-resources-for-ab-testing">Recommended Resources for A/B Testing</a></p></li>
<li><p><a href="https://elevanth.org/blog/2023/07/17/none-of-the-above/">McElreath - None of the Above</a></p></li>
<li><p><a href="https://nyu-cdsc.github.io/learningr/assets/simulation.pdf">Introduction to Scientific Programming and Simulations in R</a></p></li>
<li><p><a href="http://allendowney.blogspot.com/2015/03/statistical-inference-is-only-mostly.html">Statistical Inference is Only Mostly Wrong</a></p></li>
<li><p><a href="http://allendowney.blogspot.com/2015/05/hypothesis-testing-is-only-mostly.html">Hypothesis Testing is Only Mostly Useless</a></p></li>
<li><p><a href="https://matthew-brett.github.io/cfd2020/permutation/permutation_idea.html">The Idea of Permutation</a></p></li>
<li><p><a href="https://matthew-brett.github.io/cfd2020/permutation/permutation_and_t_test.html">Coding for Data: Permutation and the T-Test</a></p></li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-aschwanden2015" class="csl-entry" role="listitem">
Aschwanden, Christie. 2015. <span>“Science Isn’t Broken: It’s Just a Hell of a Lot Harder Than We Give It Credit For.”</span> FiveThirtyEight. <a href="https://fivethirtyeight.com/features/science-isnt-broken/">https://fivethirtyeight.com/features/science-isnt-broken/</a>.
</div>
<div id="ref-aschwanden2016" class="csl-entry" role="listitem">
———. 2016. <span>“Statisticians Found One Thing They Can Agree on: It’s Time to Stop Misusing p-Values.”</span> FiveThirtyEight. <a href="https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/">https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/</a>.
</div>
<div id="ref-blackwell2023" class="csl-entry" role="listitem">
Blackwell, Matthew. 2023. <span>“A User’s Guide to Statistical Inference and Regression.”</span> <a href="https://mattblackwell.github.io/gov2002-book/">https://mattblackwell.github.io/gov2002-book/</a>.
</div>
<div id="ref-fisher1956" class="csl-entry" role="listitem">
Fisher, Ronald A. 1956. <span>“Statistical Methods and Scientific Inference.”</span>
</div>
<div id="ref-gelman2016" class="csl-entry" role="listitem">
Gelman, Andrew. 2016. <span>“The Problems with p-Values Are Not Just with p-Values.”</span> <em>The American Statistician</em> 70. <a href="http://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108">http://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108</a>.
</div>
<div id="ref-gelman2020" class="csl-entry" role="listitem">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari Vehtari. 2020. <em>Regression and Other Stories</em>. Cambridge University Press.
</div>
<div id="ref-gelman2021" class="csl-entry" role="listitem">
Gelman, Andrew, and Eric Loken. 2021. <span>“The Statistical Crisis in Science.”</span> <em>American Scientist</em> 102 (6): 460. <a href="https://doi.org/10.1511/2014.111.460">https://doi.org/10.1511/2014.111.460</a>.
</div>
<div id="ref-gelman2006" class="csl-entry" role="listitem">
Gelman, Andrew, and Hal Stern. 2006. <span>“The Difference Between "Significant" and "Not Significant" Is Not Itself Statistically Significant.”</span> <em>The American Statistician</em> 60 (4): 328–31.
</div>
<div id="ref-greenland2016" class="csl-entry" role="listitem">
Greenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. <span>“Statistical Tests, p-Values, Confidence Intervals, and Power: A Guide to Misinterpretations.”</span> <em>European Journal of Epidemiology</em> 31: 337–50.
</div>
<div id="ref-lakens2022" class="csl-entry" role="listitem">
Lakens, Daniel. 2022. <em>Improving Your Statistical Inferences</em>. <a href="https://doi.org/10.5281/zenodo.6409077">https://doi.org/10.5281/zenodo.6409077</a>.
</div>
<div id="ref-lindelov2019" class="csl-entry" role="listitem">
Lindeløv, Jonas Kristoffer. 2019. <span>“Common Statistical Tests Are Linear Models.”</span> <a href="https://lindeloev.github.io/tests-as-linear/">https://lindeloev.github.io/tests-as-linear/</a>.
</div>
<div id="ref-mcelreath2023" class="csl-entry" role="listitem">
McElreath, Richard. 2023. <span>“None of the Above.”</span> <a href="https://elevanth.org/blog/2023/07/17/none-of-the-above/">https://elevanth.org/blog/2023/07/17/none-of-the-above/</a>.
</div>
<div id="ref-poole2022" class="csl-entry" role="listitem">
Poole, Charles. 2022. <span>“The Statistical Arc of Epidemiology.”</span> Presented at the "What is the Value of the P-Value?" Panel Discussion, Cosponsored by UNC TraCS, Duke University, and Wake Forest University CTSA Biostatistics, Epidemiology and Research Design (BERD) Cores.
</div>
<div id="ref-popper1935" class="csl-entry" role="listitem">
Popper, Karl. 1935. <em>The Logic of Scientific Discovery</em>. Routledge.
</div>
<div id="ref-prytherch2022" class="csl-entry" role="listitem">
Prytherch, Ben. 2022. <em>DSCI 335: Inferential Reasoning in Data Analysis</em>. <a href="https://bookdown.org/csu_statistics/dsci_335_spring_2022/">https://bookdown.org/csu_statistics/dsci_335_spring_2022/</a>.
</div>
<div id="ref-sinikumpu2021" class="csl-entry" role="listitem">
Sinikumpu, Suvi-Päivikki, Jari Jokelainen, Juha Auvinena, Markku Timonen, and Laura Huilaja. 2021. <span>“Association Between Psychosocial Distress, Sexual Disorders, Self-Esteem and Quality of Life with Male Androgenetic Alopecia: A Population-Based Study with Men at Age 46.”</span> <em>BMJ Open</em> 11 (12).
</div>
<div id="ref-wasserstein2016" class="csl-entry" role="listitem">
Wasserstein, Ronald L., and Nicole A. Lazar. 2016. <span>“The ASA Statement on p-Values: Context, Process, and Purpose.”</span> <em>The American Statistician</em> 70 (2): 129–33. <a href="https://doi.org/10.1080/00031305.2016.1154108">https://doi.org/10.1080/00031305.2016.1154108</a>.
</div>
<div id="ref-wasserstein2019" class="csl-entry" role="listitem">
Wasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019. <span>“Moving to a World Beyond "p &lt; 0.05".”</span> <em>The American Statistician</em> 73 (sup1): 1–19. <a href="https://doi.org/10.1080/00031305.2019.1583913">https://doi.org/10.1080/00031305.2019.1583913</a>.
</div>
<div id="ref-zablotski2019" class="csl-entry" role="listitem">
Zablotski, Yury. 2019. <span>“Statistical Tests Vs. Linear Regression.”</span> <a href="https://yury-zablotski.netlify.app/post/statistical-tests-vs-linear-regression/">https://yury-zablotski.netlify.app/post/statistical-tests-vs-linear-regression/</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>I could stubbornly refuse to write about hypothesis tests, like covering my eyes and ears and screaming as someone asks me how to do a t-test properly, but something tells me that isn’t going to stop them happening. I would rather give someone all the necessary resources (or at least as much as is possible in a relatively limited guide like this), and hope it helps them improve the way they go about asking questions of data.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This is in contrast to academia, where it is rare that anything other than precision will matter.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I won’t discuss all 25 misinterpretations that <span class="citation" data-cites="greenland2016">Greenland et al. (<a href="references.html#ref-greenland2016" role="doc-biblioref">2016</a>)</span> discuss, because that would be overkill here. For anyone that is interested in learning more about the problems with p-values, I’d recommend going straight to the source and reading their paper.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The probability that a p-value represents are specific to the statistical model we are specifying. It’s important to remember that violations of our assumptions in specifying our model can bias the p-value and invalidate our results.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>I’m assuming that anyone following this advice has good intentions, and is not looking to cheat their way to the results they want. Someone wanting to achieve this is going to find a way to do it regardless.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>A scientific hypothesis reflects a theory about the world (for example, Drug <span class="math inline">\(X\)</span> decreases the risk of Disease <span class="math inline">\(Y\)</span>) while a statistical hypothesis reflect what is observed in the data (A one-unit increase in <span class="math inline">\(x\)</span> will correspond with a one-unit decrease in <span class="math inline">\(y\)</span>).<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>It could be a hypothesis that captures a consequence of a scientific hypothesis being true (as opposed to false, in the case of the null hypothesis), or it can test two scientific hypotheses against each other (instead of one against the possibility of nothing going on) <span class="citation" data-cites="prytherch2022">(<a href="references.html#ref-prytherch2022" role="doc-biblioref">Prytherch 2022</a>)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>If you are unclear on some of the foundations that underpin what I’m discussing here, I’d recommend starting with StatQuest’s <a href="https://www.youtube.com/playlist?list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9">Statistics Fundamentals</a> YouTube playlist, and for a more detailed introduction, I would give <a href="https://crumplab.com/statistics"><em>Answering Questions With Data</em></a> a try.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Standard deviation can be computed from standard error by multiplying the standard error by the square root of the sample size. The formula for this is as follows: <span class="math display">\[\sigma = SE \times \sqrt{n}\]</span><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Using only five observations is less than ideal, but given that what we are doing is just illustrative, this will work fine.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>It is also worth noting that while the guidance here is about the estimation approach to statistical inference, it can also be applied to hypothesis testing too.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./eda_py.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./linear_regression_py.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb16" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Hypothesis Testing {#sec-hypothesis-testing}</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>{{&lt; include _links.qmd &gt;}}</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: setup</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: false</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: 'Setup Code (Click to Expand)'</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co"># packages needed to run the code in this section</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install pandas numpy matplotlib seaborn statsmodel</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># import packages</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pingouin <span class="im">as</span> pg</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> rc</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot style</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot font</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>rc(<span class="st">'font'</span>,<span class="op">**</span>{<span class="st">'family'</span>:<span class="st">'sans-serif'</span>,<span class="st">'sans-serif'</span>:[<span class="st">'Arial'</span>]})</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot colour palette</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>colours <span class="op">=</span> [<span class="st">'#1C355E'</span>, <span class="st">'#00A499'</span>, <span class="st">'#005EB8'</span>]</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>sns.set_palette(sns.color_palette(colours))</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="co"># set random seed</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>This chapter will cover a topic that is well known to many practitioners - hypothesis testing. Hypothesis testing is a popular approach for attempting to make inferences about data without being led astray by noise. Although hypothesis testing remains very popular, it has been much criticised because the most common framework for testing, Null Hypothesis Significance Testing (NHST), has faced increasing criticisms regarding it's external validity. If you have ever come across discussions about the "replication crisis" in science, you will have read about the many issues concerning the use of statistical significance as a measure of evidence in science and the misunderstanding and misuse of p-values. The null hypothesis significance testing framework is another of the central culprits in this crisis.</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>Hypothesis testing may be one of the most prominent tools for statistical inference, but there is plenty of debate about how we should be doing it, and whether the method should be consigned to the history books of statistics, only to be remembered as that silly time in our lives when we used to do science really badly. Here I hope to be able to give readers a clear understanding of how hypothesis testing works, it's biggest drawbacks, and some advice about how to carry out robust tests of hypotheses.</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>Beyond this, I will make a case for moving away from hypothesis testing and towards the estimation of effect sizes. In doing so, I will demonstrate how to carry out some common statistical tests, using the guidelines in this chapter to inform how tests are carried out and how the results are presented. I will attempt to thread the needle between caution and clarity^<span class="co">[</span><span class="ot">I could stubbornly refuse to write about hypothesis tests, like covering my eyes and ears and screaming as someone asks me how to do a t-test properly, but something tells me that isn't going to stop them happening. I would rather give someone all the necessary resources (or at least as much as is possible in a relatively limited guide like this), and hope it helps them improve the way they go about asking questions of data.</span><span class="co">]</span>, but I recognise that this is not easy, and I'm not sure how successfully I will manage it!</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>The compromise that I make here is that this chapter will be like a really terrible Choose Your Own Adventure book, where you choose to take the short route, which lays out a process for carrying out hypothesis testing in a robust manner, or you can tread the long, arduous path through this subject. The intention is that the shorter path should give you everything you need, particularly for anyone that is unfamiliar with hypothesis testing. However, if you have are looking to improve your understanding of hypothesis testing, you can choose to expand the sections with further details.</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Motivation for Hypothesis Testing</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>A hypothesis is a statement about an effect that we expect to observe in our sample data, that is intended to generalise to the population. We do not know what the true effect in the population is, but we have a theory about what it might be, and we can test that theory using a hypothesis about what we will observe in the sample.</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>Our hypothesis might relate to how the mean value of a certain quantity of interest (like height) differs from one group to another, or how we expect the value of one quantity in our sample to change as the value of another quantity changes (like the effect of weight on height). A hypothesis test is a statistical method for empirically evaluating a hypothesis, to determine whether what we observe in our sample data is likely to exist in the population.</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>Hypothesis testing is effectively a statistical method for testing the compatibility of the observed data against a relevant hypothesised effect, often the hypothesis of zero effect. The measure of that incompatibility is a test statistic, which quantifies the distance between the observed data and the expectations of the statistical test, given all the assumptions of the test (including the hypothesised effect size). For example, if testing the hypothesis that there is no effect, a distribution under the assumptions of the statistical test, including the null hypothesis, is generated, and the test statistic measures the distance between the observed data and the null distribution. A p-value is then calculated from this test statistic, representing the probability of a test statistic at least as extreme as the computed test statistic if all the assumptions of the statistical test are true, including the hypothesis that is being tested.</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>The most common approach to testing hypotheses is the Null Hypothesis Significance Testing (NHST) framework, which involves testing the hypothesis that there is no effect (the null hypothesis) and, where it is possible to reject the null, this can be used as evidence that **something** is going on, to which this may help present evidence for the theory developed by the researcher (the alternative hypothesis).</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>Hypothesis testing can be valuable for a variety of different use-cases, but they are not without their issues. Despite those issues, they remain a useful tool when you want to know whether the difference between two groups is real, or whether it's a product of noise, or when you are interested in knowing whether an effect size is different from zero. When you have a need to answer questions with greater precision, I would argue that there is a need to go further than hypothesis testing, but in industry, where speed and simplicity are often of equal importance to precision^<span class="co">[</span><span class="ot">This is in contrast to academia, where it is rare that anything other than precision will matter.</span><span class="co">]</span>, hypothesis testing can serve real value.</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>:::{.callout-note collapse='true'}</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Details</span></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>The goal of NHST is to "adjudicate between two complementary hypotheses" <span class="co">[</span><span class="ot">@blackwell2023</span><span class="co">]</span>, the null hypothesis ($H_0$) and the alternative/research hypothesis ($H_1$). The null hypothesis, which is the hypothesis that no effect exists in the population, is our starting assumption, and we require strong evidence to reject it. If we are able to find strong enough evidence to reject the null, this moves us one step closer to making a case for our alternative hypothesis. </span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>The reasoning for using two complementary hypotheses is based on @popper1935's _The Logic of Scientific Discovery_. Popper argues that it is impossible to confirm a hypothesis, because hypotheses are a form of inductive reasoning, which involves drawing conclusions about a general principle based on specific observations. We cannot confirm a hypothesis because we cannot prove that it is true by generalising from specific instances. However, we _can_ falsify a hypothesis. If what we observe in our sample provides strong evidence that our hypothesis cannot be true, then we can conclude (with a degree of certainty) that the hypothesis is false</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>For a better understanding about the null and alternative hypothesis, I would recommend Josh Starmer's StatQuest videos, embedded below:</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://youtu.be/0oc49DyA3hU?si=wDuNFfgpP4hQml8e &gt;}}</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://youtu.be/5koKb5B_YWo?si=ydyTZguZATS9wehl &gt;}}    </span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>When we carry out a hypothesis test, we are quantifying our confidence that what we observe in the sample did not occur by chance. If we are able to show that a effect is extremely unlikely to have occurred by chance, we can reject the null hypothesis. Rejecting the null doesn't demonstrate that the observed effect can be explained by our theory, however, demonstrating that the observed data does not conform to our expectations given that all of our test's assumptions, including the null hypothesis, are true, moves us one step closer to supporting our theory. </span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Problems With Hypothesis Testing</span></span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>Hypothesis testing serves an important purpose in science, both in academia and in industry. We use hypothesis testing to sift through noise in data to dry and draw conclusions, and this is a perfectly reasonable goal. </span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>However, the dominant framework for testing hypotheses, Null Hypothesis Significance Testing, is rife with issues. The primary problems with NHST are that it defines tests in terms of them being statistically significant or not, based on an arbitrary threshold value for p-values, and it frames p-values as the primary concern when interpreting a statistical model. While p-values are still relevant, they are not the whole story (nor the main part of the story).</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>Further, the importance afforded to p-values in the NHST framework encourages certain common misconceptions about them, and the framing that prioritises them also makes those misconceptions even more harmful. P-values are not the measure of the substantive importance of the effect; they are not the probability that the null hypothesis is true; and finally, a statistically significant p-value does not confirm the alternative hypothesis, just as a non-significant p-values does not confirm the null. They are a measure of how surprising the observed data would be if the hypothesis being tested (like the null hypothesis), and all other assumptions of the statistical test, are true.</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>Although these problems are of real significance in academia, where precision is of the greatest importance, </span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>:::{.callout-note collapse='true'}</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Details</span></span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>There are many complaints and criticisms that can be levelled at hypothesis testing. Here are some of the main ones.</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a><span class="fu">#### P-Values &amp; Statistical Significance {.unnumbered}</span></span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>The biggest problems with NHST relate to the misuse and misunderstanding of p-values, and their dichotomisation in order to make arbitrary judgements about statistical significance. The extent of the issues surrounding p-values and statistical significance are so extensive that it led the  American Statistical Association to make an official statement addressing the misconceptions and clarifying the correct usage of p-values <span class="co">[</span><span class="ot">@wasserstein2016</span><span class="co">]</span>. The statement makes the point that p-values do not measure the probability that a hypothesis is true, or that the data was produced by random chance alone; nor should they be interpreted as measuring the size of an effect or the importance of a result; and that by themselves p-values are not a good measure of the evidence for a model or a hypothesis <span class="co">[</span><span class="ot">@wasserstein2016</span><span class="co">]</span>. Unfortunately, these are all very common misconceptions that find their way into scientific research. The misconceptions go much further than this, however, as illustrated by @greenland2016. @greenland2016 build on the wider discussion to include statistical tests, laying out a total of 25 different misconceptions about statistical testing, p-values (and confidence intervals) and statistical power^<span class="co">[</span><span class="ot">I won't discuss all 25 misinterpretations that @greenland2016 discuss, because that would be overkill here. For anyone that is interested in learning more about the problems with p-values, I'd recommend going straight to the source and reading their paper.</span><span class="co">]</span>. They make the case that the biggest culprit is the arbitrary use of thresholds for defining statistical significance, which frame p-values in problematic terms, reduce a need for critical thinking about findings, and are far too easily abused.</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>This discussion has even permeated some of the media <span class="co">[</span><span class="ot">@aschwanden2015; @aschwanden2016</span><span class="co">]</span>. FiveThirtyEight have written about this issue in some detail, and their discussion of the topic is perhaps more accessible than the more detailed discussions going on. @aschwanden2015 is a particularly good summary of the discussion around p-values, and does a good job highlighting that the issues around p-values and the "replication crisis" are not evidence that science can't be trusted, but rather that this stuff is extremely difficult, and that it takes a community of scientists working towards the truth, to try and achieve results.</span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Garden of Forking Paths {.unnumbered}</span></span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a>@gelman2021 add to the discussion by highlighting the role that the "garden of forking paths" approach to analysis, where there are many possible explanations for a statistically significant result, plays in the statistical crisis in science. It's important to recognise how a scientific hypothesis can correspond to multiple statistical hypotheses <span class="co">[</span><span class="ot">@gelman2021</span><span class="co">]</span>, and to guard against this possibility. This effectively creates a "multiple comparisons" situation, where an analysis can uncover statistically significant results that can be used to support a theory by multiple "forking paths". A "one-to-many mapping from scientific to statistical hypotheses" can create a situation that is equivalent to p-hacking but without the necessary intent <span class="co">[</span><span class="ot">@gelman2021</span><span class="co">]</span>.</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>One of the central questions that @gelman2021 ask, is whether the same data-analysis decisions would have been made with a different data set. The garden of forking paths is driven by the fact that we devise scientific hypotheses using our common sense and domain knowledge, given the data we have, and we do not consider the myriad implicit choices that this involves. By not considering these implicit choices, we fail to account for the garden of forking paths.</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Comparisons Are Not Enough {.unnumbered}</span></span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a>I think that the biggest issue with p-values themselves is that we have placed far too much value in them. We use methods that are designed to compute p-values, we define "success" as the discovery of a statistically significant finding, and we give little thought to the design choices made in the process of carrying out these tests. This is partly driven by the frequency of misconceptions and misunderstandings of p-values, but it's also a result of the methods we use. </span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a>If we are more precise about how we think about p-values, this should help us avoid making the mistake of thinking they are proving a hypothesis or that they are sufficient evidence for a theory, but if we are more precise about what a p-value really represents, this raises significant questions about the methods we use. What purpose does a test for "statistical significance" serve if we acknowledge that p-values are not enough?</span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a>@gelman2016 argue that p-values are not the real problem, because the real problem is null hypothesis significance testing, which they refer to as a "parody of falsificationism in which straw-man null hypothesis A is rejected and this is taken as evidence in favour of preferred alternative B." The null hypothesis significance testing framework places far too much weight on the shoulders of p-values, and even if we are doing a better job of hypothesis testing (being particularly careful not to make mistakes about how we use p-values), what is it really worth? When we are treating p-values as a part of the wider toolkit, hypothesis tests become less useful. </span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a><span class="fu">## Avoiding the Pitfalls </span></span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a>There has been plenty of debate among statisticians about the issues around hypothesis testing and p-values, and while there is no detail too minor for two academics to throw hands over, there is a general consensus that our approach to p-values is a problem. What has not been settled, however, is what the right approach is. That makes this section of this chapter a little trickier. However, I will attempt to give some sensible advice about how to avoid the greatest dangers pose by hypothesis testing and the use of p-values more broadly.</span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>A more general approach to hypothesis testing would frame things in terms of a test hypothesis, which is a hypothesised effect (including but not limited to zero effect -- the null hypothesis) that is assumed to be true, and a statistical test is carried out that considers how well the observed date conforms to our expectations given that the test's assumptions, including the test hypothesis, are true.</span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a>Perhaps unsurprisingly, the primary way to improve how we do hypothesis testing is to approach p-values differently. The simplest solution is not to use statistical significance at all. This is an arbitrary threshold that gives us very little good information, and allows researchers to avoid thinking critically about their findings. Instead, we should report the p-value itself, and be clear about what it is that p-values actually tell us! </span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a>Beyond framing hypothesis tests in more appropriate terms, a big part of the improvements that can be made is about transparency. Being transparent about results, about what those results mean, and about what they don't mean, is important. Further, it is important to report all findings, and not pick and choose what we found based on which results support our theory (this becomes easier if we're not in search of statistical significance).</span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a>Ultimately, hypothesis testing requires a careful, critical approach, and this applies to the development of the hypothesis, the design of the test, and the analysis of the findings.</span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a>:::{.callout-note collapse='true'}</span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Details</span></span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Rethink P-Values {.unnumbered}</span></span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a>The p-value is not a magic number. It does not give the probability that your research hypothesis is true, nor does it give the probability that the observed association is a product of random chance <span class="co">[</span><span class="ot">@wasserstein2019</span><span class="co">]</span>. One of the biggest issues I have with p-values is that they feel, given the way we use them in NHST, like they **should** be the probability that our hypothesis is true. However, it is important to never let those urges win - the p-value is not a hypothesis probability.</span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a>Instead, p-values are the degree to which the observed data are consistent with the pattern predicted by the research hypothesis, given the statistical test's assumptions are not violated <span class="co">[</span><span class="ot">@greenland2016</span><span class="co">]</span>^<span class="co">[</span><span class="ot">The probability that a p-value represents are specific to the statistical model we are specifying. It's important to remember that violations of our assumptions in specifying our model can bias the p-value and invalidate our results.</span><span class="co">]</span>.</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a>@lakens2022 recommends thinking about p-values as a statement about the probability of data, rather than a statement about the probability of a given hypothesis or theory. I think remembering this framing is a good way to avoid misconstruing exactly what p-values represent. Similarly, we can think about p-values as our confidence in what it is we observe in our data. How confident can we be that the data we observed is the product of random chance (and the assumptions that underpin our statistical model)? A p-value is an approximation of this. </span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a>The point is that none of these definitions (or more precisely, intuitive explanations) of p-values are claiming that p-values are the probability that the research hypothesis is true. They say nothing about a hypothesis, because they are not a hypothesis probability.</span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a>Framing p-values appropriately is one part of the necessary rethink about p-values. The other is the need to place less weight in what p-values tell us. Having acknowledged all the things that p-values are not, this part should be easy to understand. Given that p-values are not a hypothesis probability, and they are really a statement about our data, they are far from sufficient (though I would argue they are at least necessary) for our understanding of our data. Instead, they are part of a wider toolkit. A toolkit that contains even more important tools, like effect sizes!</span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stop Using Statistical Significance {.unnumbered}</span></span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a>In addition to the common misuse of p-values, the null hypothesis significance testing framework also defines an arbitrary threshold for what is considered a sufficiently "meaningful" result - statistical significance.</span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a>The commonly defined significance threshold (or the alpha ($\alpha$) level) is a p-value of 0.05. A p-value of less than 0.05 is deemed statistically significant, while a p-value greater than 0.05 is insignificant. So two p-values, one that equals 0.499 and another that equals 0.501 are both deemed to offer something very different to each other. One is treated as a sign of success, and the other is filed away in the desk drawer, never to be seen again! The arbitrariness of this threshold has been best highlighted by @gelman2006 when they demonstrated the fact that the difference between statistically significant and insignificant is not itself statistically significant. </span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>Defining whether a research finding is of real value based on such a meaningless threshold is clearly one of the contributing factors in the problems with hypothesis testing, and the simple solution is to just stop treating p-values as either statistically significant or not!</span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a>If you need further evidence of the problems with how we have been using statistical significance, look no further than the man credited with first developing the method, Ronald A Fisher. @fisher1956 claimed that no researcher has a fixed level of significance that they are using to reject all hypotheses, regardless of their context. Ronald, I have some terrible, terrible news. </span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a>Where possible, do not use terms like statistical significance, and do not use a threshold (the $\alpha$ level) for significance. Report p-values instead. P-values should be "viewed as a continuous measure of the compatibility between the data and the entire model used to compute it, ranging from 0 for complete incompatibility to 1 for perfect compatibility, and in this sense may be viewed as measuring the fit of the model to the data." <span class="co">[</span><span class="ot">@greenland2016, pp. 339</span><span class="co">]</span></span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Report All Findings {.unnumbered}</span></span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a>This suggestion ties in closely with the previous two. If p-values are treated like a continuous measure of compatibility, and we are no longer using thresholds to define whether a result is good or bad, this should reduce the urge to report findings selectively, based only on those that say the right thing.</span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a>Do not carry out multiple tests and only show the significant results. Be open and transparent about your process and what was tested. Only reporting significant results is problematic for many reasons. There is no reason to favour statistical significant results. Instead, we should either report all our findings, as this is the most transparent approach, or we should select the substantively meaningful results, taking great care to avoid only reporting those that confirm our theory^<span class="co">[</span><span class="ot">I'm assuming that anyone following this advice has good intentions, and is not looking to cheat their way to the results they want. Someone wanting to achieve this is going to find a way to do it regardless.</span><span class="co">]</span>.</span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a>I think this principle is easier to follow in academia, where extensive reporting of findings is encouraged. I don't think there is a downside to open and transparent reporting in academia (ignoring the potential risks posed to publication). However, in industry, there is often a need to be as clear and concise as possible. Reporting all results could potentially cause confusion, and make statistical findings much harder for non-technical audiences to understand. Therefore I think the advice for industry should be that reporting results should not be defined by the statistical significance of the results. We should report the results that matter, that contribute the most to our understanding of a quantity of interest, and that help others understand the findings. Communication of findings in industry should focus on communicating the key results from the analysis, and that should not be defined by significance.</span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Build Scientific Models {.unnumbered}</span></span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a>Finally, it is important to think carefully about the implications of any hypothesis, and to understand that there is a difference between a scientific hypothesis and a statistical hypothesis^<span class="co">[</span><span class="ot">A scientific hypothesis reflects a theory about the world (for example, Drug $X$ decreases the risk of Disease $Y$) while a statistical hypothesis reflect what is observed in the data (A one-unit increase in $x$ will correspond with a one-unit decrease in $y$).</span><span class="co">]</span>. Statistical models do not, on their own, allow scientific hypotheses</span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a>There are often many different statistical hypotheses that would be consistent with a particular scientific hypothesis. This is well explained and multiple examples are given by @gelman2021.</span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a>The garden of forking paths may be a particularly big issue in hypothesis testing, but it doesn't go away if we scrap NHST. It is important to think about the ways that any scientific hypothesis can be explained by multiple statistical hypotheses, and it's also very important to think about the many choices made when building any statistical model, and what this might mean for our work. There are so many different assumptions that go into the process of building a statistical model, and when we fail to consider the breadth of assumptions that underpin our work, we can easily invalidate our findings. As discussed earlier in this chapter, we have to remember that p-values are assuming that **all** of our assumptions are true, and if any of them are not true, we are in hot water. </span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a>What is needed is to build coherent, logically-specified scientific models from which we design our statistical analysis. @mcelreath2023 makes this case, arguing that we need to build theory-driven scientific models that describe the causal relationship we are modelling before we build statistical models that attempt to test our scientific models.</span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a>Building scientific models doesn't necessarily address all concerns around the "garden of forking paths" issue, but by doing so we are forced to be more thoughtful about our research, and we are more able to identify problems with our analysis.</span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-181"><a href="#cb16-181" aria-hidden="true" tabindex="-1"></a><span class="fu">## Building Statistical Tests</span></span>
<span id="cb16-182"><a href="#cb16-182" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a>A generalised framework for hypothesis testing, that discards of NHST's problematic baggage (instead following a Fisherian approach <span class="co">[</span><span class="ot">@fisher1956</span><span class="co">]</span>), would take the following steps:</span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Specify the test hypothesis ($H_0$) that assumes some effect size against which the observed effect size will be compared - this will often be the assumption of zero effect (the null hypothesis), but it can be anything^<span class="co">[</span><span class="ot">It could be a hypothesis that captures a consequence of a scientific hypothesis being true (as opposed to false, in the case of the null hypothesis), or it can test two scientific hypotheses against each other (instead of one against the possibility of nothing going on) [@prytherch2022].</span><span class="co">]</span>.</span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Generate the test distribution, defined by the data being analysed and the test being carried out, which represents our expectation of what we would observe if the statistical model's assumptions, which include the test hypothesis, are all true.</span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Compute the test statistic, which quantifies how extreme the observed data is, given the test distribution.</span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Compute the p-value, representing the probability of observing a test statistic as large or larger than the observed test statistic if all of the assumptions of the statistical model, including the test hypothesis, are true.</span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a>The important takeaway is that the test statistic is a measure of the sample data, the test distribution is a measure of what we would expect to observe if the test hypothesis, and all other model assumptions, are true, and the statistical test is simply quantifying how compatible the observed data would be with the generated distribution.</span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a>While there are many different statistical tests that are designed to test different data distributions, using different assumptions, the general framework is the same. Below are some examples of statistical tests that are commonly used.</span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-194"><a href="#cb16-194" aria-hidden="true" tabindex="-1"></a><span class="fu">### T-Tests</span></span>
<span id="cb16-195"><a href="#cb16-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-196"><a href="#cb16-196" aria-hidden="true" tabindex="-1"></a>The general idea of a t-test is to calculate the statistical significance of the difference between two groups. What the groups represent depends on the type of t-test being carried out, whether it be comparing a single group of observed data against a null distribution that assumes the data was generated by chance, or comparing the difference between two observed groups against a null distribution that assumes the difference is zero.</span>
<span id="cb16-197"><a href="#cb16-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-198"><a href="#cb16-198" aria-hidden="true" tabindex="-1"></a>There are several broad groups of t-tests:</span>
<span id="cb16-199"><a href="#cb16-199" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-200"><a href="#cb16-200" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>One-sample t-test - comparing the sample mean of a single group against a "known mean", generally testing whether a sample is likely to be generated by chance.</span>
<span id="cb16-201"><a href="#cb16-201" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Paired t-test - comparing means from the same group measured multiple times, like how someone responds to a survey at different points in time.</span>
<span id="cb16-202"><a href="#cb16-202" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Two samples t-test - comparing the means of two groups, such as measuring the difference in blood glucose levels of two different groups.</span>
<span id="cb16-203"><a href="#cb16-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-204"><a href="#cb16-204" aria-hidden="true" tabindex="-1"></a>There are several variations within these groups, for example a paired t-test generally assumes equal variance between the two groups, but this is often not true, so an unequal variance t-test can be used. </span>
<span id="cb16-205"><a href="#cb16-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-206"><a href="#cb16-206" aria-hidden="true" tabindex="-1"></a>Finally, when deciding on the nature of a t-test, you must also decide if the test should be one-tailed or two-tailed. A one-tailed t-test is used to test a hypothesis that includes a directional component (such as $x &gt; y$), while a two-tailed t-test is just testing whether there is a difference between two groups. A one-tailed t-test is appropriate when you believe the difference between two groups should be positive or negative, but if the only belief is that the groups are different, a two-tailed t-test is appropriate.</span>
<span id="cb16-207"><a href="#cb16-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-208"><a href="#cb16-208" aria-hidden="true" tabindex="-1"></a>A two-tailed t-test compares whether the sample mean(s) is different, whether larger or smaller, while a one-tailed t-test assumes the direction of the difference is known. For example, if it is known that Group A is either the same or bigger than Group B (or that Group A being smaller is not of interest), then a one-tailed t-test would be appropriate.</span>
<span id="cb16-209"><a href="#cb16-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-210"><a href="#cb16-210" aria-hidden="true" tabindex="-1"></a>I will simulate data from real-world examples to demonstrate one-sample, paired, and two-sample t-tests, however, these examples are not exhaustive, and there are many variations on t-test implementations.^<span class="co">[</span><span class="ot">If you are unclear on some of the foundations that underpin what I'm discussing here, I'd recommend starting with StatQuest's [Statistics Fundamentals] YouTube playlist, and for a more detailed introduction, I would give [_Answering Questions With Data_] a try.</span><span class="co">]</span></span>
<span id="cb16-211"><a href="#cb16-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-212"><a href="#cb16-212" aria-hidden="true" tabindex="-1"></a><span class="fu">#### One-Sample T-Test</span></span>
<span id="cb16-213"><a href="#cb16-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-214"><a href="#cb16-214" aria-hidden="true" tabindex="-1"></a>We can carry out a one-sample t-test using IQ scores as our example, because there is plenty of data out there about IQ scores that we can base our simulations on. The average IQ score is said to be around 100, and scores are normally distributed with a standard deviation of 15, meaning that the majority of people have an IQ in the range of 85-115.</span>
<span id="cb16-215"><a href="#cb16-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-216"><a href="#cb16-216" aria-hidden="true" tabindex="-1"></a>In a one-sample t-test we are interested in testing a sample mean against an "expected" mean. For example, perhaps we have a group of individuals that are viewed as "high achievers", and we want to use IQ scores to test whether this group is meaningfully different to the rest of the population (despite being aware that IQ is not a particularly good test of either an individual's intelligence or their potential).</span>
<span id="cb16-217"><a href="#cb16-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-218"><a href="#cb16-218" aria-hidden="true" tabindex="-1"></a>A one-sample t-test would allow us to do this because we are able to test the mean IQ score for our high achievers against the expected mean value if there was no real difference between them and the population, which would just be the mean IQ score in the population (100).</span>
<span id="cb16-219"><a href="#cb16-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-220"><a href="#cb16-220" aria-hidden="true" tabindex="-1"></a>Lets first start by simulating our high achievers' IQ scores, assuming that their mean score is 105 (perhaps some of the group studied IQ tests extensively and are now able to ace the test), with a standard deviation of 15.</span>
<span id="cb16-221"><a href="#cb16-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-222"><a href="#cb16-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-225"><a href="#cb16-225" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-226"><a href="#cb16-226" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: simulate-high-achievers</span></span>
<span id="cb16-227"><a href="#cb16-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-228"><a href="#cb16-228" aria-hidden="true" tabindex="-1"></a><span class="co"># mean and standard deviation</span></span>
<span id="cb16-229"><a href="#cb16-229" aria-hidden="true" tabindex="-1"></a>mean, sd <span class="op">=</span> <span class="dv">105</span>, <span class="dv">15</span> </span>
<span id="cb16-230"><a href="#cb16-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-231"><a href="#cb16-231" aria-hidden="true" tabindex="-1"></a><span class="co"># draw sample from normal distribution</span></span>
<span id="cb16-232"><a href="#cb16-232" aria-hidden="true" tabindex="-1"></a>high_achievers <span class="op">=</span> np.random.normal(mean, sd, <span class="dv">10</span>)</span>
<span id="cb16-233"><a href="#cb16-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-234"><a href="#cb16-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-235"><a href="#cb16-235" aria-hidden="true" tabindex="-1"></a>We can take a look at the sample data we have simulated, to see how close it is to the true mean and standard deviation (because we are simulating the data it won't be a perfect match).</span>
<span id="cb16-236"><a href="#cb16-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-239"><a href="#cb16-239" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-240"><a href="#cb16-240" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: summarise-sample</span></span>
<span id="cb16-241"><a href="#cb16-241" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-242"><a href="#cb16-242" aria-hidden="true" tabindex="-1"></a><span class="co"># compute summary statistics for our sample</span></span>
<span id="cb16-243"><a href="#cb16-243" aria-hidden="true" tabindex="-1"></a>np.mean(high_achievers)</span>
<span id="cb16-244"><a href="#cb16-244" aria-hidden="true" tabindex="-1"></a>np.std(high_achievers)</span>
<span id="cb16-245"><a href="#cb16-245" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-246"><a href="#cb16-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-247"><a href="#cb16-247" aria-hidden="true" tabindex="-1"></a>And we can visualise the data to get a better sense of what the group looks like.</span>
<span id="cb16-248"><a href="#cb16-248" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-251"><a href="#cb16-251" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-252"><a href="#cb16-252" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: visualise-sample</span></span>
<span id="cb16-253"><a href="#cb16-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-254"><a href="#cb16-254" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb16-255"><a href="#cb16-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-256"><a href="#cb16-256" aria-hidden="true" tabindex="-1"></a>sns.histplot(x<span class="op">=</span>high_achievers, binwidth<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb16-257"><a href="#cb16-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-258"><a href="#cb16-258" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot labels and title</span></span>
<span id="cb16-259"><a href="#cb16-259" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'IQ Score'</span>)</span>
<span id="cb16-260"><a href="#cb16-260" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">''</span>)</span>
<span id="cb16-261"><a href="#cb16-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-262"><a href="#cb16-262" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb16-263"><a href="#cb16-263" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-264"><a href="#cb16-264" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-265"><a href="#cb16-265" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-266"><a href="#cb16-266" aria-hidden="true" tabindex="-1"></a>It is not clear what the underlying distribution of our sample data is here. We already know that the data was drawn from a normal distribution, but if we did not, we would need more observations to confirm this.</span>
<span id="cb16-267"><a href="#cb16-267" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-270"><a href="#cb16-270" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-271"><a href="#cb16-271" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: one-sample-test</span></span>
<span id="cb16-272"><a href="#cb16-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-273"><a href="#cb16-273" aria-hidden="true" tabindex="-1"></a>pg.ttest(high_achievers, y <span class="op">=</span> <span class="dv">100</span>, alternative <span class="op">=</span> <span class="st">"greater"</span>)</span>
<span id="cb16-274"><a href="#cb16-274" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-275"><a href="#cb16-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-276"><a href="#cb16-276" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-277"><a href="#cb16-277" aria-hidden="true" tabindex="-1"></a>The p-value of our one-sample t-test is 0.105. This means that we would expect to observe a sample mean at least as extreme as our observed mean (106), if the null that there is no difference between the high achiever group's mean IQ score and the population mean IQ score is true, a little more than 10% of the time, or 1 in 10 times. That's quite high, so I wouldn't be confident that we are observing a meaningful difference.</span>
<span id="cb16-278"><a href="#cb16-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-279"><a href="#cb16-279" aria-hidden="true" tabindex="-1"></a>However, we know that there is a meaningful difference between our high achievers and the population, because we specified a difference when we simulated our sample data. If we were to conclude that the results we observe in our t-test do not lend any support to our hypothesis that the high achievers are better at IQ tests than the average person, this would be a false negative (or sometimes unhelpfully referred to as a Type 1 Error).</span>
<span id="cb16-280"><a href="#cb16-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-281"><a href="#cb16-281" aria-hidden="true" tabindex="-1"></a>If we increase our sample size, maybe we will have more luck!</span>
<span id="cb16-282"><a href="#cb16-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-285"><a href="#cb16-285" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-286"><a href="#cb16-286" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: increase-sample-size</span></span>
<span id="cb16-287"><a href="#cb16-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-288"><a href="#cb16-288" aria-hidden="true" tabindex="-1"></a>larger_sample <span class="op">=</span> np.random.normal(mean, sd, <span class="dv">30</span>)</span>
<span id="cb16-289"><a href="#cb16-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-290"><a href="#cb16-290" aria-hidden="true" tabindex="-1"></a>pg.ttest(larger_sample, y <span class="op">=</span> <span class="dv">100</span>, alternative <span class="op">=</span> <span class="st">"greater"</span>)</span>
<span id="cb16-291"><a href="#cb16-291" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-292"><a href="#cb16-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-293"><a href="#cb16-293" aria-hidden="true" tabindex="-1"></a>Increasing the sample size from 10 to 30 has decreased the probability of observing a test statistic at least as extreme as our sample mean if our data was generated by chance (and the assumptions of our statistical model are all met) from around 10% to just over 1.5%. That seems a lot more reasonable, and at this point I'd be more comfortable concluding that the high achiever's IQ scores are meaningfully different to the average score in the population.</span>
<span id="cb16-294"><a href="#cb16-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-295"><a href="#cb16-295" aria-hidden="true" tabindex="-1"></a>Of course, our p-value doesn't tell us anything about the substantive importance of the difference in IQ scores. We might be willing to conclude that there **is** a difference, but does it matter? Well, the answer is no because it's IQ scores and IQ scores don't matter. </span>
<span id="cb16-296"><a href="#cb16-296" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-297"><a href="#cb16-297" aria-hidden="true" tabindex="-1"></a>But what if, for some truly baffling reason, we had reason to believe IQ scores are not nonsense? In that case, how do we know if the difference between the mean value in the high achievers group and the population is large enough to be of scientific and substantive relevance? This is not a question that significance tests can answer, and is instead something that our domain expertise should answer.</span>
<span id="cb16-298"><a href="#cb16-298" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-299"><a href="#cb16-299" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Paired T-Test</span></span>
<span id="cb16-300"><a href="#cb16-300" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-301"><a href="#cb16-301" aria-hidden="true" tabindex="-1"></a>When respondents in surveys are asked to self-report certain details about themselves, this can invite bias. Take, for example, the difference in <span class="co">[</span><span class="ot">self-reported and interviewer-measured height among men in the UK</span><span class="co">][Male Heights]</span>. Although there are many proud short kings out there that will confidently declare their height, safe in the knowledge that good things come in small packages, there are plenty of men that wish they were a little bit taller (someone should write a song about that), and for whom a self-reported survey is an opportunity to dream big (literally). Any time that respondents are given the opportunity to report details about themselves that have any social baggage attached, there is a possibility that the responses will be subject to social desirability bias.</span>
<span id="cb16-302"><a href="#cb16-302" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-303"><a href="#cb16-303" aria-hidden="true" tabindex="-1"></a>In the case of self-reported and measured male heights, the differences are not particularly large in absolute terms. It's very possible that the difference is actually a product of some random variation, or measurement error. We can test this! </span>
<span id="cb16-304"><a href="#cb16-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-305"><a href="#cb16-305" aria-hidden="true" tabindex="-1"></a>Using the mean values for reported and measured heights in 2016, calculating the standard deviation from the reported standard error^<span class="co">[</span><span class="ot">Standard deviation can be computed from standard error by multiplying the standard error by the square root of the sample size. The formula for this is as follows: $$\sigma = SE \times \sqrt{n}$$</span><span class="co">]</span>, and calculating the correlation between the two variables using the value for each variable from 2011-2016^<span class="co">[</span><span class="ot">Using only five observations is less than ideal, but given that what we are doing is just illustrative, this will work fine.</span><span class="co">]</span>, we can simulate our sample data.</span>
<span id="cb16-306"><a href="#cb16-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-309"><a href="#cb16-309" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-310"><a href="#cb16-310" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: simulate-male-heights</span></span>
<span id="cb16-311"><a href="#cb16-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-312"><a href="#cb16-312" aria-hidden="true" tabindex="-1"></a>mean, cov, n <span class="op">=</span> [<span class="fl">177.23</span>, <span class="fl">175.71</span>], [[<span class="dv">1</span>, <span class="fl">.83</span>], [<span class="fl">.83</span>,  <span class="dv">1</span>]], <span class="dv">100</span></span>
<span id="cb16-313"><a href="#cb16-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-314"><a href="#cb16-314" aria-hidden="true" tabindex="-1"></a>self_reported, measured <span class="op">=</span> np.random.multivariate_normal(mean, cov, n).T</span>
<span id="cb16-315"><a href="#cb16-315" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-316"><a href="#cb16-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-317"><a href="#cb16-317" aria-hidden="true" tabindex="-1"></a>We have simulated the self-reported and measured height for 100 men, given the mean, standard deviation, and correlation as detailed in Table 1 of the above analysis, and carried out a paired t-test that compares the difference between the two simulated groups.</span>
<span id="cb16-318"><a href="#cb16-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-321"><a href="#cb16-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-322"><a href="#cb16-322" aria-hidden="true" tabindex="-1"></a>pg.ttest(self_reported, measured, alternative<span class="op">=</span><span class="st">"greater"</span>)</span>
<span id="cb16-323"><a href="#cb16-323" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-324"><a href="#cb16-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-325"><a href="#cb16-325" aria-hidden="true" tabindex="-1"></a>The difference between the self-reported and measured heights has a p-value of 0.00719, suggesting there is a 0.7% probability of observing a t-statistic at least as extreme as we observe here, if the data was generated by chance and if the assumptions of our statistical model are valid.</span>
<span id="cb16-326"><a href="#cb16-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-327"><a href="#cb16-327" aria-hidden="true" tabindex="-1"></a>The estimated difference between self-reported height and measured height is 1.4cm, with about 1cm difference from our estimate and our lower and upper confidence intervals, which suggests that the difference between self-reported height for men is larger than their measured heights. However, is 1.4cm significant enough to really care? I mean, none of this is important enough (or valid enough) to really care, but if we suspend disbelief for a moment, is a 1.4cm difference substantively meaningful? I'll leave that up to you to decide.</span>
<span id="cb16-328"><a href="#cb16-328" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-329"><a href="#cb16-329" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Two Samples T-Test</span></span>
<span id="cb16-330"><a href="#cb16-330" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-331"><a href="#cb16-331" aria-hidden="true" tabindex="-1"></a>So we've established a very real, totally scientific over-reporting of male height across the UK, and some clever, entrepreneurial, and not at all exploitative pharmaceutical company has had the good sense to develop a medication that gives you an extra couple of inches in height. </span>
<span id="cb16-332"><a href="#cb16-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-333"><a href="#cb16-333" aria-hidden="true" tabindex="-1"></a>In order to prove that it works it needs to go through testing (because we are particularly reckless, we don't really care if it is safe in this example). An experiment is designed, with a treatment and control group that is representative of the wider male population, and the wonder drug is given to the treatment group.</span>
<span id="cb16-334"><a href="#cb16-334" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-335"><a href="#cb16-335" aria-hidden="true" tabindex="-1"></a>We can compare the change in height for the two groups in the 6 months after the treatment group have taken the medication, hypothesising that the drug has had a positive effect on the treatment group, helping them grow by a whopping 1.5cm! This is compared against the average increase in height for the control group that is approximately zero, with a very small standard deviation. A two-sample t-test can help us shed some light on whether we can be confident that this difference did not occur by chance.</span>
<span id="cb16-336"><a href="#cb16-336" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-337"><a href="#cb16-337" aria-hidden="true" tabindex="-1"></a>We simulate our treatment and control groups, with 20 observations in each, with the treatment group having a mean growth of 3cm and standard deviation of 3cm, and the control group having a mean growth of 0cm and a standard deviation of 0.5cm.</span>
<span id="cb16-338"><a href="#cb16-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-341"><a href="#cb16-341" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-342"><a href="#cb16-342" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: simulate-height-experiment</span></span>
<span id="cb16-343"><a href="#cb16-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-344"><a href="#cb16-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-345"><a href="#cb16-345" aria-hidden="true" tabindex="-1"></a><span class="co"># create an empty list for the simulated data</span></span>
<span id="cb16-346"><a href="#cb16-346" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb16-347"><a href="#cb16-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-348"><a href="#cb16-348" aria-hidden="true" tabindex="-1"></a><span class="co"># for loop to simulate 300 not bald observations</span></span>
<span id="cb16-349"><a href="#cb16-349" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">300</span>):</span>
<span id="cb16-350"><a href="#cb16-350" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Not Bald"</span></span>
<span id="cb16-351"><a href="#cb16-351" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb16-352"><a href="#cb16-352" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb16-353"><a href="#cb16-353" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.039</span>, <span class="fl">0.594</span>, <span class="fl">0.367</span>]</span>
<span id="cb16-354"><a href="#cb16-354" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-355"><a href="#cb16-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-356"><a href="#cb16-356" aria-hidden="true" tabindex="-1"></a>  <span class="co"># collect all simulated variables into a dictionary</span></span>
<span id="cb16-357"><a href="#cb16-357" aria-hidden="true" tabindex="-1"></a>  not_bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb16-358"><a href="#cb16-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-359"><a href="#cb16-359" aria-hidden="true" tabindex="-1"></a>  <span class="co"># append dictionary to data</span></span>
<span id="cb16-360"><a href="#cb16-360" aria-hidden="true" tabindex="-1"></a>  data.append(not_bald)</span>
<span id="cb16-361"><a href="#cb16-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-362"><a href="#cb16-362" aria-hidden="true" tabindex="-1"></a><span class="co"># create pandas dataframe from simulated data</span></span>
<span id="cb16-363"><a href="#cb16-363" aria-hidden="true" tabindex="-1"></a>baldness_survey <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb16-364"><a href="#cb16-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-365"><a href="#cb16-365" aria-hidden="true" tabindex="-1"></a><span class="co"># draw samples from normal distribution</span></span>
<span id="cb16-366"><a href="#cb16-366" aria-hidden="true" tabindex="-1"></a>treatment, control <span class="op">=</span> np.random.normal(<span class="dv">3</span>, <span class="fl">.5</span>, <span class="dv">20</span>), np.random.normal(<span class="dv">0</span>, <span class="fl">.5</span>, <span class="dv">20</span>)</span>
<span id="cb16-367"><a href="#cb16-367" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-368"><a href="#cb16-368" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-369"><a href="#cb16-369" aria-hidden="true" tabindex="-1"></a>Having simulated our treatment and control groups, we can compute a t-test to see if there is a difference in how much each group has grown, on average.</span>
<span id="cb16-370"><a href="#cb16-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-373"><a href="#cb16-373" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-374"><a href="#cb16-374" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: height-experiment</span></span>
<span id="cb16-375"><a href="#cb16-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-376"><a href="#cb16-376" aria-hidden="true" tabindex="-1"></a>pg.ttest(x<span class="op">=</span>treatment, y<span class="op">=</span>control, paired<span class="op">=</span><span class="va">False</span>, alternative<span class="op">=</span><span class="st">'greater'</span>)</span>
<span id="cb16-377"><a href="#cb16-377" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-378"><a href="#cb16-378" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-379"><a href="#cb16-379" aria-hidden="true" tabindex="-1"></a>  The probability that we would observe a test statistic as large or larger than observed here is very, very small, and the results of the t-test estimate that the difference between the two groups is a little bit less than 4cm, with a lower confidence interval of just under 3cm, suggesting that this medication is having a positive difference on height in the treatment group!</span>
<span id="cb16-380"><a href="#cb16-380" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-381"><a href="#cb16-381" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chi-Squared Tests</span></span>
<span id="cb16-382"><a href="#cb16-382" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-383"><a href="#cb16-383" aria-hidden="true" tabindex="-1"></a>Although there are many ways that you may encounter chi-squared ($\chi^2$) tests, the two most common are tests of independence and a goodness of fit tests. The chi-squared test of independence is a test of the association between two categorical variables, meaning whether the variables are related or not. It is testing whether they vary independently of each other. A chi-squared goodness of fit test is used to examine how well a theoretical distribution fits the distribution of a categorical variable. We will focus on the test of independence here, because I think this is the most common use case.</span>
<span id="cb16-384"><a href="#cb16-384" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-385"><a href="#cb16-385" aria-hidden="true" tabindex="-1"></a>A common use-case for chi-squared tests of independence is survey data, where individuals from different groups respond to a survey question that offers a finite number of discrete answers, like a scale from good to bad. A good example of categorical variables used in survey data is the impact of male baldness on 46 year old men, carried out by @sinikumpu2021. In this survey they grouped men by levels of baldness, and asked them various questions about their lives, from education to sex-life, to consider the impact that being bald has on men's lives and well-being. </span>
<span id="cb16-386"><a href="#cb16-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-387"><a href="#cb16-387" aria-hidden="true" tabindex="-1"></a>We will use the education question as the basis for simulating the data for our chi-squared test. The survey splits education-level in to three groups - basic, secondary, and tertiary - while I have simplified the question by turning the levels of baldness into a binary categorical variable.</span>
<span id="cb16-388"><a href="#cb16-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-391"><a href="#cb16-391" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-392"><a href="#cb16-392" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: simulate-survey</span></span>
<span id="cb16-393"><a href="#cb16-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-394"><a href="#cb16-394" aria-hidden="true" tabindex="-1"></a><span class="co"># create an empty list for the simulated data</span></span>
<span id="cb16-395"><a href="#cb16-395" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb16-396"><a href="#cb16-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-397"><a href="#cb16-397" aria-hidden="true" tabindex="-1"></a><span class="co"># for loop to simulate 300 not bald observations</span></span>
<span id="cb16-398"><a href="#cb16-398" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">300</span>):</span>
<span id="cb16-399"><a href="#cb16-399" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Not Bald"</span></span>
<span id="cb16-400"><a href="#cb16-400" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb16-401"><a href="#cb16-401" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb16-402"><a href="#cb16-402" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.039</span>, <span class="fl">0.594</span>, <span class="fl">0.367</span>]</span>
<span id="cb16-403"><a href="#cb16-403" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-404"><a href="#cb16-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-405"><a href="#cb16-405" aria-hidden="true" tabindex="-1"></a>  <span class="co"># collect all simulated variables into a dictionary</span></span>
<span id="cb16-406"><a href="#cb16-406" aria-hidden="true" tabindex="-1"></a>  not_bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb16-407"><a href="#cb16-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-408"><a href="#cb16-408" aria-hidden="true" tabindex="-1"></a>  <span class="co"># append dictionary to data</span></span>
<span id="cb16-409"><a href="#cb16-409" aria-hidden="true" tabindex="-1"></a>  data.append(not_bald)</span>
<span id="cb16-410"><a href="#cb16-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-411"><a href="#cb16-411" aria-hidden="true" tabindex="-1"></a><span class="co"># for loop to simulate 600 bald observations</span></span>
<span id="cb16-412"><a href="#cb16-412" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">600</span>):</span>
<span id="cb16-413"><a href="#cb16-413" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Bald"</span></span>
<span id="cb16-414"><a href="#cb16-414" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb16-415"><a href="#cb16-415" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb16-416"><a href="#cb16-416" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.03</span>, <span class="fl">0.603</span>, <span class="fl">0.367</span>]</span>
<span id="cb16-417"><a href="#cb16-417" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-418"><a href="#cb16-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-419"><a href="#cb16-419" aria-hidden="true" tabindex="-1"></a>  <span class="co"># collect all simulated variables into a dictionary</span></span>
<span id="cb16-420"><a href="#cb16-420" aria-hidden="true" tabindex="-1"></a>  bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb16-421"><a href="#cb16-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-422"><a href="#cb16-422" aria-hidden="true" tabindex="-1"></a>  <span class="co"># append dictionary to data</span></span>
<span id="cb16-423"><a href="#cb16-423" aria-hidden="true" tabindex="-1"></a>  data.append(bald)</span>
<span id="cb16-424"><a href="#cb16-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-425"><a href="#cb16-425" aria-hidden="true" tabindex="-1"></a><span class="co"># create pandas dataframe from simulated data</span></span>
<span id="cb16-426"><a href="#cb16-426" aria-hidden="true" tabindex="-1"></a>baldness_survey <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb16-427"><a href="#cb16-427" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-428"><a href="#cb16-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-429"><a href="#cb16-429" aria-hidden="true" tabindex="-1"></a>We have simulated each of the different baldness categories individually, before combining them into a single dataset. I think there should be a more concise way of doing this, but I haven't wrapped my head round how I should do that yet.</span>
<span id="cb16-430"><a href="#cb16-430" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-431"><a href="#cb16-431" aria-hidden="true" tabindex="-1"></a>We can visualise the difference between the three education-levels in terms of proportion of baldness, to see if there is an obvious difference.</span>
<span id="cb16-432"><a href="#cb16-432" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-435"><a href="#cb16-435" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-436"><a href="#cb16-436" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: visualise-survey</span></span>
<span id="cb16-437"><a href="#cb16-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-438"><a href="#cb16-438" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb16-439"><a href="#cb16-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-440"><a href="#cb16-440" aria-hidden="true" tabindex="-1"></a>sns.histplot(</span>
<span id="cb16-441"><a href="#cb16-441" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> baldness_survey, </span>
<span id="cb16-442"><a href="#cb16-442" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="st">'education'</span>, hue <span class="op">=</span> <span class="st">'hair_status'</span>,</span>
<span id="cb16-443"><a href="#cb16-443" aria-hidden="true" tabindex="-1"></a>  multiple<span class="op">=</span><span class="st">'fill'</span></span>
<span id="cb16-444"><a href="#cb16-444" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb16-445"><a href="#cb16-445" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-446"><a href="#cb16-446" aria-hidden="true" tabindex="-1"></a><span class="co"># set plot labels and title</span></span>
<span id="cb16-447"><a href="#cb16-447" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Education Level'</span>)</span>
<span id="cb16-448"><a href="#cb16-448" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Proportion'</span>)</span>
<span id="cb16-449"><a href="#cb16-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-450"><a href="#cb16-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-451"><a href="#cb16-451" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb16-452"><a href="#cb16-452" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-453"><a href="#cb16-453" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-454"><a href="#cb16-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-455"><a href="#cb16-455" aria-hidden="true" tabindex="-1"></a>There's a small difference between people with a basic education, but for secondary and tertiary education it seems to be pretty evenly split. Our chi-squared test can test this empirically.</span>
<span id="cb16-456"><a href="#cb16-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-459"><a href="#cb16-459" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-460"><a href="#cb16-460" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: chi-sq-test</span></span>
<span id="cb16-461"><a href="#cb16-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-462"><a href="#cb16-462" aria-hidden="true" tabindex="-1"></a>expected, observed, stats <span class="op">=</span> pg.chi2_independence(baldness_survey, x<span class="op">=</span><span class="st">'education'</span>, y<span class="op">=</span><span class="st">'hair_status'</span>)</span>
<span id="cb16-463"><a href="#cb16-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-464"><a href="#cb16-464" aria-hidden="true" tabindex="-1"></a>stats.<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb16-465"><a href="#cb16-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-466"><a href="#cb16-466" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-467"><a href="#cb16-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-468"><a href="#cb16-468" aria-hidden="true" tabindex="-1"></a>So our p-value tells us that we should expect to observe data at least extreme as our simulated survey data a little less than 20% of the time, if the null hypothesis that there is no difference in education levels across levels of hair loss is true. This is a smaller p-value than @sinikumpu2021 observe (0.299), but that will be a consequence of the fact we are simulating the data using probabilistic samples, not replicating their results exactly.</span>
<span id="cb16-469"><a href="#cb16-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-470"><a href="#cb16-470" aria-hidden="true" tabindex="-1"></a>If we were to fudge the numbers a little bit, so as to artificially increase the difference in education levels based on the amount of hair a 46 year old man has, what would our results look like?</span>
<span id="cb16-471"><a href="#cb16-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-474"><a href="#cb16-474" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-475"><a href="#cb16-475" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: add-effect</span></span>
<span id="cb16-476"><a href="#cb16-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-477"><a href="#cb16-477" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1, empty list `data`:</span></span>
<span id="cb16-478"><a href="#cb16-478" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb16-479"><a href="#cb16-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-480"><a href="#cb16-480" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: for-loop:</span></span>
<span id="cb16-481"><a href="#cb16-481" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">300</span>):</span>
<span id="cb16-482"><a href="#cb16-482" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 3: simulate all real-world factors:</span></span>
<span id="cb16-483"><a href="#cb16-483" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Not Bald"</span></span>
<span id="cb16-484"><a href="#cb16-484" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb16-485"><a href="#cb16-485" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb16-486"><a href="#cb16-486" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.15</span>, <span class="fl">0.55</span>, <span class="fl">0.3</span>]</span>
<span id="cb16-487"><a href="#cb16-487" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-488"><a href="#cb16-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-489"><a href="#cb16-489" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 4: accumulate all factors in dictionary `d`:</span></span>
<span id="cb16-490"><a href="#cb16-490" aria-hidden="true" tabindex="-1"></a>  not_bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb16-491"><a href="#cb16-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-492"><a href="#cb16-492" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 5: append `d` to `data`</span></span>
<span id="cb16-493"><a href="#cb16-493" aria-hidden="true" tabindex="-1"></a>  data.append(not_bald)</span>
<span id="cb16-494"><a href="#cb16-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-495"><a href="#cb16-495" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: for-loop:</span></span>
<span id="cb16-496"><a href="#cb16-496" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">600</span>):</span>
<span id="cb16-497"><a href="#cb16-497" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 3: simulate all real-world factors:</span></span>
<span id="cb16-498"><a href="#cb16-498" aria-hidden="true" tabindex="-1"></a>  hair_status <span class="op">=</span> <span class="st">"Bald"</span></span>
<span id="cb16-499"><a href="#cb16-499" aria-hidden="true" tabindex="-1"></a>  education <span class="op">=</span> np.random.choice(</span>
<span id="cb16-500"><a href="#cb16-500" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Basic"</span>, <span class="st">"Secondary"</span>, <span class="st">"Tertiary"</span>],</span>
<span id="cb16-501"><a href="#cb16-501" aria-hidden="true" tabindex="-1"></a>    p<span class="op">=</span>[<span class="fl">0.05</span>, <span class="fl">0.6</span>, <span class="fl">0.35</span>]</span>
<span id="cb16-502"><a href="#cb16-502" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-503"><a href="#cb16-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-504"><a href="#cb16-504" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 4: accumulate all factors in dictionary `d`:</span></span>
<span id="cb16-505"><a href="#cb16-505" aria-hidden="true" tabindex="-1"></a>  bald <span class="op">=</span> { <span class="st">"hair_status"</span>: hair_status, <span class="st">"education"</span>: education }</span>
<span id="cb16-506"><a href="#cb16-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-507"><a href="#cb16-507" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Step 5: append `d` to `data`</span></span>
<span id="cb16-508"><a href="#cb16-508" aria-hidden="true" tabindex="-1"></a>  data.append(bald)</span>
<span id="cb16-509"><a href="#cb16-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-510"><a href="#cb16-510" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: create the DataFrame (outside of the for-loop)</span></span>
<span id="cb16-511"><a href="#cb16-511" aria-hidden="true" tabindex="-1"></a>baldness_survey_with_effect <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb16-512"><a href="#cb16-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-513"><a href="#cb16-513" aria-hidden="true" tabindex="-1"></a>expected, observed, stats <span class="op">=</span> pg.chi2_independence(baldness_survey_with_effect, x<span class="op">=</span><span class="st">'education'</span>, y<span class="op">=</span><span class="st">'hair_status'</span>)</span>
<span id="cb16-514"><a href="#cb16-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-515"><a href="#cb16-515" aria-hidden="true" tabindex="-1"></a>stats.<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb16-516"><a href="#cb16-516" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-517"><a href="#cb16-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-518"><a href="#cb16-518" aria-hidden="true" tabindex="-1"></a>Having increased the number of respondents with a basic education (and in turn decreasing those with secondary and tertiary educations) for the not bald group, our chi-squared test results suggest the data we observe is much more extreme. The p-value for this test is extremely small, but that's not totally surprising. We did fake our data, after all.</span>
<span id="cb16-519"><a href="#cb16-519" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-520"><a href="#cb16-520" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">###</span><span class="co"> Simulation-Based Statistical Tests --&gt;</span></span>
<span id="cb16-521"><a href="#cb16-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-522"><a href="#cb16-522" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- One of the reasons I always struggled with hypothesis testing when first learning about them is the seemingly endless variations of tests, each suited to a slightly different data distribution or context. While you could take on the arduous task of learning as many different tests as possible, it is probably (definitely) better to think in terms of the framework for carrying out a hypothesis test that was detailed earlier, and understanding all tests as generalisations of this framework. --&gt;</span></span>
<span id="cb16-523"><a href="#cb16-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-524"><a href="#cb16-524" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- This was an idea pitched by the computer scientist Allen Downey, when he wrote that "there is only one test". He's right. This framework for understanding all hypothesis tests as variations on the same basic method uses simulated data from permutation or bootstrapping methods to generate the null distribution against which observed data can be evaluated.  --&gt;</span></span>
<span id="cb16-525"><a href="#cb16-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-526"><a href="#cb16-526" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- When classical significance testing methods were developed, simulation wasn't possible, due to the absence of computational power. Now that we have high-powered machines that will do everything for us, we don't have to worry about this so much, which makes simulation much more viable. --&gt;</span></span>
<span id="cb16-527"><a href="#cb16-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-528"><a href="#cb16-528" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Downey visualises how this simulation-based workflow for hypothesis testing works using the following diagram: --&gt;</span></span>
<span id="cb16-529"><a href="#cb16-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-530"><a href="#cb16-530" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ![](figures/hypothesis_testing.png) --&gt;</span></span>
<span id="cb16-531"><a href="#cb16-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-532"><a href="#cb16-532" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- The `{infer}` workflow for carrying out simulation-based hypothesis tests is designed based on the same principles (`specify()`, `hypothesize()`, `generate()`, `calculate()`, &amp; `visualize()`). This all-in-one process for hypothesis testing is a little more involved than the humble t-test, and the code for computing hypothesis tests using this approach is definitely more verbose. However, the power that this process gives you is that you can easily carry out any hypothesis test once you've learned Downey's framework, and the consistency of the `{infer}` workflow makes it easier to learn how to do this. The `{infer}` documentation also provides a [pretty exhaustive list][{infer} Examples] of how to compute hypothesis tests under a variety of conditions. --&gt;</span></span>
<span id="cb16-533"><a href="#cb16-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-534"><a href="#cb16-534" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- We can see `{infer}` in action by recalculating our IQ scores t-test that we carried out earlier on. Here is what that looks like using simulation-based methods. --&gt;</span></span>
<span id="cb16-535"><a href="#cb16-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-536"><a href="#cb16-536" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- https://allendowney.github.io/ElementsOfDataScience/13_hypothesis.html  --&gt;</span></span>
<span id="cb16-537"><a href="#cb16-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-538"><a href="#cb16-538" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{python} --&gt;</span></span>
<span id="cb16-539"><a href="#cb16-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-540"><a href="#cb16-540" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # compute test statistic --&gt;</span></span>
<span id="cb16-541"><a href="#cb16-541" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- np.mean(larger_sample) --&gt;</span></span>
<span id="cb16-542"><a href="#cb16-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-543"><a href="#cb16-543" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # generate null distribution --&gt;</span></span>
<span id="cb16-544"><a href="#cb16-544" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- null = np.random.normal(100, sd, 1000) --&gt;</span></span>
<span id="cb16-545"><a href="#cb16-545" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb16-546"><a href="#cb16-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-547"><a href="#cb16-547" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{python} --&gt;</span></span>
<span id="cb16-548"><a href="#cb16-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-549"><a href="#cb16-549" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- np.sum(null &lt; np.mean(larger_sample)) / len(null) --&gt;</span></span>
<span id="cb16-550"><a href="#cb16-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-551"><a href="#cb16-551" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb16-552"><a href="#cb16-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-553"><a href="#cb16-553" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{python} --&gt;</span></span>
<span id="cb16-554"><a href="#cb16-554" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- #| label: visualise-sample --&gt;</span></span>
<span id="cb16-555"><a href="#cb16-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-556"><a href="#cb16-556" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- fig, ax = plt.subplots() --&gt;</span></span>
<span id="cb16-557"><a href="#cb16-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-558"><a href="#cb16-558" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- sns.histplot(x=null, binwidth=5) --&gt;</span></span>
<span id="cb16-559"><a href="#cb16-559" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- plt.axvline(x=np.mean(larger_sample), color = "red") --&gt;</span></span>
<span id="cb16-560"><a href="#cb16-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-561"><a href="#cb16-561" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # set plot labels and title --&gt;</span></span>
<span id="cb16-562"><a href="#cb16-562" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- plt.xlabel('IQ Score') --&gt;</span></span>
<span id="cb16-563"><a href="#cb16-563" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- plt.ylabel('') --&gt;</span></span>
<span id="cb16-564"><a href="#cb16-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-565"><a href="#cb16-565" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- sns.despine() --&gt;</span></span>
<span id="cb16-566"><a href="#cb16-566" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- plt.show() --&gt;</span></span>
<span id="cb16-567"><a href="#cb16-567" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb16-568"><a href="#cb16-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-569"><a href="#cb16-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-570"><a href="#cb16-570" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{python} --&gt;</span></span>
<span id="cb16-571"><a href="#cb16-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-572"><a href="#cb16-572" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- def get_bootstrap(data_column_1, data_column_2, iterations=15000, statistic=np.mean, sign_level=0.05): --&gt;</span></span>
<span id="cb16-573"><a href="#cb16-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-574"><a href="#cb16-574" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   boot_len = max(len(data_column_1), len(data_column_2)) --&gt;</span></span>
<span id="cb16-575"><a href="#cb16-575" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   boot_data, samples_1_a, samples_2_a = [], [], [] --&gt;</span></span>
<span id="cb16-576"><a href="#cb16-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-577"><a href="#cb16-577" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   # Bootstrap sampling --&gt;</span></span>
<span id="cb16-578"><a href="#cb16-578" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   for _ in tqdm(range(iterations)): --&gt;</span></span>
<span id="cb16-579"><a href="#cb16-579" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     sample_1, sample_2 = data_column_1.sample(boot_len, replace=True).values, data_column_2.sample(boot_len, replace=True).values --&gt;</span></span>
<span id="cb16-580"><a href="#cb16-580" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     boot_data.append(statistic(sample_2 - sample_1)) --&gt;</span></span>
<span id="cb16-581"><a href="#cb16-581" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     samples_1_a.append(np.mean(sample_1)) --&gt;</span></span>
<span id="cb16-582"><a href="#cb16-582" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     samples_2_a.append(np.mean(sample_2)) --&gt;</span></span>
<span id="cb16-583"><a href="#cb16-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-584"><a href="#cb16-584" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     # Calculate quantiles --&gt;</span></span>
<span id="cb16-585"><a href="#cb16-585" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     quants = pd.DataFrame(boot_data).quantile([sign_level / 2, 1 - sign_level / 2]) --&gt;</span></span>
<span id="cb16-586"><a href="#cb16-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-587"><a href="#cb16-587" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     # Calculate p-values --&gt;</span></span>
<span id="cb16-588"><a href="#cb16-588" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     boot_mean, boot_std = np.mean(boot_data), np.std(boot_data) --&gt;</span></span>
<span id="cb16-589"><a href="#cb16-589" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     p_1 = norm.cdf(0, boot_mean, boot_std) --&gt;</span></span>
<span id="cb16-590"><a href="#cb16-590" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     p_2 = norm.cdf(0, -boot_mean, boot_std) --&gt;</span></span>
<span id="cb16-591"><a href="#cb16-591" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     p_value = min(p_1, p_2) * 2 --&gt;</span></span>
<span id="cb16-592"><a href="#cb16-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-593"><a href="#cb16-593" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     # Plotting --&gt;</span></span>
<span id="cb16-594"><a href="#cb16-594" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     fig, axes = plt.subplots(2, 1, figsize=(10, 10), dpi=100) --&gt;</span></span>
<span id="cb16-595"><a href="#cb16-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-596"><a href="#cb16-596" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     # Plot 1 --&gt;</span></span>
<span id="cb16-597"><a href="#cb16-597" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[0].grid(True, linestyle='-', linewidth=0.7, color='lightgrey') --&gt;</span></span>
<span id="cb16-598"><a href="#cb16-598" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[0].set_facecolor('white') --&gt;</span></span>
<span id="cb16-599"><a href="#cb16-599" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     n, _, bars = axes[0].hist(boot_data, bins=50) --&gt;</span></span>
<span id="cb16-600"><a href="#cb16-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-601"><a href="#cb16-601" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     for bar in bars: --&gt;</span></span>
<span id="cb16-602"><a href="#cb16-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-603"><a href="#cb16-603" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--       if bar.get_x() &lt;= quants.iloc[0][0] or bar.get_x() &gt;= quants.iloc[1][0]: --&gt;</span></span>
<span id="cb16-604"><a href="#cb16-604" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--         bar.set_facecolor('palevioletred') --&gt;</span></span>
<span id="cb16-605"><a href="#cb16-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-606"><a href="#cb16-606" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--       else: --&gt;</span></span>
<span id="cb16-607"><a href="#cb16-607" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--         bar.set_facecolor('deepskyblue') --&gt;</span></span>
<span id="cb16-608"><a href="#cb16-608" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--         bar.set_edgecolor('purple') --&gt;</span></span>
<span id="cb16-609"><a href="#cb16-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-610"><a href="#cb16-610" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[0].vlines(boot_mean, ymin=0, ymax=max(n),linestyle='-', color='purple',linewidth=0.7)    --&gt;</span></span>
<span id="cb16-611"><a href="#cb16-611" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[0].vlines(quants,ymin=0,ymax=max(n),linestyle='--', color='purple',linewidth=0.7) --&gt;</span></span>
<span id="cb16-612"><a href="#cb16-612" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[0].annotate(f"{quants.iloc[0][0]:.3f}", xy=(quants.iloc[0][0], 0), xytext=(quants.iloc[0][0]+.001, max(n)/2)) --&gt;</span></span>
<span id="cb16-613"><a href="#cb16-613" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[0].annotate(f"{quants.iloc[1][0]:.3f}", xy=(quants.iloc[1][0], 0), xytext=(quants.iloc[1][0]+.001, max(n)/2)) --&gt;</span></span>
<span id="cb16-614"><a href="#cb16-614" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[0].set(xlabel='boot_data', ylabel='Frequency', title=f'Distribution of differences in {statistic.__name__}s') --&gt;</span></span>
<span id="cb16-615"><a href="#cb16-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-616"><a href="#cb16-616" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     # Plot 2 --&gt;</span></span>
<span id="cb16-617"><a href="#cb16-617" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[1].grid(True, linestyle='-', linewidth=0.7, color='lightgrey') --&gt;</span></span>
<span id="cb16-618"><a href="#cb16-618" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[1].set_facecolor('white') --&gt;</span></span>
<span id="cb16-619"><a href="#cb16-619" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[1].hist(samples_1_a, bins=50, color='deepskyblue', alpha=0.5, label=f"{statistic.__name__}s of boot dataset 1") --&gt;</span></span>
<span id="cb16-620"><a href="#cb16-620" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[1].hist(samples_2_a, bins=50, color='crimson', alpha=0.5, label=f"{statistic.__name__}s of boot dataset 2") --&gt;</span></span>
<span id="cb16-621"><a href="#cb16-621" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[1].set(ylabel='Frequency') --&gt;</span></span>
<span id="cb16-622"><a href="#cb16-622" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     axes[1].legend() --&gt;</span></span>
<span id="cb16-623"><a href="#cb16-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-624"><a href="#cb16-624" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     plt.subplots_adjust(hspace=0.2) --&gt;</span></span>
<span id="cb16-625"><a href="#cb16-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-626"><a href="#cb16-626" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     # Result --&gt;</span></span>
<span id="cb16-627"><a href="#cb16-627" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     conclusion = "reject" if p_value &lt;= sign_level else "fail to reject" --&gt;</span></span>
<span id="cb16-628"><a href="#cb16-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-629"><a href="#cb16-629" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     print(f"The p-value of {np.round(p_value, 6)} suggests that there is evidence to {conclusion} the null hypothesis at the {sign_level} significance level.") --&gt;</span></span>
<span id="cb16-630"><a href="#cb16-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-631"><a href="#cb16-631" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     return {'quants': quants, 'boot mean': boot_mean, 'p_value': p_value} --&gt;</span></span>
<span id="cb16-632"><a href="#cb16-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-633"><a href="#cb16-633" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb16-634"><a href="#cb16-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-635"><a href="#cb16-635" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r} --&gt;</span></span>
<span id="cb16-636"><a href="#cb16-636" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- #| label: high-achievers-sim-test --&gt;</span></span>
<span id="cb16-637"><a href="#cb16-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-638"><a href="#cb16-638" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # compute test statistic --&gt;</span></span>
<span id="cb16-639"><a href="#cb16-639" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- test_statistic &lt;-  --&gt;</span></span>
<span id="cb16-640"><a href="#cb16-640" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   larger_sample |&gt;  --&gt;</span></span>
<span id="cb16-641"><a href="#cb16-641" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   specify(response = score) |&gt;  --&gt;</span></span>
<span id="cb16-642"><a href="#cb16-642" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   calculate(stat = "mean") --&gt;</span></span>
<span id="cb16-643"><a href="#cb16-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-644"><a href="#cb16-644" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # generate null distribution --&gt;</span></span>
<span id="cb16-645"><a href="#cb16-645" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- null_distribution &lt;-  --&gt;</span></span>
<span id="cb16-646"><a href="#cb16-646" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   larger_sample |&gt; --&gt;</span></span>
<span id="cb16-647"><a href="#cb16-647" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   specify(response = score) |&gt;  --&gt;</span></span>
<span id="cb16-648"><a href="#cb16-648" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   hypothesize(null = "point", mu = 100) |&gt;  --&gt;</span></span>
<span id="cb16-649"><a href="#cb16-649" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   generate(reps = 1000, type = "bootstrap") |&gt;  --&gt;</span></span>
<span id="cb16-650"><a href="#cb16-650" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   calculate(stat = "mean") --&gt;</span></span>
<span id="cb16-651"><a href="#cb16-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-652"><a href="#cb16-652" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # calculate the p-value of observing a test statistic at least as extreme as  --&gt;</span></span>
<span id="cb16-653"><a href="#cb16-653" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # our observed statistic if the data was generated by chance --&gt;</span></span>
<span id="cb16-654"><a href="#cb16-654" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- p_value &lt;-  --&gt;</span></span>
<span id="cb16-655"><a href="#cb16-655" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   null_distribution |&gt; --&gt;</span></span>
<span id="cb16-656"><a href="#cb16-656" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   get_p_value(obs_stat = test_statistic, direction = "greater") --&gt;</span></span>
<span id="cb16-657"><a href="#cb16-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-658"><a href="#cb16-658" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # visualise the test statistic against the null --&gt;</span></span>
<span id="cb16-659"><a href="#cb16-659" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- null_distribution |&gt; --&gt;</span></span>
<span id="cb16-660"><a href="#cb16-660" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   visualize() +  --&gt;</span></span>
<span id="cb16-661"><a href="#cb16-661" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   shade_p_value(test_statistic, direction = NULL, color = "#00A499") + --&gt;</span></span>
<span id="cb16-662"><a href="#cb16-662" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   geom_hline(yintercept = 0, colour = "#333333", linewidth = 1) + --&gt;</span></span>
<span id="cb16-663"><a href="#cb16-663" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   annotate( --&gt;</span></span>
<span id="cb16-664"><a href="#cb16-664" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     "text", x = 95, y = 125, --&gt;</span></span>
<span id="cb16-665"><a href="#cb16-665" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     label = paste0("t = ", round(test_statistic, 2), "\n p = ", p_value), --&gt;</span></span>
<span id="cb16-666"><a href="#cb16-666" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     size = rel(8), color="grey30" --&gt;</span></span>
<span id="cb16-667"><a href="#cb16-667" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     ) + --&gt;</span></span>
<span id="cb16-668"><a href="#cb16-668" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   labs(x = "IQ Score", y = NULL, title = NULL) --&gt;</span></span>
<span id="cb16-669"><a href="#cb16-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-670"><a href="#cb16-670" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb16-671"><a href="#cb16-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-672"><a href="#cb16-672" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- I think that one of the main strengths of this approach is the intuition it gives us for how hypothesis testing really works. By going through the process step-by-step, we can see what we are doing more easily. We compute our test statistic, which is some summary measure of the data (in this case the mean value), before simulating a null distribution under the assumption that there is no difference between our high achievers and the population. We do this by specifying the data we are testing (the response variable), stating our null hypothesis (that we are carrying out a test using a point estimate, and that the mean of the null is 100), and generating a distribution by taking 1000 bootstrap samples (where sample size is equal to our input and samples are drawn from our data with replacement).  --&gt;</span></span>
<span id="cb16-673"><a href="#cb16-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-674"><a href="#cb16-674" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Having built a null distribution, we can compare our test statistic against that null and see how extreme it appears to be if the assumption that the null hypothesis is true is correct. We can quantify this by computing a p-value, which is just a measure of the proportion of values in our null distribution that are at least as extreme as the test statistic. In the above example, because we are carrying out a one-tailed test (`direction = "greater"`), we would just need to filter the null distribution for values equal to or greater than our sample mean. If we were carrying out a two-tailed test, we would have to calculate the absolute distance of all values from the null mean, and then filter for all values equal to or greater than the difference between our sample mean and the null mean. --&gt;</span></span>
<span id="cb16-675"><a href="#cb16-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-676"><a href="#cb16-676" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- To further illustrate this workflow, below is the chi-squared test equivalent. --&gt;</span></span>
<span id="cb16-677"><a href="#cb16-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-678"><a href="#cb16-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-679"><a href="#cb16-679" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r} --&gt;</span></span>
<span id="cb16-680"><a href="#cb16-680" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- #| label: chi-squared-sim-test --&gt;</span></span>
<span id="cb16-681"><a href="#cb16-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-682"><a href="#cb16-682" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # calculate test statistic --&gt;</span></span>
<span id="cb16-683"><a href="#cb16-683" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- chisq_statistic &lt;-  --&gt;</span></span>
<span id="cb16-684"><a href="#cb16-684" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   baldness_survey |&gt;  --&gt;</span></span>
<span id="cb16-685"><a href="#cb16-685" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   specify(education ~ hair_status) |&gt;  --&gt;</span></span>
<span id="cb16-686"><a href="#cb16-686" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   hypothesize(null = "independence") |&gt;  --&gt;</span></span>
<span id="cb16-687"><a href="#cb16-687" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   calculate(stat = "Chisq") --&gt;</span></span>
<span id="cb16-688"><a href="#cb16-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-689"><a href="#cb16-689" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # generate null distribution --&gt;</span></span>
<span id="cb16-690"><a href="#cb16-690" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- chisq_null &lt;-  --&gt;</span></span>
<span id="cb16-691"><a href="#cb16-691" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   baldness_survey |&gt;  --&gt;</span></span>
<span id="cb16-692"><a href="#cb16-692" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   specify(education ~ hair_status) |&gt;  --&gt;</span></span>
<span id="cb16-693"><a href="#cb16-693" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   hypothesize(null = "independence") |&gt;  --&gt;</span></span>
<span id="cb16-694"><a href="#cb16-694" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   generate(reps = 1000, type = "permute") |&gt;  --&gt;</span></span>
<span id="cb16-695"><a href="#cb16-695" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   calculate(stat = "Chisq") --&gt;</span></span>
<span id="cb16-696"><a href="#cb16-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-697"><a href="#cb16-697" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # calculate the p value from the test statistic and null distribution --&gt;</span></span>
<span id="cb16-698"><a href="#cb16-698" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- chisq_p &lt;- --&gt;</span></span>
<span id="cb16-699"><a href="#cb16-699" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   chisq_null |&gt;  --&gt;</span></span>
<span id="cb16-700"><a href="#cb16-700" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   get_p_value(chisq_statistic, direction = "greater") --&gt;</span></span>
<span id="cb16-701"><a href="#cb16-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-702"><a href="#cb16-702" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # visualize the null distribution and test statistic --&gt;</span></span>
<span id="cb16-703"><a href="#cb16-703" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- chisq_null |&gt;  --&gt;</span></span>
<span id="cb16-704"><a href="#cb16-704" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   visualize() +  --&gt;</span></span>
<span id="cb16-705"><a href="#cb16-705" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   shade_p_value(chisq_statistic, direction = NULL, color = "#00A499") + --&gt;</span></span>
<span id="cb16-706"><a href="#cb16-706" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   geom_hline(yintercept = 0, colour = "#333333", linewidth = 1) + --&gt;</span></span>
<span id="cb16-707"><a href="#cb16-707" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   annotate( --&gt;</span></span>
<span id="cb16-708"><a href="#cb16-708" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     "text", x = 7.5, y = 250, --&gt;</span></span>
<span id="cb16-709"><a href="#cb16-709" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     label =  --&gt;</span></span>
<span id="cb16-710"><a href="#cb16-710" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--       paste0("χ2 = ", round(chisq_statistic$stat, 2), --&gt;</span></span>
<span id="cb16-711"><a href="#cb16-711" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--              "\n p = ", chisq_p), --&gt;</span></span>
<span id="cb16-712"><a href="#cb16-712" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     size = rel(8), color="grey30" --&gt;</span></span>
<span id="cb16-713"><a href="#cb16-713" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     ) + --&gt;</span></span>
<span id="cb16-714"><a href="#cb16-714" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   labs(x = "Chi-Squared Statistic", y = NULL, title = NULL) --&gt;</span></span>
<span id="cb16-715"><a href="#cb16-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-716"><a href="#cb16-716" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb16-717"><a href="#cb16-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-718"><a href="#cb16-718" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- As discussed above, the simulation-based approach to hypothesis testing involves carrying out the processes in the hypothesis test more explicitly, and this helps us gain intuition for what we are doing. This also forces us to think about how we specify and compute our test. It's pretty easy to build a t-test without really getting as far as actually checking if it is the right approach, and you don't need to think long and hard about exactly what is going on when you run a t-test. Downey's "There is only one test" framework, and the `{infer}` implementation, forces you to think about what you are doing and to be a little more deliberate about your choices. --&gt;</span></span>
<span id="cb16-719"><a href="#cb16-719" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-720"><a href="#cb16-720" aria-hidden="true" tabindex="-1"></a><span class="fu">## Moving Beyond Testing</span></span>
<span id="cb16-721"><a href="#cb16-721" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-722"><a href="#cb16-722" aria-hidden="true" tabindex="-1"></a>Hypothesis testing is still a useful tool when we are looking to make conclusions about data without overinterpreting noise <span class="co">[</span><span class="ot">@gelman2020</span><span class="co">]</span>. Simple statistical tests of difference between groups or differences from a hypothesised value have plenty of utility, especially in industry, where getting reasonably reliable results quickly can often be more valuable than taking more time to get more precision.</span>
<span id="cb16-723"><a href="#cb16-723" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-724"><a href="#cb16-724" aria-hidden="true" tabindex="-1"></a>However, where more precision is needed, a different approach may be necessary. Testing a test hypothesis can only give us some sense of the degree to which our data is consistent with the hypothesis. This may move us closer to being able to make inferences, especially if the testing approach is robust, and sound scientific models have been developed for which the hypothesis test is a reasonable statistical representation, but answering meaningful questions usually involves trying to make some assertions not just about the existence of an effect or a difference, but also quantifying it too. Where this is the goal, estimation is a better choice. The estimation approach to statistical inference does not rely on a hypothesis, and instead focuses on a "quantity of interest", for which we use the data to estimate <span class="co">[</span><span class="ot">@prytherch2022</span><span class="co">]</span>. While testing tries to give us a sense of whether or not what we observe in our sample is likely to exist in the population, the estimation approach tries to measure the effect size itself.</span>
<span id="cb16-725"><a href="#cb16-725" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-726"><a href="#cb16-726" aria-hidden="true" tabindex="-1"></a>Although estimating an effect size is a good start, it is also important to quantify the precision of that estimate as well. Estimating the size of an effect, a point estimate, is inherently uncertain. This uncertainty can be a strength, rather than a weakness. By incorporating uncertainty in to our estimates, we can say more about what our sample data is telling us, and we are better able to make inferences about the population. </span>
<span id="cb16-727"><a href="#cb16-727" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-728"><a href="#cb16-728" aria-hidden="true" tabindex="-1"></a>Further, by acknowledging the uncertainty in the statistical modelling process, not just about the model's point estimates, the process should become a more honest representation of the quantity of interest. @greenland2016 argue that the goal of statistical analysis is to evaluate the certainty of an effect size, and acknowledging and embracing that uncertainty should include every aspect of the process, including the conclusions we draw and the way we present our findings.</span>
<span id="cb16-729"><a href="#cb16-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-730"><a href="#cb16-730" aria-hidden="true" tabindex="-1"></a>:::{.callout-note collapse='true'}</span>
<span id="cb16-731"><a href="#cb16-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-732"><a href="#cb16-732" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Details</span></span>
<span id="cb16-733"><a href="#cb16-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-734"><a href="#cb16-734" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Estimate Meaningful Quantities {.unnumbered}</span></span>
<span id="cb16-735"><a href="#cb16-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-736"><a href="#cb16-736" aria-hidden="true" tabindex="-1"></a>This is, I believe, the most important suggestion made in this chapter. I am borrowing a phrase used by the UNC Epidemiologist, Charles Poole, in a talk he gave about the value of p-values. He states that we should "stop testing null hypotheses, start estimating meaningful quantities" <span class="co">[</span><span class="ot">@poole2022</span><span class="co">]</span>. His (very firmly-held) position is that there is no reforming null hypothesis significance testing. Instead, our focus should be on estimation. @poole2022 argues that wherever significance testing is of some utility, we should instead frame the results in terms of substantive results and estimation of effect size. This would still do what hypothesis testing is attempting to do, but it should be more robust (if done properly), and the framing itself offers a lot more.</span>
<span id="cb16-737"><a href="#cb16-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-738"><a href="#cb16-738" aria-hidden="true" tabindex="-1"></a>@gelman2020 argue that we are generally capable of being interested in issues where effects exist, and that the presence of an effect is therefore not entirely interesting, in and of itself. Therefore, we should concern ourselves with the things that do matter, like the size of the effect, not the existence of the effect itself.</span>
<span id="cb16-739"><a href="#cb16-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-740"><a href="#cb16-740" aria-hidden="true" tabindex="-1"></a>Although I'm not going to take quite as strong a position as @poole2022 or @gelman2020, I do think his position is pretty much correct. I am of the view that hypothesis testing is generally a sub-optimal approach when trying to better understand phenomena using data. I think there are still plenty of situations where a hypothesis test is sufficient, and in industry where there is often value to providing functionally useful (if relatively imprecise) conclusions quickly, it is reasonable to turn to testing. However, there are lots of situations where an estimate of the effect size, and the precision of that estimate, are both very valuable!</span>
<span id="cb16-741"><a href="#cb16-741" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-742"><a href="#cb16-742" aria-hidden="true" tabindex="-1"></a>It is rare that we should only care that the effect exists. Generally speaking, this should be a prerequisite to the more important question of what the magnitude and direction of that effect is. Even where the knowledge of the existence of an effect is sufficient, it is hard for me to imagine a situation where knowing the magnitude and direction of the effect wouldn't be useful, and wouldn't allow us to estimate our confidence in that effect more effectively.</span>
<span id="cb16-743"><a href="#cb16-743" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-744"><a href="#cb16-744" aria-hidden="true" tabindex="-1"></a>Taking an estimation approach to statistical inference doesn't necessarily alter the statistical methods used (though more on this at the end of this chapter), but it does require our adjusting how we approach these methods and what we look to get out of them.</span>
<span id="cb16-745"><a href="#cb16-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-746"><a href="#cb16-746" aria-hidden="true" tabindex="-1"></a>Estimation is not specifically about method. If statistical tests are used, then it requires taking the estimate of the effect size, or calculating it if the context means that any output does not offer an intuitive interpretation of effect. Other methods, like regression, can give a more direct measure of effect size (though there are plenty of regression models that also require work to produce more interpretable effect estimates).</span>
<span id="cb16-747"><a href="#cb16-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-748"><a href="#cb16-748" aria-hidden="true" tabindex="-1"></a>The important change in approach is to focus on the importance of the estimated effect size, not on the p-value, or statistical significance!</span>
<span id="cb16-749"><a href="#cb16-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-750"><a href="#cb16-750" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Embrace Uncertainty {.unnumbered}</span></span>
<span id="cb16-751"><a href="#cb16-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-752"><a href="#cb16-752" aria-hidden="true" tabindex="-1"></a>When we move away from testing null hypotheses and focus instead on the estimation of effect sizes, we go from testing for the existence of an effect to estimating the magnitude (effect size) and the direction (whether the effect is positive or negative). However, a key part of this that is missing is the precision with which we are able to estimate the effect. It's important to remember, in any statistical modelling, that we are estimating quantities, and therefore any point estimate is just our best effort at quantifying an unknown population parameter. The point estimate is probably wrong! It is important that we acknowledge the uncertainty in our estimations and account for them in the way we present model outputs^<span class="co">[</span><span class="ot">It is also worth noting that while the guidance here is about the estimation approach to statistical inference, it can also be applied to hypothesis testing too.</span><span class="co">]</span>.</span>
<span id="cb16-753"><a href="#cb16-753" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-754"><a href="#cb16-754" aria-hidden="true" tabindex="-1"></a>However, the embrace of uncertainty is not limited to our uncertainty in our estimates. It is a wider issue. We have to accept that every part of the process of estimating meaningful quantities is done with a degree of uncertainty, and as such the way we think about statistical modelling and the way we present our results should account for the existence of variance. Uncertainty exists everywhere in statistical analysis and inference <span class="co">[</span><span class="ot">@wasserstein2019</span><span class="co">]</span>.</span>
<span id="cb16-755"><a href="#cb16-755" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-756"><a href="#cb16-756" aria-hidden="true" tabindex="-1"></a>@wasserstein2019 describe the use of "significance tests and dichotomized p-values" as an attempt by some practitioners to avoid dealing with uncertainty by escaping to a more simplistic world where results are either statistically significant or not. Perhaps a little uncharitable (though in an academic context I am more inclined to agree), but we've already discussed the importance of moving away from significance testing, so we should complete the journey and confront uncertainty head-on! We have to accept that findings are more uncertain than is often acknowledged. Our embrace of uncertainty should help us to seek out better methods, apply the necessary amount of care and critical thinking to our work, and draw inferences with the appropriate amount of confidence.</span>
<span id="cb16-757"><a href="#cb16-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-758"><a href="#cb16-758" aria-hidden="true" tabindex="-1"></a>One way of acknowledging and working with this uncertainty is the use of confidence intervals when estimating effect sizes. Confidence intervals estimate a range of values that would be compatible with the data, given all of the statistical model's assumptions are met. While confidence intervals are ultimately estimated from the p-value, and the typical 95% confidence interval corresponds to the p &gt; .05 threshold that I have warned against, they are useful because they frame results in terms of effect sizes, and they incorporate uncertainty in our estimates, helping to quantify the precision with which we are able to estimate effect size. </span>
<span id="cb16-759"><a href="#cb16-759" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-760"><a href="#cb16-760" aria-hidden="true" tabindex="-1"></a>Confidence intervals are a step in the right direction, because point estimates, while they are our best shot at the quantity of interest, are highly likely to be wrong. Estimating the interval within which we would expect the quantity of interest to fall, acknowledges that uncertainty, and there's a better chance that the population parameter falls within the estimated range than bang on the point estimate. However, it's important that we don't treat confidence intervals as the fix for all uncertainty! @gelman2016 states that "the solution is not to reform p-values or to replace them with some other statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty and embracing of variation".</span>
<span id="cb16-761"><a href="#cb16-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-762"><a href="#cb16-762" aria-hidden="true" tabindex="-1"></a>Ultimately, confidence intervals are valuable when estimating meaningful quantities, but there are very far from sufficient when it comes to embracing uncertainty. We have to acknowledge the uncertainty in the entire statistical modelling process, both when we are making decisions about that modelling process and when we present our findings to others.</span>
<span id="cb16-763"><a href="#cb16-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-764"><a href="#cb16-764" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-765"><a href="#cb16-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-766"><a href="#cb16-766" aria-hidden="true" tabindex="-1"></a><span class="fu">## Next Steps</span></span>
<span id="cb16-767"><a href="#cb16-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-768"><a href="#cb16-768" aria-hidden="true" tabindex="-1"></a>The goal for this chapter is that readers come away with an understanding of the significant limitations of null hypothesis significance testing and the more robust approaches to testing hypotheses, and the alternative approach of estimating meaningful effects. I hope that this chapter has helped set people up reasonably well when carrying out tests. Equally, I hope that they will consider the alternatives, recognising when it is defensible to carry out a statistical test and when estimating the effect size is necessary. </span>
<span id="cb16-769"><a href="#cb16-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-770"><a href="#cb16-770" aria-hidden="true" tabindex="-1"></a>I have taken a slightly more forgiving position than many of the statisticians that I respect, and from which I have learned so much, but I have deliberately done so because I think industry has slightly different incentives, and therefore slightly different requirements. I think that there are situations where the use of hypothesis testing is sufficient and defensible, and those situations are much more common in industry. The criticisms remain perfectly valid, but I hope I have given readers the tools to carry out more robust testing, and a good understanding of when to choose estimation instead, where necessary.</span>
<span id="cb16-771"><a href="#cb16-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-772"><a href="#cb16-772" aria-hidden="true" tabindex="-1"></a>While it is possible to use statistical tests for estimation, I also believe that the big wide world of estimation is better served by regression models. I think regression modelling is a better starting point than statistical tests because the structure of regression modelling places more emphasis on the effect size. Further, estimating effect sizes does not rule out the use of statistical tests, but it does require redirecting our focus and making sure to treat the magnitude and direction of effect size as the most important part of the analysis. The premise of statistical tests is that they test for the existence of an effect, and while packages like {infer} frame test outputs in a way that makes using statistical tests for these purposes, it does still require a little work to think about about the results of statistical tests in these terms. Further, I think that regression modelling gives a lot more flexibility when we are attempting to estimate an effect as precisely as possible, and the process and resulting outputs of a regression model lend themselves more naturally to a focus that steers away from hypothesis testing and towards estimating effects.</span>
<span id="cb16-773"><a href="#cb16-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-774"><a href="#cb16-774" aria-hidden="true" tabindex="-1"></a>If it hasn't been made clear before, I think it is important to stress the distinction between the null hypothesis testing framework and the statistical tests used to carry them out. The tests themselves are not really the problem. However, they are closely tied to null hypothesis testing. Ultimately, the statistical tests discussed in this chapter (and many more) are just special cases of linear models <span class="co">[</span><span class="ot">@lindelov2019</span><span class="co">]</span>, and the results in both will be identical <span class="co">[</span><span class="ot">@zablotski2019</span><span class="co">]</span> (if not the way the outputs are presented), so the same things can be achieved with either approach, but regression modelling offers more flexibility, allows for the layering of complexity/precision in estimates, and the way regression outputs are framed are generally a little more balanced. Treating regressions as the first point of call for inferential statistics is not a necessary prerequisite for "doing things the right way", but doing things the right way becomes easier, because it is easier to frame everything in terms of the magnitude and direction of effects.</span>
<span id="cb16-775"><a href="#cb16-775" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-776"><a href="#cb16-776" aria-hidden="true" tabindex="-1"></a>The good news? The next chapter is about modelling linear effects using regression!</span>
<span id="cb16-777"><a href="#cb16-777" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-778"><a href="#cb16-778" aria-hidden="true" tabindex="-1"></a><span class="fu">## Resources</span></span>
<span id="cb16-779"><a href="#cb16-779" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-780"><a href="#cb16-780" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">{infer}</span><span class="co">]</span></span>
<span id="cb16-781"><a href="#cb16-781" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Seeing Theory</span><span class="co">]</span></span>
<span id="cb16-782"><a href="#cb16-782" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">The Permutation Test</span><span class="co">]</span></span>
<span id="cb16-783"><a href="#cb16-783" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">There is Only One Test</span><span class="co">]</span></span>
<span id="cb16-784"><a href="#cb16-784" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">More Hypotheses, Less Trivia</span><span class="co">]</span></span>
<span id="cb16-785"><a href="#cb16-785" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Permutation Tests</span><span class="co">]</span></span>
<span id="cb16-786"><a href="#cb16-786" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Permutation Test as an Alternative to Two-Sample T-Test Using R</span><span class="co">]</span></span>
<span id="cb16-787"><a href="#cb16-787" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Power Analysis</span><span class="co">]</span></span>
<span id="cb16-788"><a href="#cb16-788" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Recommended Resources for A/B Testing</span><span class="co">]</span></span>
<span id="cb16-789"><a href="#cb16-789" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">McElreath - None of the Above</span><span class="co">]</span></span>
<span id="cb16-790"><a href="#cb16-790" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Introduction to Scientific Programming and Simulations in R</span><span class="co">]</span></span>
<span id="cb16-791"><a href="#cb16-791" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-792"><a href="#cb16-792" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-793"><a href="#cb16-793" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Statistical Inference is Only Mostly Wrong</span><span class="co">]</span></span>
<span id="cb16-794"><a href="#cb16-794" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Hypothesis Testing is Only Mostly Useless</span><span class="co">]</span></span>
<span id="cb16-795"><a href="#cb16-795" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">The Idea of Permutation</span><span class="co">]</span></span>
<span id="cb16-796"><a href="#cb16-796" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Coding for Data: Permutation and the T-Test</span><span class="co">]</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center"><a href="https://r-data-science-guides.netlify.app"> R Data Science Guides</a> <i class="fa-brands fa-r-project fa-xl" aria-label="r-project"></i></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>