{
  "hash": "a3e66b8d551fe76568e5abc69e42542d",
  "result": {
    "markdown": "# End-to-End Machine Learning Workflow {#sec-ml-workflow}\n\n\n::: {.cell hash='ml_workflow_r_cache/html/setup_5b0c6c6599127e9f14f5cbab23794227'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Setup Code (Click to Expand)\"}\n# import packages\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(ggplot2)\n  library(tidymodels)\n})\n\n# set plot theme\ntheme_set(scwplot::theme_scw(base_size = 14))\n\n# import data\ndf <- readr::read_csv(here::here('data', 'heart_disease.csv'))\n\n# specify categorical features as factors\ndf <- df |> \n  mutate(\n    sex = as.factor(sex),\n    fasting_bs = as.factor(fasting_bs),\n    resting_ecg = as.factor(resting_ecg),\n    angina = as.factor(angina),\n    heart_disease = as.factor(heart_disease)\n  )\n```\n:::\n\n\nAn end-to-end machine learning workflow can be broken down into many steps, and there are an extensive number of layers of complexity that can be added, serving a variety of purposes. However, in this guide we will work through a bare bones workflow, using a simple dataset, in order to get a better understanding of the process.\n\nA simple end-to-end ML solution will typically include the following steps:\n\n1. Importing & Cleaning Data\n2. Exploratory Data Analysis (which will be skipped in this guide because it has been carried out in an earlier guide which can be found [here](https://htmlpreview.github.io/?https://github.com/NHS-South-Central-and-West/data-science-guides/blob/main/guides/R/eda.html))\n2. Data Preprocessing\n3. Model Training\n    - Selecting from Candidate Models\n    - Hyperparameter Tuning\n    - Identifying Best Model\n4. Fitting Model on Test Data\n5. Model Evaluation\n\nFor a more detailed discussion about a simple modeling workflow in R, the [Model Workflow](https://www.tmwr.org/workflows.html) section in Tidy Modeling with R is a great resource.\n\n## Data\n\n\n::: {.cell hash='ml_workflow_r_cache/html/inspect-data_d29534b73736a4ecd7a976e024008c7a'}\n\n```{.r .cell-code}\n# inspect the data\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 918\nColumns: 10\n$ age                <dbl> 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39,…\n$ sex                <fct> M, F, M, F, M, M, F, M, M, F, F, M, M, M, F, F, M, …\n$ resting_bp         <dbl> 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 1…\n$ cholesterol        <dbl> 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 2…\n$ fasting_bs         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ resting_ecg        <fct> Normal, Normal, ST, Normal, Normal, Normal, Normal,…\n$ max_hr             <dbl> 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 14…\n$ angina             <fct> N, N, N, Y, N, N, N, N, Y, N, N, Y, N, Y, N, N, N, …\n$ heart_peak_reading <dbl> 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0…\n$ heart_disease      <fct> 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n```\n:::\n:::\n\n\n### Train/Test Split\n\nOne of the central tenets of machine learning is that the model should not be trained on the same data that it is evaluated on. This is because the model could learn spurious/random patterns and correlations in the training data, and this will harm the model's ability to make good predictions on new, unseen data. There are many way of trying to resolve this, but the most simple approach is to split the data into a training and test set. The training set will be used to train the model, and the test set will be used to evaluate the model.\n\n\n::: {.cell hash='ml_workflow_r_cache/html/train-test-split_891943c278e026582ea1bb8d3bf1be92'}\n\n```{.r .cell-code}\n# set random seed to ensure reproducibility\nset.seed(123)\n\n# split data into train/test sets\ntrain_test_split <-\n  rsample::initial_split(df,\n                         strata = heart_disease,\n                         prop = 0.7)\n\n# extract training and test sets\ntrain_df <- rsample::training(train_test_split)\ntest_df <- rsample::testing(train_test_split)\n```\n:::\n\n\n\n### Cross-Validation Folds\n\nOn top of the train/test split, we will also use cross-validation to train our models. Cross-validation is a method of training a model on a subset of the data, and then evaluating the model on the remaining data. This process is repeated multiple times, and the average performance is used to evaluate the model. This helps make the training process more generalisable.\n\nFor more information on cross-validation and how it can be used to train models the [Resampling Methods](https://www.tmwr.org/resampling.html#resampling-methods) section in Tidy Modeling with R discusses cross-validation in detail, in the wider context of resampling methods for training machine learning models. This helps to provide context for the purpose of cross-validation, as well as discussing some of the strategies for cross-validation.\n\nWe will use 5-fold stratified cross-validation to train our models. This means that the training data will be split into 5 folds, ensuring that each fold has the same proportion of the target classes. Each fold will be used as a test set once, and the remaining folds will be used as training sets. This will be repeated 5 times, and the average performance will be used to evaluate the model.\n\n\n::: {.cell hash='ml_workflow_r_cache/html/cv-folds_ca28ba41295307839df6f69d775a4671'}\n\n```{.r .cell-code}\n# set random seed to ensure reproducibility\nset.seed(123)\n\n# create cross-validation folds\ntrain_folds <- vfold_cv(train_df, v = 5, strata = heart_disease)\n\n# inspect the folds\ntrain_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [513/129]> Fold1\n2 <split [513/129]> Fold2\n3 <split [514/128]> Fold3\n4 <split [514/128]> Fold4\n5 <split [514/128]> Fold5\n```\n:::\n:::\n\n\n### Preprocessing\n\nThe purpose of preprocessing is to prepare the data for model fitting. This can take a number of different forms, but some of the most common preprocessing steps include:\n\n- Normalizing/Standardizing data\n- One-hot encoding categorical variables\n- Removing outliers\n- Imputing missing values\n\nWe will build two different preprocessing recipes and see which performs better. The first will be a 'basic' recipe that one-hot encodes all categorical features, and removes any features that have strong correlation with another feature in the dataset. The second recipe will include both these steps but will also normalize the numeric features so that they have a mean of zero and a standard deviation of one.\n\n\n::: {.cell hash='ml_workflow_r_cache/html/preprocessing_6731bc42901bb8f1453c43a4debdef81'}\n\n```{.r .cell-code}\n# specify simple preprocessing recipe\nbasic_recipe <- recipe(heart_disease ~ ., data = train_df) |>\n  # one-hot encode categorical variables\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>\n  # remove features that have strong correlation with another feature\n  step_corr(all_numeric_predictors(), threshold = .5)\n\n# specify scaled preprocessing recipe\nscaled_recipe <- recipe(heart_disease ~ ., data = train_df) |>\n  # normalize data so that it has mean zero and sd one\n  step_normalize(all_numeric_predictors()) |> \n  # one-hot encode categorical variables\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>\n  # remove features that have strong correlation with another feature\n  step_corr(all_numeric_predictors(), threshold = .5)\n\n# inspect basic preprocessing recipe\nbasic_recipe |>\n  prep() |>\n  bake(new_data = NULL)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 642 × 11\n     age resting_bp cholesterol max_hr heart_peak_reading heart_disease sex_M\n   <dbl>      <dbl>       <dbl>  <dbl>              <dbl> <fct>         <dbl>\n 1    37        130         283     98                0   0                 1\n 2    39        120         339    170                0   0                 1\n 3    45        130         237    170                0   0                 0\n 4    48        120         284    120                0   0                 0\n 5    39        120         204    145                0   0                 1\n 6    42        115         211    137                0   0                 0\n 7    54        120         273    150                1.5 0                 0\n 8    43        100         223    142                0   0                 0\n 9    44        120         184    142                1   0                 1\n10    40        130         215    138                0   0                 1\n# ℹ 632 more rows\n# ℹ 4 more variables: fasting_bs_X1 <dbl>, resting_ecg_LVH <dbl>,\n#   resting_ecg_ST <dbl>, angina_Y <dbl>\n```\n:::\n\n```{.r .cell-code}\n# inspect scaled preprocessing recipe\nscaled_recipe |>\n  prep() |>\n  bake(new_data = NULL)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 642 × 11\n       age resting_bp cholesterol  max_hr heart_peak_reading heart_disease sex_M\n     <dbl>      <dbl>       <dbl>   <dbl>              <dbl> <fct>         <dbl>\n 1 -1.74       -0.133      0.753  -1.52               -0.833 0                 1\n 2 -1.53       -0.666      1.25    1.31               -0.833 0                 1\n 3 -0.900      -0.133      0.341   1.31               -0.833 0                 0\n 4 -0.585      -0.666      0.762  -0.656              -0.833 0                 0\n 5 -1.53       -0.666      0.0457  0.328              -0.833 0                 1\n 6 -1.21       -0.933      0.108   0.0134             -0.833 0                 0\n 7  0.0436     -0.666      0.663   0.525               0.604 0                 0\n 8 -1.11       -1.73       0.216   0.210              -0.833 0                 0\n 9 -1.00       -0.666     -0.133   0.210               0.125 0                 1\n10 -1.42       -0.133      0.144   0.0528             -0.833 0                 1\n# ℹ 632 more rows\n# ℹ 4 more variables: fasting_bs_X1 <dbl>, resting_ecg_LVH <dbl>,\n#   resting_ecg_ST <dbl>, angina_Y <dbl>\n```\n:::\n:::\n\n\n## Model Training\n\nThere are many different types of models that can be used for classification problems, and selecting the right model for the problem at hand can be difficult when you are first starting out with machine learning.\n\nSimple models like [linear](https://parsnip.tidymodels.org/reference/linear_reg.html) and [logistic](https://parsnip.tidymodels.org/reference/logistic_reg.html) regressions are often a good place to start and can be used to get a better understanding of the data and the problem at hand, and can give you a good idea of baseline performance before building more complex models to improve performance. Another example of a simple model is [K-Nearest Neighbours](https://parsnip.tidymodels.org/reference/nearest_neighbor.html) (KNN), which is a non-parametric model that can be used for both classification and regression problems.\n\nWe will fit a logistic regression and a KNN, as well as fitting a Random Forest model, which is a good example of a slightly more complex model that will often perform well on structured data.\n\n### Model Selection\n\n\n::: {.cell hash='ml_workflow_r_cache/html/setup-workflow_8888d36049cc21a6502cfa0975e625e2'}\n\n```{.r .cell-code}\n# create a list of preprocessing recipes\npreprocessers <-\n  list(\n    basic = basic_recipe,\n    scaled = scaled_recipe\n  )\n\n# create a list of candidate models\nmodels <- list(\n  # logistic regression\n  log = \n    logistic_reg(\n      # tune regularization parameters\n      penalty = tune(),\n      mixture = tune()\n    ) |>\n    set_engine('glmnet') |>\n    set_mode('classification'),\n  # k-nearest neighbours\n  knn = \n    nearest_neighbor(\n      # tune weighting function and number of neighbours\n      weight_func = tune(),\n      neighbors = tune()\n      ) |>\n    set_engine('kknn') |>\n    set_mode('classification'),\n  # random forest\n  rf =\n    rand_forest(\n      # number of trees\n      trees = 1000,\n      # tune number of features to consider at each split\n      # and minimum number of observations in a leaf\n      mtry = tune(),\n      min_n = tune()\n      ) |>\n    set_engine('ranger') |>\n    set_mode('classification')\n  )\n\n# combine these lists as a workflow set\nmodel_workflows <-\n  workflow_set(\n    preproc = preprocessers,\n    models = models\n  )\n\n# specify the metrics on which the models will be evaluated\neval_metrics <- metric_set(f_meas, accuracy)\n```\n:::\n\n\nHaving defined the preprocessing recipes and candidate models, we can now train and tune the models. We will use the `tune_grid()` function to tune the models, which will search over a grid of hyperparameter values to find the best performing model. We will use 5-fold cross-validation to train the models, and will evaluate the models using  F1 score and accuracy.\n\nFor a more detailed discussion of hyperparameter tuning, and the different methods for tuning in tidymodels, the [Model Tuning and the Dangers of Overfitting](https://www.tmwr.org/tuning.html) section of Tidy Modeling with R is a good place to start. In addition, the AWS overview of [hyperparameter tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) is a good resource if you are only looking for a brief overview. Finally, for an in-depth discussion about how hyperparameter tuning works, including the mathematics behind it, Jeremy Jordan's [Hyperparameter Tuning for Machine Learning Models](https://www.jeremyjordan.me/hyperparameter-tuning/) blogpost is thorough but easy to follow.\n\n\n::: {.cell hash='ml_workflow_r_cache/html/train-models_b6ee306abdde90bd570a1837fb5f7e61'}\n\n```{.r .cell-code}\n# train and tune models\ntrained_models <- \n  model_workflows |>\n  workflow_map(\n    # function to use for hyperparameter tuning\n    fn = 'tune_grid',\n    # cv folds for resampling\n    resamples = train_folds,\n    # grid of hyperparameter values to search over\n    grid = 10,\n    # metrics to evaluate model performance\n    metrics = eval_metrics,\n    verbose = TRUE,\n    seed = 123,\n    # control parameters for tuning\n    control = control_grid(\n      save_pred = TRUE,\n      parallel_over = 'everything',\n      save_workflow = TRUE)\n    )\n```\n:::\n\n\n#### Comparing Model Performance\n\nOnce the models have been trained we can inspect the results to see which model performed best. We can also plot the performance of the models to get a better idea of how they performed.\n\n\n::: {.cell hash='ml_workflow_r_cache/html/compare-models_7f19c3e58ed884e127d48cc621746596'}\n\n```{.r .cell-code}\n# inspect the best performing models\ntrained_models |>\n  rank_results(select_best=TRUE) |> \n  filter(.metric == 'f_meas') |> \n  select(wflow_id, model, f1 = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  wflow_id   model               f1\n  <chr>      <chr>            <dbl>\n1 basic_rf   rand_forest      0.786\n2 scaled_rf  rand_forest      0.784\n3 scaled_knn nearest_neighbor 0.771\n4 basic_knn  nearest_neighbor 0.771\n5 basic_log  logistic_reg     0.770\n6 scaled_log logistic_reg     0.770\n```\n:::\n\n```{.r .cell-code}\n# plot performance\ntrained_models |> \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](ml_workflow_r_files/figure-html/compare-models-1.png){width=1152}\n:::\n:::\n\n\n## Finalising Model\n\n\n::: {.cell hash='ml_workflow_r_cache/html/finalise-model_75f6e88a86f97a02ea05e0c4ec0545d9'}\n\n```{.r .cell-code}\n# save best performing model\nbest_results <-\n   trained_models |>  \n   extract_workflow_set_result('basic_rf') |>  \n   select_best(metric = 'f_meas')\n\nbest_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     3    30 Preprocessor1_Model01\n```\n:::\n:::\n\n\n##  Fitting Model on Test Data\n\nHaving selected the best performing model, we can now fit the model to the test data.\n\n\n::: {.cell hash='ml_workflow_r_cache/html/test-model_f4dfc3fc9d89c3841bafc02fafe1f5cc'}\n\n```{.r .cell-code}\n# fit model on test data\nrf_test_results <-\n   trained_models |>\n   # extract the best performing model\n   extract_workflow('basic_rf') |>\n   finalize_workflow(best_results) |>\n   # fit to test data and evaluate performance\n   last_fit(split = train_test_split, metrics=eval_metrics)\n```\n:::\n\n\n## Model Evaluation\n\nFinally, we can inspect the performance of the model on the test data.\n\n\n::: {.cell hash='ml_workflow_r_cache/html/model-evaluation_24b523c9c0c4a4085d6aae1850690f37'}\n\n```{.r .cell-code}\n# inspect model performance on evaluation metrics\ncollect_metrics(rf_test_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 f_meas   binary         0.835 Preprocessor1_Model1\n2 accuracy binary         0.848 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n# inspect confusion matrix\nrf_test_results |> \n  conf_mat_resampled(tidy=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    0   1\n0 106  25\n1  17 128\n```\n:::\n\n```{.r .cell-code}\n# plot confusion matrix\nrf_test_results |>\n  conf_mat_resampled(tidy=FALSE) |> \n  autoplot(type='heatmap')\n```\n\n::: {.cell-output-display}\n![](ml_workflow_r_files/figure-html/model-evaluation-1.png){width=1152}\n:::\n:::\n\n\n## Next Steps\n\nIn this post, we have covered a simple but robust machine learning workflow. We have have used the `tidymodels` package to fit a logistic regression model, a k-nearest neighbours model, and a random forest model to the heart disease dataset. We used cross-validation to make the results more generalizable, and we used hyperparameter tuning to improve model performance.\n\nThe next steps would be to consider what strategies for cross-validation would be most appropriate for the model we are building, what hyperparameter tuning process would produce the best performance, and some more complex algorithms that might outperform the models we've built here.\n\n## Resources\n\nThere are lots of great (and free) introductory resources for machine learning:\n\n- [Machine Learning University](https://aws.amazon.com/machine-learning/mlu) (Python)\n- [MLU Explain](https://mlu-explain.github.io/)\n- [Machine Learning for Beginners](https://microsoft.github.io/ML-For-Beginners/?utm_source=substack&utm_medium=email#/) (Python/R)\n\nFor a guided video course, Data Talks Club's Machine Learning Zoomcamp (available on [Github](https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp) and [Youtube](https://www.youtube.com/watch?v=MqI8vt3-cag&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)) is well-paced and well-presented, covering a variety of machine learning methods and even covering some of the aspects that introductory course often skip over, like deployment and data engineering principles. However, while the ML Zoomcamp course is intended as an introduction to machine learning, it does assume a certain level of familiarity with programming (the course uses Python) and software engineering. The appendix section of the course is definitely helpful for bridging some of the gaps in the course, but it is still worth being aware of the way the course is structured.\n\nIf you are keen to learn more about machine learning but are particularly focused on (or already have experience in) R, the ML Zoomcamp may not be the best fit. If you want to learn about machine learning in R, Max Kuhn & Julia Silge's [Tidy Modeling with R](https://www.tmwr.org/) is a great place to start.\n\nIf you are looking for something that goes into greater detail about a wide range of machine learning methods, then there is no better resource than [An Introduction to Statistical Learning](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf) (or its accompanying [online course](https://www.statlearning.com/online-course)), by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n\nFinally, if you are particularly interested in learning the mathematics that underpins machine learning, I would highly recommend [Mathematics of Machine Learning](https://tivadardanka.com/books/mathematics-of-machine-learning), which is admittedly not free but is very, very good. If you want to learn about the mathematics of machine learning but are not comfortable enough tackling the Mathematics of ML book, the [StatQuest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw) and [3Blue1Brown](https://www.youtube.com/c/3blue1brown) Youtube channels are both really accessible and well-presented.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}