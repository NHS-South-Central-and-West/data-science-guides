{
  "hash": "2629ab262f6b81586b71f9a3f09cdeeb",
  "result": {
    "markdown": "# Linear Regression {#sec-linear-regression}\n\n\n::: {.cell hash='linear_regression_r_cache/html/setup_6e34bcabdcbb777e446742526aebdf1d'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Setup Code (Click to Expand)\"}\n# import packages\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(ggplot2)\n})\n\n# ggplot theme\n# set minimal theme\ntheme_set(scwplot::theme_scw(base_size = 14))\n\n# replace na with empty string in table outputs\noptions(\n  knitr.kable.NA = \"\",\n  knitr.table.format = \"pipe\"\n)\n\n# import data\ndf <- readr::read_csv(here::here(\"data\", \"deprivation.csv\"))\n```\n:::\n\n\nLinear regression is a cornerstone of data science and statistical analysis, and is almost certainly the most common statistical method in use today (t-tests might run it close in industry). Its popularity is owed to its incredible flexibility and simplicity, as it can be used to model a wide range of relationships in a variety of contexts, and it is both extremely simple to use and to interpret. If there is one statistical method that every data scientist and analyst should be familiar with, it is linear regression (followed closely by logistic regression, and then t-tests).\n\nNot only is linear regression a phenomenally useful tool in a data scientist's arsenal, but it is also a fantastic way to learn about the fundamentals of statistical analysis. Linear regression serves as the perfect introduction to statistical inference, and it is a great starting point for learning about regression analysis because all regression models are based on linear regression. Once you have a good understanding of linear regression, you can then move on to more advanced regression models, such as logistic regression, Poisson regression, and so on.\n\nThis tutorial will introduce you to linear regression in R, using data pulled from two different sources. The first data source is the Public Health England's [Fingertips API](https://fingertips.phe.org.uk/) (for which there are packages available in [R](https://github.com/ropensci/fingertipsR) and [Python](https://github.com/publichealthengland/PHDS_fingertips_py)), which is a repository of public health data on a wide range of topics. The data included in this tutorial relates to health outcomes (life expectancy at birth) and deprivation (Indices of Multiple Deprivation (IMD) score and decile). The second data source is the Office of National Statistics' [Health Index](https://www.ons.gov.uk/releases/healthindexforengland2015to2020), which is a composite measure of the health of the population in England. The Health Index is derived from a a wide range of indicators related to three health domains: Healthy People (health outcomes), Healthy Lives (health-related behaviours and personal circumstances), and Healthy Places (wider drivers of health that relate to the places people live). The data included in this tutorial comes from two composite sub-domains in the Healthy Lives domain, which measure the physiological and behavioural risk factors that are associated with poor health outcomes. Both data sources are aggregated at the upper-tier local authority level, and the data covers the years 2015 to 2018.\n\n## Exploratory Data Analysis\n\nFirst we will carry out some exploratory data analysis (EDA) to get a feel for the data, and to see if there are any obvious relationships between the variables that we can use to inform our model.\n\nWe will use the `dplyr` package's `glimpse()` function to get a quick overview of the data, before using the `psych` package's `describe()` function to calculate summary statistics. The `describe()` function can compute a wide range of summary statistics, but we will only use a few of them here (the `select()` function below indicates the summary statistics that will be removed).\n\nThe `knitr` package's `kable()` function is used to format the table output to make it easier to read in HTML format.\n\n\n::: {.cell hash='linear_regression_r_cache/html/df-summary_d2f8619f793e7abd7394ea9242ec0ff4'}\n\n```{.r .cell-code}\n# get a quick overview of the data\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 583\nColumns: 7\n$ area_code           <chr> \"E06000001\", \"E06000001\", \"E06000001\", \"E06000001\"…\n$ year                <dbl> 2015, 2016, 2017, 2018, 2015, 2016, 2017, 2018, 20…\n$ life_expectancy     <dbl> 78.73839, 79.07321, 79.08000, 78.81000, 77.78726, …\n$ imd_score           <dbl> 35.037, 35.037, 35.037, 35.037, 40.460, 40.460, 40…\n$ imd_decile          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 4, 4, 4, 4, 4,…\n$ physiological_score <dbl> 89.9, 88.8, 84.8, 84.6, 96.4, 95.8, 96.7, 96.2, 87…\n$ behavioural_score   <dbl> 84.9, 84.4, 82.7, 81.0, 80.3, 82.6, 84.5, 82.2, 96…\n```\n:::\n\n```{.r .cell-code}\n# calculate summary statistics\ndf |>\n  psych::describe() |>\n  # remove area code and year\n  filter(!row_number() %in% c(1, 2)) |>\n  # remove summary statistics that we won't use\n  select(\n    -vars, -n, -trimmed, -mad,\n    -skew, -kurtosis, -se, -range\n  ) |>\n  # format table output\n  relocate(sd, .after = median) |>\n  # kable output with summary stats rounded to 2 decimal places\n  knitr::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n|                    |  mean| median|    sd|   min|    max|\n|:-------------------|-----:|------:|-----:|-----:|------:|\n|life_expectancy     | 81.17|  81.21|  1.61| 76.53|  86.05|\n|imd_score           | 23.18|  22.96|  8.04|  5.85|  45.04|\n|imd_decile          |  5.44|   5.00|  2.83|  1.00|  10.00|\n|physiological_score | 99.02|  97.90| 10.01| 78.20| 125.90|\n|behavioural_score   | 99.40| 100.60|  8.70| 72.40| 120.00|\n:::\n:::\n\n\nHaving taken a quick look at the data, we can now start to explore how life expectancy, our outcome variable (the variable that we are trying to explain), varies with the other variables in our dataset (the explanatory variables). We will start by plotting how life expectancy is distributed by IMD score, which is a measure of deprivation. The expectation is that there will be a negative association between the two variables, which means that areas with higher levels of deprivation (i.e. lower IMD scores) will have lower life expectancies.\n\n\n::: {.cell hash='linear_regression_r_cache/html/imd-plot_82e7bfcd0f4cdf48282ccea9f703f6d3'}\n\n```{.r .cell-code}\ndf |>\n  ggplot(aes(life_expectancy, imd_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'gray30') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8')\n```\n\n::: {.cell-output-display}\n![](linear_regression_r_files/figure-html/imd-plot-1.png){width=1152}\n:::\n:::\n\n\nThere is a clear negative association between life expectancy and IMD score, which is what we would expect. However, there are some outliers where life expectancy seems to be higher than would be expected given the level of deprivation. The plot seems to suggest that deprivation may have a 'floor effect' on life expectancy, i.e. that higher deprivation raises the floor of life expectancy, but that there are other factors that can raise life expectancy above the floor.\n\nWe can also plot the distribution of life expectancy by the Health Index scores for physiological and behavioural risk factors. These scores are index values that measure the prevalence of physiological and behavioural risk factors that are associated with poor health outcomes, against a baseline value of the national average for each risk factor in 2015. This baseline score is 100, and higher scores mean that an area has a higher prevalence of that risk factor than the national average in 2015, and lower scores mean that the area has lower prevalence. The expectation is that when the risk factor index scores increase, meaning that the risk factors are less prevalent compared against the national average, life expectancy will increase, while a decrease in the scores will be associated with a decrease in life expectancy.\n\n\n::: {.cell hash='linear_regression_r_cache/html/health-index-plots_140d1d6ecf509b00dcd3edb12f663b3a'}\n\n```{.r .cell-code}\ndf |>\n  ggplot(aes(life_expectancy, physiological_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'grey50') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8')\n```\n\n::: {.cell-output-display}\n![](linear_regression_r_files/figure-html/health-index-plots-1.png){width=1152}\n:::\n\n```{.r .cell-code}\ndf |>\n  ggplot(aes(life_expectancy, behavioural_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'grey50') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8')\n```\n\n::: {.cell-output-display}\n![](linear_regression_r_files/figure-html/health-index-plots-2.png){width=1152}\n:::\n:::\n\n\nThe plots show that there is a clear negative association between life expectancy and both the physiological and behavioural risk factor scores, which is what we would expect. However, the plots also show that there are some outliers where life expectancy seems to be higher than would be expected given the level of risk factors. The plots seem to suggest that risk factors may have a 'floor effect' on life expectancy, i.e. that higher risk factors raise the floor of life expectancy, but that there are other factors that can raise life expectancy above the floor.\n\nThe above plots suggest there are strong correlations between life expectancy and the three independent variables. We can confirm this by calculating a correlation matrix for the variables in our dataset, using the `correlation` package.\n\n\n::: {.cell hash='linear_regression_r_cache/html/correlations_4a3ac7052346a6520e7e02d84b84798d'}\n\n```{.r .cell-code}\ndf |>\n  select(\n    area_code,\n    life_expectancy,\n    physiological_score,\n    behavioural_score,\n    imd_score\n  ) |>\n  group_by(area_code) |>\n  summarise_all(.funs = \"mean\") |>\n  correlation::correlation() |>\n  summary() |>\n  knitr::kable(digit = 2)\n```\n\n::: {.cell-output-display}\n|Parameter           | imd_score| behavioural_score| physiological_score|\n|:-------------------|---------:|-----------------:|-------------------:|\n|life_expectancy     |     -0.84|              0.91|                 0.6|\n|physiological_score |     -0.39|              0.62|                    |\n|behavioural_score   |     -0.83|                  |                    |\n:::\n:::\n\n\nThere are strong correlations between life expectancy and the three explanatory variables, however, the correlation between IMD score and the two Health Index scores is also relatively strong, and in the case of behavioural risk factors, it is very strong. This could be an issue, as it could mean that the model is not able to distinguish between the effects of deprivation and the effects of risk factors on life expectancy. This is known as multicollinearity, and it can cause problems when interpreting the results of a regression model.\n\nWe can also visualise the correlation between our explanatory variables, to get a better sense of how they might be related to each other. We will start by plotting IMD score against the two Health Index scores, using scatter plots, before plotting the distributions of the scores by IMD decile, using density plots. We can use the `ggridges` package to plot the density plots, which can be useful for plotting how continuous variables are distributed across discrete groups.\n\n\n::: {.cell hash='linear_regression_r_cache/html/correlation-eda_2170a2b2fedbb98d6b5762b7170f1547'}\n\n```{.r .cell-code}\ndf |>\n  ggplot(aes(imd_score, physiological_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'grey50') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8')\n```\n\n::: {.cell-output-display}\n![](linear_regression_r_files/figure-html/correlation-eda-1.png){width=1152}\n:::\n\n```{.r .cell-code}\ndf |>\n  ggplot(aes(imd_score, behavioural_score)) +\n  geom_point(alpha = 0.8, size = 3, colour = 'grey50') +\n  geom_smooth(method = lm, se = FALSE, linewidth = 2, colour='#005EB8')\n```\n\n::: {.cell-output-display}\n![](linear_regression_r_files/figure-html/correlation-eda-2.png){width=1152}\n:::\n\n```{.r .cell-code}\ndf |>\n  mutate(imd_decile = as.factor(imd_decile)) |>\n  ggplot(aes(physiological_score, imd_decile)) +\n  ggridges::geom_density_ridges()\n```\n\n::: {.cell-output-display}\n![](linear_regression_r_files/figure-html/correlation-eda-3.png){width=1152}\n:::\n\n```{.r .cell-code}\ndf |>\n  mutate(imd_decile = as.factor(imd_decile)) |>\n  ggplot(aes(behavioural_score, imd_decile)) +\n  ggridges::geom_density_ridges()\n```\n\n::: {.cell-output-display}\n![](linear_regression_r_files/figure-html/correlation-eda-4.png){width=1152}\n:::\n:::\n\n\nThe correlations are a little more obvious when visualised. The behavioural index scores have a negative linear association with IMD scores (and a positive linear association with IMD decile). This means that areas with higher level of deprivation also tend to have a higher prevalence of behavioural risk factors that are associated with poor health outcomes. The physiological index scores have a similar association with deprivation, but the relationship is not as strong as the behavioural risk factors, and there is significant variance around the linear trend.\n\n## Linear Regression\n\nFirst we will transform each of the explanatory variables to make them a little easier to interpret (particularly with regard to the intercept). If we don't transform the variables, the intercept will be based on zero values of each explanatory variable, which is not very meaningful. For example, a zero value of either risk factors variable is effectively meaningless because the ONS Health Index is centred around a baseline value of 100 (which is the average value for England in 2015, the first year for which the Health Index was calculated).\n\nThese transformations won't impact the regression results, but will just make the results easier to interpret and explain, as the intercept is no longer based on zero values of each explanatory variable.\n\n\n::: {.cell hash='linear_regression_r_cache/html/transforms_c2843876d3e9b16d6d308683e4313c1c'}\n\n```{.r .cell-code}\n# calculate the imd score mean and standard deviation\nimd_centre <- mean(df$imd_score)\nimd_scale <- sd(df$imd_score)\n\n# use 100 as the centre for the risk factors and calculate the standard deviation\nphysiological_centre <- 100\nphysiological_scale <- sd(df$physiological_score)\nbehavioural_centre <- 100\nbehavioural_scale <- sd(df$physiological_score)\n\n# transform the variables\ndf <-\n  df |>\n  mutate(\n    imd_transformed = (imd_score - imd_centre)/imd_scale,\n    physiological_transformed = (physiological_score - physiological_centre)/physiological_scale,\n    behavioural_transformed = (behavioural_score - behavioural_centre)/behavioural_scale\n  )\n\n# check the transformations\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 583\nColumns: 10\n$ area_code                 <chr> \"E06000001\", \"E06000001\", \"E06000001\", \"E060…\n$ year                      <dbl> 2015, 2016, 2017, 2018, 2015, 2016, 2017, 20…\n$ life_expectancy           <dbl> 78.73839, 79.07321, 79.08000, 78.81000, 77.7…\n$ imd_score                 <dbl> 35.037, 35.037, 35.037, 35.037, 40.460, 40.4…\n$ imd_decile                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 4, 4, 4,…\n$ physiological_score       <dbl> 89.9, 88.8, 84.8, 84.6, 96.4, 95.8, 96.7, 96…\n$ behavioural_score         <dbl> 84.9, 84.4, 82.7, 81.0, 80.3, 82.6, 84.5, 82…\n$ imd_transformed           <dbl> 1.4739156, 1.4739156, 1.4739156, 1.4739156, …\n$ physiological_transformed <dbl> -1.0087421, -1.1186051, -1.5181069, -1.53808…\n$ behavioural_transformed   <dbl> -1.5081194, -1.5580571, -1.7278454, -1.89763…\n```\n:::\n:::\n\n\nHaving transformed the variables, we can now fit a linear regression model to the data. We will use the `lm()` function from the Base R `stats` package. Constructing a linear regression in R is very simple, and the syntax is relatively intuitive. The first argument in the function is the formula, which is the dependent variable on the left-hand side, and the independent variables on the right-hand side, separated by a `~` symbol. The second required argument is the `data` argument, which gives the `lm()` function a dataset to fit the regression to. There are a number of optional arguments that can be passed to the function to control the model fitting process, but we will use the default values for now.\n\nAfter fitting the regression model, we can call the `summary()` function to get a summary of the model results. This will give us the model coefficients, standard errors, t-statistics, p-values, and $R^2$ value. However, in this instance we will use the `sjPlot::tab_model()` function to get a nice table of the model results which renders better in HTML.\n\n\n::: {.cell hash='linear_regression_r_cache/html/life-expectancy-ols_719fe01eda075b3d238ba9a82826e715'}\n\n```{.r .cell-code}\n# fit linear regression\nlife_expectancy_ols <-\n  lm(\n    life_expectancy ~ imd_transformed + \n      physiological_transformed + behavioural_transformed,\n    data = df\n    )\n\n# get model summary\n# summary(life_expectancy_ols)\n\n# get model results table\nsjPlot::tab_model(\n  life_expectancy_ols,\n  show.se=TRUE,\n  collapse.ci = TRUE,\n  p.style = c('numeric_stars'),\n  p.threshold = c(0.05, 0.01, 0.001),\n  string.est = 'Coefficients',\n  string.se = 'Standard Error',\n  pred.labels =\n    c(\n      'Intercept',\n      'IMD Score',\n      'Physiological Score',\n      'Behavioural Score'\n    ),\n  dv.labels = c('Life Expectancy')\n)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Life Expectancy</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Coefficients</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Standard Error</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Intercept</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">81.25 <sup>***</sup><br>(81.19&nbsp;&ndash;&nbsp;81.30)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">IMD Score</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.62 <sup>***</sup><br>(-0.71&nbsp;&ndash;&nbsp;-0.53)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.05</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Physiological Score</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.24 <sup>***</sup><br>(0.17&nbsp;&ndash;&nbsp;0.30)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Behavioural Score</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.91 <sup>***</sup><br>(0.79&nbsp;&ndash;&nbsp;1.03)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.06</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">583</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.844 / 0.843</td>\n</tr>\n<tr>\n<td colspan=\"4\" style=\"font-style:italic; border-top:double black; text-align:right;\">* p&lt;0.05&nbsp;&nbsp;&nbsp;** p&lt;0.01&nbsp;&nbsp;&nbsp;*** p&lt;0.001</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n## Evaluating and Interpreting Regression Models\n\nWhen we fit a regression model, we have to consider both how well the model fits the data (evaluation) and the influence that each explanatory variable is having on the outcome (interpretation). The evaluation process is making sure that the model is doing a good enough job that we can reasonably draw some conclusions from it, while the interpretation process is understanding the conclusions we can draw. It's important to understand the purpose that both of these processes serve, and to understand that the evaluation process is just kicking the tires, while what we are ultimately concerned with is the interpretation.\n\nThere are several elements to consider when interpreting the results of a linear regression model. Broadly speaking the focus should be on the magnitude and the precision of the model coefficients. The magnitude of the coefficients indicates the strength of the association between the explanatory variables and the outcome variable, while the precision of the coefficients indicates the uncertainty in the estimated coefficients. We use the regression model coefficients to understand the magnitude of the effect and standard errors (and confidence intervals) to understand the precision of the effect.\n\n### Model Evaluation\n\nThere are a number of ways we might evaluate model performance, but the starting point should be the model's $R^2$ and the p-values of the model coefficients. These can help us consider whether the model fits the data and whether we have enough data to trust the conclusions we can draw from each explanatory variable.\n\n#### $R^2$\n\nThe $R^2$ value is the proportion of the variance in the outcome that is explained by the model. It is often used as a measure of the goodness of fit of the model, and by extension the extent to which the specified model explains the outcome, but caution should be applied when using $R^2$ this way, because $R^2$ can be misleading and doesn't always tell the whole story. One of the main problems with $R^2$ when used in an inferential context is that adding more variables to the model will always increase the $R^2$ value, even if the new variables are not actually related to the outcome. Adjusted $R^2$ values are used to account for this problem, but they don't resolve all issues with $R^2$, so it is important to consider $R^2$ as just one of a number of indicators of the model's fit. If the $R^2$ is high, this doesn't necessarily mean that the model is a good fit for the data, but if the $R^2$ is particularly low, this might be a good indication that the model is not a good fit for the data, and at the very least it is good reason to be cautious about the results of the analysis, and to take a closer look at the model and the data to understand why the $R^2$ is so low.\n\nDefining high/low $R^2$ values is dependent on the context of the analysis. The theory behind the model should inform our expectations about the $R^2$ value, and we should also consider the number of explanatory variables in the model (adding variables to a regression will increase the $R^2$). If the theory suggests that the model should explain a small amount of the variance in the outcome, and the model has only a few explanatory variables, then a low $R^2$ value might not be a cause for concern.\n\nThe Adjusted $R^2$ value for the above regression model is 0.843, which means that the model explains 84.3% of the observed variance in life expectancy in our dataset. This is a relatively high $R^2$ value.\n\n#### P-Values\n\nP-values are the probability of observing a coefficient as large as the estimated coefficient for a particular explanatory variable, given that the null hypothesis is true (i.e. that the coefficient is equal to zero), indicating that no effect is present. This can be a little tricky to understand at first, but a slightly crude way of thinking about this is that p-values are an estimate of the probability that the model coefficient you observe is actually distinguishable from zero.\n\nP-values are used to test the statistical significance of the coefficients. If the p-value is less than the significance level then the coefficient is statistically significant, and we can reject the null hypothesis that the coefficient is equal to zero. If the p-value is greater than the significance level, then the coefficient is not statistically significant, and we cannot reject the null hypothesis that the coefficient is equal to zero. The significance level is often set at 0.05, which means that we are willing to accept a 5% chance of incorrectly rejecting the null hypothesis. This means that if the p-value is less than 0.05, then there is a greater than 95% probability that the coefficient is not equal to zero. However, the significance level is a subjective decision, and can be set at any value. For example, if the significance level is set at 0.01, then we are willing to accept a 1% chance of incorrectly rejecting the null hypothesis.\n\nIn recent years p-values (and the wider concept of statistical significance) have been the subject of a great deal of discussion in the scientific community, because there is a belief among many practitioners that a lot of scientific research places too much weight in the p-value, treating it as the most important factor when interpreting a regression model. I think this perspective is correct, but I think dismissing p-values entirely is a mistake too. The real problem is not with p-values themselves, but with the goal of an analysis being the identification of a statistically significant effect.\n\nI think the best way to think about p-values is the position prescribed by Gelman, Hill, & Vehtari in [Regression & Other Stories](https://users.aalto.fi/~ave/ROS.pdf). Asking whether our coefficient is distinguishable from zero is the wrong question. As analysts, we should typically know enough about the context we are studying to build regression models with explanatory variables that will **some** effect. The question is how much. A statistically insignificant model coefficient is less a sign that a variable has no effect, and more a sign that there is insufficient data to detect a meaningful effect. If our model coefficients are not statistically significant, it is simply a good indication that we need more data. If the model coefficients are statistically significant, then our focus should be on interpreting the coefficients, not on the p-value itself.\n\n## Model Interpretation\n\nModel interpretation is the process of understanding the influence that each explanatory variable has on the outcome. This involves understanding the direction, the magnitude, and the precision of the model coefficients. The coefficients themselves will tell us about direction and magnitude, while the standard errors and confidence intervals will tell us about precision.\n\n#### Model Coefficients\n\nThe model coefficients are the estimated values of the regression coefficients, or the estimated change in the outcome variable for a one unit change in the explanatory variable, while holding all other explanatory variables constant. The intercept is the estimated value of the outcome variable  when all of the explanatory variables are equal to zero (or the baseline values if the variables are transformed).\n\nA regression's coefficients are the most important part of interpreting the model. They specify the association between the explanatory variables and the outcome, telling us both the direction and the magnitude of the association. The direction of the association is indicated by the sign of the coefficient (positive = outcome increases with the explanatory variable, negative = outcome decreases with the explanatory variable). The magnitude of the association is indicated by the size of the coefficient (the larger the coefficient, the stronger the association). If the direction of the association is going in the wrong direction to that which we expect, this obviously indicates issues either in the theory or in the model, and if the magnitude of the effect is too small to be practically significant, then either the explanatory variable is not a good predictor of the outcome, or the model is not doing a good job of fitting the data (either the explanatory variable doesn't really matter or the model is not capturing the true relationship between the explanatory variables and the outcome). However, while the direction is relatively easy to interpret, magnitude is not, because what counts as a practically significant effect is dependent on the context of the analysis.\n\n#### Standard Errors\n\nThe standard errors are the standard deviation of the estimated coefficients. This is a measure of the precision of the estimated association between the explanatory variables and the outcome. The larger the standard error, the less precise the coefficient estimate. Precisely measured coefficients suggest that the model is fitting the data well, and we can be confident that the association between the explanatory variables and the outcome is real, and not just a result of random variation in the data. Defining what counts as 'precise' in any analysis, much like the coefficient magnitude, is dependent on the context of the analysis. However, in general, a standard error of less than 0.5 is considered to be a good estimate, and a standard error of less than 0.25 is considered to be a very precise estimate.\n\nIt is important to recognise that a precise coefficient alone cannot be treated as evidence of a causal effect between the explanatory variable and the outcome but it is, at least, a good indication that the explanatory variable is a good predictor of the outcome. In order to establish a causal relationship, we may consider methods that are more robust to the presence of confounding or omitted variables, the gold standard being randomised controlled trials, but with a strong theoretical basis and a good understanding of the data, we can make a good case for a causal relationship based on observational data.\n\n## Next Steps\n\n## Resources\n\n- [StatQuest: Linear Regression](https://www.youtube.com/watch?v=7ArmBVF2dCs)\n- [Regression & Other Stories](https://avehtari.github.io/ROS-Examples/)\n- [Statistical Inference via Data Science](https://moderndive.com)\n- [A User's Guide to Statistical Inference and Regression](https://mattblackwell.github.io/gov2002-book/)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}