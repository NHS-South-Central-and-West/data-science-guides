---
title: "Deep Learning in Python"
author: "Paul Johnson"
date: today
---

Deep learning is a subset of machine learning that uses neural networks to learn from data. Neural networks are a type of machine learning model that are inspired by the way the human brain works. Neural networks are made up of neurons, which are connected together in layers. Each neuron receives input from the previous layer, performs a calculation, and passes the output to the next layer. The output of the final layer is the prediction of the neural network.

Neural networks are trained by adjusting the weights of the connections between neurons. The weights are adjusted based on the error of the prediction. The error is calculated by comparing the prediction to the actual value. The error is then used to adjust the weights in a way that reduces the error in the next prediction. This process is repeated until the neural network is able to make accurate predictions.

This notebook will demonstrate how to build a neural network using PyTorch to classify images of COVID-19 lung CT scans.

This notebook will take a while to run locally as neural networks are computationally expensive. Neural networks are designed to run on GPUs, which are much faster than CPUs, however, this notebook is designed to run on a CPU. If you want to run this notebook on a GPU, you can use [Google Colab](https://colab.research.google.com/), which provides free access to a GPU, but the code will need to be modified slightly. For more information on how to utilize GPUs with PyTorch, see [this tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).

## Setup
```{python}
#| label: imports

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn.functional as F

from torchvision import datasets, transforms
from torch import nn, optim
from torch.utils.data import DataLoader, Dataset, random_split, Subset

```

## Data

The data for this notebook is a small sample taken from a public COVID-19 lung CT scan dataset, containing a total of 2,068 CT scans which consists of 1,124 positive cases (COVID-19 infection) and 944 negative ones (normal and non-COVID-19). A small sample has been used to help speed up model training, but this will have a significant negative impact on model performance. The full dataset is available on [Kaggle](https://www.kaggle.com/datasets/mehradaria/covid19-lung-ct-scans).

The data is stored in a folder structure, where each folder contains images of a particular class. The folder names are the class labels (covid, normal).

```{python}
#| label: import-data

data_dir = '../../../data/ct-scans/'

transform = transforms.Compose([
    transforms.Resize(255),
    transforms.CenterCrop(224),
    transforms.ToTensor()
    ])

dataset = datasets.ImageFolder(data_dir, transform=transform)

# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

```

### Train & Test Sets

The data is split into training and test sets. The training set is used to train the neural network. The test set is used to evaluate the neural network after training.

```{python}
#| label: split-data

train, val, test = random_split(dataset, [0.6, 0.2, 0.2])

print(len(train))
print(len(val))
print(len(test))

train_loader = DataLoader(train, batch_size=32, shuffle=True, num_workers=4)
val_loader = DataLoader(val, batch_size=32, shuffle=True, num_workers=4)
test_loader = DataLoader(test, batch_size=32, shuffle=True, num_workers=4)

```


### Data Exploration

Because the data is stored in a folder structure, we cannot inspect the data in typical ways, like using `head()` or `describe()`. Instead, we can visualize the images themselves to get a sense of what the data looks like.

Below is a simple helper function to visualize images. We can use this function to visualize a batch of images from the training set.

```{python}
#| label: imshow-helper

%matplotlib inline

def imshow(image, ax=None, title=None, normalize=True):
    """Imshow for Tensor."""
    if ax is None:
        fig, ax = plt.subplots()
    image = image.numpy().transpose((1, 2, 0))

    if normalize:
        mean = np.array([0.5, 0.5, 0.5])
        std = np.array([0.5, 0.5, 0.5])
        image = std * image + mean
        image = np.clip(image, 0, 1)

    ax.imshow(image)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.tick_params(axis='both', length=0)
    ax.set_xticklabels('')
    ax.set_yticklabels('')

    return ax

```

```{python}
#| label: visualize-images

# obtain one batch of training images
images, labels = next(iter(train_loader))

# plot 20 images in the batch, along with the corresponding labels
fig = plt.figure(figsize=(25, 4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])
    imgshow(images[idx], normalize=False, ax=ax)
    ax.set_title(labels[idx].item())

```

```{python}
#| label: image-shape

# check the shape of the images
images[0].shape

```

```{python}
#| label: image-pixels

# check how many pixels are in each image
images[0].shape[1] * images[0].shape[2]

```

## Neural Network

The neural network is defined using the `nn.Module` class. The `__init__` method is used to define the layers of the model. The `forward` method is used to define the forward pass through the network.

```{python}
#| label: specify-model

class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)
        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)
        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(25088, 512)
        self.fc2 = nn.Linear(512, 120)
        self.fc3 = nn.Linear(120, 84)
        self.fc4 = nn.Linear(84, 10)
        self.dropout = nn.Dropout(p=0.2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = x.view(x.shape[0], -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.dropout(x)
        x = F.relu(self.fc4(x))
        x = self.dropout(x)

        return x

model = CNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

```

### Model Training

The model is trained for 10 epochs. The loss and accuracy are printed every 5 steps.

```{python}
#| label: train

num_epochs = 10
print_every = 5

train_losses = []
val_losses = []

for epoch in range(1, num_epochs + 1):
    # track train & val loss
    train_loss = 0.0
    val_loss = 0.0
    correct = 0
    total = 0
    val_acc = 0
    
    # train model
    model.train()
    for images, labels in train_loader:

        # clear gradients of optimized variables
        optimizer.zero_grad()
        # predict outputs by passing inputs to model
        output = model(images)
        # batch loss
        loss = criterion(output, labels)
        # compute gradient of loss from model params
        loss.backward()
        # optimization step
        optimizer.step()
        # update train loss
        train_loss += loss.item() * images.size(0)
        
    # validate model
    model.eval()
    for images, labels in val_loader:

        output = model(images)
        
        loss = criterion(output, labels)

        # val loss
        val_loss += loss.item() * images.size(0)

        # val accuracy
        _, pred = torch.max(output.data, 1)
        total += labels.size(0)
        correct += (pred == labels).sum().item()
        val_acc += correct/total
    
    # avg loss & accuracy
    train_loss = train_loss/len(train_loader.sampler)
    val_loss = val_loss/len(val_loader.sampler)
    val_acc = val_acc/len(val_loader.sampler)
    train_losses.append(train_loss)
    val_losses.append(val_loss)
        
    # print train loss, val loss, val accuracy
    print(f"Epoch {epoch}/{num_epochs}\t"
          f"Train Loss: {train_loss:.3f}\t"
          f"Validation Loss: {val_loss:.3f}\t "
          f"Validation Accuracy: {val_acc:.3f}")

```

### Model Evaluation

The model is evaluated on the test set.

```{python}
#| label: evaluation

test_loss = 0
accuracy = 0
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        log_ps = model(images)
        batch_loss = criterion(log_ps, labels)

        test_loss += batch_loss.item()

        # Calculate accuracy
        ps = torch.exp(log_ps)
        top_p, top_class = ps.topk(1, dim=1)
        equals = top_class == labels.view(*top_class.shape)
        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()

print(f"Test loss: {test_loss/len(test_loader):.3f}.. "
      f"Test accuracy: {accuracy/len(test_loader):.3f}")

```

### Predictions

The model is used to predict the class of a single image.

```{python}
#| label: predict

images, labels = next(iter(test_loader))

img = images[0].view(1, 3, 224, 224)

# Turn off gradients to speed up this part
with torch.no_grad():
    logps = model(img)

# Output of the network are log-probabilities, need to take exponential for probabilities
ps = torch.exp(logps)
probab = list(ps.numpy()[0])
print("Predicted Probability:", probab)
print("Predicted Label:", probab.index(max(probab)))

```

## For more information

* [PyTorch](https://pytorch.org/)
* [PyTorch Tutorials](https://pytorch.org/tutorials/)
* [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)