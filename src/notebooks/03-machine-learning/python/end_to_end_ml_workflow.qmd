---
title: "End-to-End Machine Learning Workflow in Python"
subtitle: "Using Logistic Regression, KNN, and Random Forest in Scikit-Learn to Predict Heart Disease"
author: "Paul Johnson"
date: today
---

```{python}
#| label: setup
#| code-fold: true
#| code-summary: 'Setup Code (Click to Expand)'

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.base import BaseEstimator
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline

# data path
path = '../../../data/'
file_name = 'heart_disease.csv'

# import data
df = pd.read_csv(f"{path}{file_name}")

```

An end-to-end machine learning workflow can be broken down into many steps, and there are an extensive number of layers of complexity that can be added, serving a variety of purposes. However, in this guide we will work through a bare bones workflow, using a simple dataset, in order to get a better understanding of the process.

A simple end-to-end ML solution will typically include the following steps:

1. Importing & Cleaning Data
2. Exploratory Data Analysis (which will be skipped in this guide because it has been carried out in an earlier guide which can be found [here](https://htmlpreview.github.io/?https://github.com/NHS-South-Central-and-West/data-science-guides/blob/main/guides/R/eda.html))
2. Data Preprocessing
3. Model Training
    - Selecting from Candidate Models
    - Hyperparameter Tuning
    - Identifying Best Model
4. Fitting Model on Test Data
5. Model Evaluation

```{python}
#| label: data_summary

# data summary
df.info()
df.describe()
df.head()
```

## Data Cleaning

Though in this instance, the data is relatively clean, the following steps are included to demonstrate how you might clean data in a real-world scenario. Dropping NAs and duplicates should, however, be done with care (see discussion below).

```{python}
#| label: data_cleaning

# check for missing values
df.isna().sum()

# drop missing values
df.dropna(inplace=True)

# check for duplicates
df.duplicated().sum()

# drop duplicates
df.drop_duplicates(inplace=True)

```

There are several variables for which there appear to be missing values in the form of zeroes. This was identified in the exploratory data analysis notebook in this project. Variables like `cholesterol` and `resting_bp` should not be zero, and because the target class is disproportionately distributed within these zero values, it is important to address the problem.

There are a number of choices that could be made when it comes to dealing with null or missing values, and there are robust approaches to imputation (though it is necessary to take great care when doing so), however in the interest of simplicity, we will simply remove the rows with zero values in this instance.[^1]

[^1]: In a real-world analysis, it would be important to understand why the data is missing, and to consider the implications of removing the data. For example, if the data is missing at random, then removing the data will not have a significant impact on the analysis. However, if the data is missing because it was not collected, or if the data is missing because it is not available, then removing the data could have a significant impact on the analysis.

```{python}
#| label: remove-zero-values

# remove zero values
df = df[(df['cholesterol'] != 0) & (df['resting_bp'] != 0)]

```

# Data Preprocessing

```{python}
#| label: data_preprocessing

# get categorical columns
cat_cols = df.select_dtypes(include='object').columns
# get numerical columns (excluding target)
num_cols = df.drop('heart_disease', axis=1).select_dtypes(exclude='object').columns

# one-hot encode categorical columns
# df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

# check data
df.info()

```

## Train/Test Split

```{python}
#| label: train_test_split

# split data into train and test sets
X = df.drop('heart_disease', axis=1)
y = df['heart_disease']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

```

### Cross-Validation

For more information on cross-validation and how it can be used to train models:

- The [Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html) section in the Scikit-learn documentation provides a good overview of cross-validation and how it can be used to train models. It includes a discussion of a typical cross-validation workflow; the different types of cross-validation, their implementation in Scikit-learn, and their pros and cons; and the documentation is accompanied by a lot of really helpful visuals.
- Neptune AI's [blog post](https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right) on cross-validation goes into detail discussing cross-validation strategies, and gives clear visual demonstrations of how each strategy works.

```{python}
#| label: cross-val

# cross-validation
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=123)

```

## Model Training

```{python}
#| label: pipeline

scaler = Pipeline(steps=[('scaler', StandardScaler())])
one_hot = Pipeline(steps=[('one_hot', OneHotEncoder(drop='first'))])

preprocess = ColumnTransformer(
    transformers=[
        ('nums', scaler, num_cols),
        ('cats', one_hot, cat_cols)
        ])
```

### Hyperparameter Tuning

For a more detailed discussion of hyperparameter tuning, and the different methods for tuning in Scikit-learn, the [Tuning the Hyperparameters of an Estimator](https://scikit-learn.org/stable/modules/grid_search.html#) section of the Scikit-learn documentation is a good place to start, and the [Hyperparameter Tuning by Grid Search](https://inria.github.io/scikit-learn-mooc/python_scripts/parameter_tuning_grid_search.html) section of the Scikit-learn course is an excellent accompaniment.

In addition, the AWS overview of [hyperparameter tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) is a good resource if you are only looking for a brief overview. For an in-depth discussion about how hyperparameter tuning works, including the mathematics behind it, Jeremy Jordan's [Hyperparameter Tuning for Machine Learning Models](https://www.jeremyjordan.me/hyperparameter-tuning/) blogpost is thorough but easy to follow.

There are a lot of different Python libraries for hyperparameter tuning, however the most popular libraries that are specifically designed for tuning are [Hyperopt](http://hyperopt.github.io/hyperopt/) and [Optuna](https://optuna.org/). Both can be used to tune models built with Scikit-learn (and many other frameworks) and can often produce better results than Scikit-learn's built-in hyperparameter tuning methods. I have had limited experience using Hyperopt, however I have used Optuna often and I would highly recommend it if/when you are ready to move beyond Scikit-learn's options for tuning.

```{python}
#| label: log-reg

log_pipe = Pipeline([
    ('preprocess', preprocess),
    ('log', LogisticRegression()),
])

# create parameter grid based on the results of random search
param_grid = {
    'log__max_iter': [10, 100, 250, 500, 1000, 10000],
    'log__C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100],
    'log__warm_start': [True, False],
}

# grid search
grid_search = RandomizedSearchCV(
    estimator = log_pipe,
    param_distributions = param_grid,
    n_iter = 100,
    cv = cv,
    random_state=123,
    n_jobs = -1,
    verbose = 2)

# fit model
log = grid_search.fit(X_train, y_train)

grid_search.best_params_
knn.score(X_train, y_train)
```

```{python}
#| label: knn

knn_pipe = Pipeline([
    ('preprocess', preprocess),
    ('knn', KNeighborsClassifier()),
])

# create parameter grid based on the results of random search
param_grid = {
        'knn__n_neighbors': [1, 3, 5, 7, 10, 15],
        'knn__weights': ['uniform', 'distance'],
        'knn__p': [1, 2],
        'knn__leaf_size': [20, 20, 30, 40, 50],
}

# grid search
grid_search = RandomizedSearchCV(
    estimator = knn_pipe,
    param_distributions = param_grid,
    n_iter = 100,
    cv = cv,
    random_state=123,
    n_jobs = -1,
    verbose = 2)

# fit model
knn = grid_search.fit(X_train, y_train)
knn.best_params_
knn.score(X_train, y_train)

```

```{python}
#| label: random-forest

rf_pipe = Pipeline([
    ('preprocess', preprocess),
    ('rf', RandomForestClassifier()),
])

# create parameter grid based on the results of random search
param_grid = {
    'rf__bootstrap': [True],
    'rf__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
    'rf__max_features': ['auto', 'sqrt'],
    'rf__min_samples_leaf': [1, 2, 4],
    'rf__min_samples_split': [2, 5, 10],
    'rf__n_estimators': [100] #, 200, 300, 1000, 1000]
}

# grid search
grid_search = RandomizedSearchCV(
    estimator = rf_pipe,
    param_distributions = param_grid,
    n_iter = 10,
    cv = 5,
    random_state=123,
    n_jobs = -1,
    verbose = 2)

# fit model
rf = grid_search.fit(X_train, y_train)

rf.score(X_train, y_train)

best_params = grid_search.best_params_

```

### Refit Best Model

```{python}
#| label: refit

# refit best model
best_model = grid_search.best_estimator_

```

## Model Evaluation

```{python}
#| label: evaluation

best_model

best_model.fit(X_train, y_train)

# get predictions
y_pred = rf.predict(X_test)

# compute f1 score
f1 = f1_score(y_test, y_pred)

# compare model performance
print(f"Random Forest F1 Score: {f1:.4f}")

```

## Model Interpretation

The random forest has a higher F1 score than the logistic regression model, so that is the model we will use. We can compute the features that are most important in predicting heart disease in the random forest.

```{python}
#| label: feature-importance

# get feature importance
feat_importance = pd.DataFrame({'feature': X.columns, 'importance': rf[1].feature_importances_})

# plot feature importance
plt.figure(figsize=(8, 10))
sns.barplot(x='importance', y='feature', data=feat_importance.sort_values(by='importance', ascending=False))
sns.despine()
plt.show()

```

## Recreating the Pipeline Used in the R Guide

The R guide uses a pipeline to preprocess the data and train the model. We can recreate this pipeline in Python, but it is a more complex process.

In order to test multiple classifiers, we have to specify a class for switching between classifiers. This class is based on this [StackOverflow answer](https://stackoverflow.com/questions/48507651/multiple-classification-models-in-a-scikit-pipeline-python).


```{python}
#| label: switcher-class
class ClfSwitcher(BaseEstimator):

    def __init__(self,estimator = LogisticRegression(),):

        """
        A Custom BaseEstimator that can switch between classifiers.
        :param estimator: sklearn object - The classifier
        """

        self.estimator = estimator

    def fit(self, X, y=None, **kwargs):
        self.estimator.fit(X, y)
        return self

    def predict(self, X, y=None):
        return self.estimator.predict(X)

    def predict_proba(self, X):
        return self.estimator.predict_proba(X)

    def score(self, X, y):
        return self.estimator.score(X, y)

```

```{python}
# label: model-pipeline

scaler = Pipeline(steps=[('scaler', StandardScaler())])
one_hot = Pipeline(steps=[('one_hot', OneHotEncoder(drop='first'))])

preprocess = ColumnTransformer(
    transformers=[
        ('nums', scaler, num_cols),
        ('cats', one_hot, cat_cols)
        ])

pipeline = Pipeline([
    ('preprocess', preprocess),
    ('clf', ClfSwitcher()),
])

parameters = [
    {
        'clf__estimator': [LogisticRegression()],
        'clf__estimator__C': (1e-2, 1e-1, 1e0, 1e1, 1e2),
    },
    {
        'clf__estimator': [KNeighborsClassifier()],
        'clf__estimator__n_neighbors': [1, 3, 5, 7, 10],
        'clf__estimator__p': [1, 2],
        'clf__estimator__leaf_size': [1, 5, 10, 15],
    },
    {
        'clf__estimator': [RandomForestClassifier()],
        'clf__estimator__n_estimators': [10],
        'clf__estimator__max_depth': [80, 100, 120, 140],
        'clf__estimator__max_features': [2, 3, 4, 5],
        'clf__estimator__min_samples_leaf': [3, 4, 5],
        'clf__estimator__min_samples_split': [8, 10, 12],
    },
]

gscv = GridSearchCV(pipeline, parameters, cv=5, n_jobs=12, scoring='f1', return_train_score=True, verbose=3)
gscv.fit(X_train, y_train)

# currently producing same score each time, so not sure if it's working
gscv.best_params_
gscv.score(X_test, y_test)

```

## Next Steps

## Resources

There are lots of great (and free) introductory resources for machine learning:

- [Machine Learning University](https://aws.amazon.com/machine-learning/mlu)
- [MLU Explain](https://mlu-explain.github.io/)
- [Machine Learning for Beginners](https://microsoft.github.io/ML-For-Beginners/?utm_source=substack&utm_medium=email#/)

For a guided video course, Data Talks Club's Machine Learning Zoomcamp (available on [Github](https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp) and [Youtube](https://www.youtube.com/watch?v=MqI8vt3-cag&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)) is well-paced and well-presented, covering a variety of machine learning methods and even covering some of the aspects that introductory course often skip over, like deployment and data engineering principles. However, while the ML Zoomcamp course is intended as an introduction to machine learning, it does assume a certain level of familiarity with programming (the course uses Python) and software engineering. The appendix section of the course is definitely helpful for bridging some of the gaps in the course, but it is still worth being aware of the way the course is structured.

If you are looking for something that goes into greater detail about a wide range of machine learning methods, then there is no better resource than [An Introduction to Statistical Learning](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf) (or its accompanying [online course]((https://www.statlearning.com/online-course))), by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.

Finally, if you are particularly interested in learning the mathematics that underpins machine learning, I would highly recommend [Mathematics of Machine Learning](https://tivadardanka.com/books/mathematics-of-machine-learning), which is admittedly not free but is very, very good. If you want to learn about the mathematics of machine learning but are not comfortable enough tackling the Mathematics of ML book, the [StatQuest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw) and [3Blue1Brown](https://www.youtube.com/c/3blue1brown) Youtube channels are both really accessible and well-presented.
