---
title: "End-to-End Machine Learning in R: Predicting Heart Disease"
author: "Paul Johnson"
date: today
---


## Setup

```{r}
#| label: setup

# import packages
suppressPackageStartupMessages({
  library(dplyr)
  library(ggplot2)
  library(tidymodels)
})

theme_set(theme_minimal())

```

## Data

An important step in any machine learning workflow is exploratory analysis, in order to get a better understanding of the data. 

### Import Data

```{r}
#| label: import-data

df <- readr::read_csv(here::here('data', 'heart_disease.csv'))

df <- df |> 
  mutate(
    sex = as.factor(sex),
    fasting_bs = as.factor(fasting_bs),
    resting_ecg = as.factor(resting_ecg),
    angina = as.factor(angina),
    heart_disease = as.factor(heart_disease)
  )

glimpse(df)

```

### Train/Test Split

```{r}

train_test_split <-
  rsample::initial_split(df,
                         strata = heart_disease,
                         prop = 0.7)

train_df <- rsample::training(train_test_split)
test_df <- rsample::testing(train_test_split)

```

#### Specify Cross-Validation Folds

```{r}

train_folds <- vfold_cv(train_df, strata = heart_disease)
train_folds

```

## Preprocessing

```{r}

model_rec <- recipe(heart_disease ~ ., data = train_df) |>
  # normalize data so that it is between 0 and 1
  step_normalize(all_numeric_predictors()) |>
  # standardize data so that is has mean zero and sd one
  step_center(all_numeric_predictors()) |>
  step_scale(all_numeric_predictors()) |>
  # one-hot encode categorical variables
  step_dummy(all_nominal_predictors(), one_hot = TRUE)

model_rec |>
  prep() |>
  bake(new_data = NULL)

model_train <- model_rec |>
  prep() |>
  bake(new_data = NULL)

model_test <- model_rec |>
  prep() |>
  bake(new_data = test_df)

```

## Fit Models

### Logistic Regression

```{r}
#| label: logistic-regression

log_mod <-
  logistic_reg() |>
  set_engine('glm') |>
  set_mode('classification')

model_wf <-
  workflow() |>
  add_recipe(model_rec)

eval_metrics <- metric_set(roc_auc, accuracy, sensitivity, specificity)

log_rs <- model_wf |>
  add_model(log_mod) |>
  fit_resamples(
    resamples = train_folds,
    metrics = eval_metrics,
    control = control_resamples(save_pred = TRUE)
  )

collect_metrics(log_rs)

log_rs |>
  conf_mat_resampled()

log_rs |>
  collect_predictions() |>
  group_by(id) |>
  roc_curve(heart_disease, .pred_1) |>
  ggplot(aes(1 - specificity, sensitivity, color = id)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) +
  coord_equal()

```

### Random Forest

```{r}
#| label: random-forest

rf_mod <- 
  rand_forest(trees = 1000) |>
  set_mode("classification") |>
  set_engine("ranger")

rf_mod

rf_rs <- model_wf |>
  add_model(rf_mod) |>
  fit_resamples(
    resamples = train_folds,
    metrics = eval_metrics,
    control = control_resamples(save_pred = TRUE)
  )

collect_metrics(rf_rs)

rf_rs |>
  conf_mat_resampled()

rf_rs |>
  collect_predictions() |>
  group_by(id) |>
  roc_curve(heart_disease, .pred_1) |>
  ggplot(aes(1 - specificity, sensitivity, color = id)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) +
  coord_equal()

```